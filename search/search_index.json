{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"","title":"Home"},{"location":"getting-started.html","text":"Getting Started \u00b6","title":"Getting Started"},{"location":"getting-started.html#getting-started","text":"","title":"Getting Started"},{"location":"Integrations/index.html","text":"","title":"Index"},{"location":"Integrations/AzureActiveDirectory.html","text":"Azure active directory \u00b6 Register Komodor App with the Microsoft identity platform \u00b6 Under Manage , select App registrations > New registration \u00b6 Name: Komodor Supported account types: Accounts in any organizational directory (Any Azure AD directory - Multitenant) Redirect URI: Platform: Web URL: https://auth.komodor.com/login/callback Click on the Register button Create a Client secret \u00b6 Select Certificates & secrets > Client secrets > New client secret Description: Komodor client secret Expires: choose whatever right for you Click on the Add button Once generated, copy its value and save it! This secret value is never displayed again after you leave this page Make sure to record the expiration date, you will need to renew the key before that day to avoid a service interruption Add permissions \u00b6 Select API permissions > Add a permission > Microsoft APIs > Microsoft Graph choose Delegated permissions Search Directory choose Directory > Directory.Read.All Click the Add Premission button Add another Redirect URIs \u00b6 Select Authentication > WEB > Add URI Add this url https://komodorio.us.auth0.com/login/callback Save Done! The Komodor app is registered \ud83c\udf3b \u00b6 Please send the next values to your contact at Komodor: \u00b6 Microsoft Azure AD Domain Your Azure AD domain name. You can find this on your Azure AD directory's overview page in the Microsoft Azure portal. Client ID Unique identifier for your registered Azure AD application. Enter the saved value of the Application (client) ID for the app you just registered in Azure AD. Client Secret String used to gain access to your registered Azure AD application. Enter the saved value of the Client secret for the app you just registered in Azure AD. resources: \u00b6 Auth0 tutorial youtube (old Azure version but a really nice video)","title":"Azure Active Directory"},{"location":"Integrations/AzureActiveDirectory.html#azure-active-directory","text":"","title":"Azure active directory"},{"location":"Integrations/AzureActiveDirectory.html#register-komodor-app-with-the-microsoft-identity-platform","text":"","title":"Register Komodor App with the Microsoft identity platform"},{"location":"Integrations/AzureActiveDirectory.html#under-manage-select-app-registrations-new-registration","text":"Name: Komodor Supported account types: Accounts in any organizational directory (Any Azure AD directory - Multitenant) Redirect URI: Platform: Web URL: https://auth.komodor.com/login/callback Click on the Register button","title":"Under\u00a0Manage, select\u00a0App registrations\u00a0&gt;\u00a0New registration"},{"location":"Integrations/AzureActiveDirectory.html#create-a-client-secret","text":"Select Certificates & secrets > Client secrets > New client secret Description: Komodor client secret Expires: choose whatever right for you Click on the Add button Once generated, copy its value and save it! This secret value is never displayed again after you leave this page Make sure to record the expiration date, you will need to renew the key before that day to avoid a service interruption","title":"Create a Client secret"},{"location":"Integrations/AzureActiveDirectory.html#add-permissions","text":"Select API permissions > Add a permission > Microsoft APIs > Microsoft Graph choose Delegated permissions Search Directory choose Directory > Directory.Read.All Click the Add Premission button","title":"Add permissions"},{"location":"Integrations/AzureActiveDirectory.html#add-another-redirect-uris","text":"Select Authentication > WEB > Add URI Add this url https://komodorio.us.auth0.com/login/callback Save","title":"Add another Redirect URIs"},{"location":"Integrations/AzureActiveDirectory.html#done-the-komodor-app-is-registered","text":"","title":"Done! The Komodor app is registered \u00a0\ud83c\udf3b"},{"location":"Integrations/AzureActiveDirectory.html#please-send-the-next-values-to-your-contact-at-komodor","text":"Microsoft Azure AD Domain Your Azure AD domain name. You can find this on your Azure AD directory's overview page in the Microsoft Azure portal. Client ID Unique identifier for your registered Azure AD application. Enter the saved value of the Application (client) ID for the app you just registered in Azure AD. Client Secret String used to gain access to your registered Azure AD application. Enter the saved value of the Client secret for the app you just registered in Azure AD.","title":"Please send the next values to your contact at Komodor:"},{"location":"Integrations/AzureActiveDirectory.html#resources","text":"Auth0 tutorial youtube (old Azure version but a really nice video)","title":"resources:"},{"location":"Integrations/Datadog-Monitor-Notification.html","text":"Datadog Monitor Notification \u00b6 Adding A Komodor dynamic link to DataDog Monitor Notifications will generate a direct link to the relevant service in Komodor. You will see the alert link in your Alerting provider connected to DataDog. Installation \u00b6 Prerequisites \u00b6 To use DataDog variables in monitor notifications, there are two prerequisites imposed by DataDog: Make sure there are variables defined for: cluster name, namespace, service name. Make sure the tags are used in the monitor, or grouped by them. Link Setup \u00b6 Make sure the prerequisites above are met. Add the dynamic link to the DataDog monitor notification message: * https://app.komodor.com/main/deep-dive/YourAccountName.{{cluster_name}}-{{namespace}}.{{service_name}} Please note variable names will vary depending on your local DataDog setup. Example: For account greatcompany alerting PagerDuty. variables: cluster_name k8s_clustername ,namespace k8s_namespace , service_name k8s_servicename The dynamic link will be: @pagerduty-Datadog https://app.komodor.com/main/deep-dive/greatcompany. {{ k8s_clustername }} - {{ k8s_namespace }} . {{ k8s_servicename }} Confirmation and testing \u00b6 Dynamic Komodor link will be added to your next DataDog alert. The link will include the relevant information. Testing \u00b6 For an end-to-end testing, add the dynamic link to a test monitor and use the 'Test Notification' button in DataDog. Use the generated link in your alert provider and make sure it directs you to the correct service in Komodor. If the link fails, check the dynamic link prerequisites . See also \u00b6 DataDog documentation on monitor notifications","title":"Datadog Monitor Notifications"},{"location":"Integrations/Datadog-Monitor-Notification.html#datadog-monitor-notification","text":"Adding A Komodor dynamic link to DataDog Monitor Notifications will generate a direct link to the relevant service in Komodor. You will see the alert link in your Alerting provider connected to DataDog.","title":"Datadog Monitor Notification"},{"location":"Integrations/Datadog-Monitor-Notification.html#installation","text":"","title":"Installation"},{"location":"Integrations/Datadog-Monitor-Notification.html#prerequisites","text":"To use DataDog variables in monitor notifications, there are two prerequisites imposed by DataDog: Make sure there are variables defined for: cluster name, namespace, service name. Make sure the tags are used in the monitor, or grouped by them.","title":"Prerequisites"},{"location":"Integrations/Datadog-Monitor-Notification.html#link-setup","text":"Make sure the prerequisites above are met. Add the dynamic link to the DataDog monitor notification message: * https://app.komodor.com/main/deep-dive/YourAccountName.{{cluster_name}}-{{namespace}}.{{service_name}} Please note variable names will vary depending on your local DataDog setup. Example: For account greatcompany alerting PagerDuty. variables: cluster_name k8s_clustername ,namespace k8s_namespace , service_name k8s_servicename The dynamic link will be: @pagerduty-Datadog https://app.komodor.com/main/deep-dive/greatcompany. {{ k8s_clustername }} - {{ k8s_namespace }} . {{ k8s_servicename }}","title":"Link Setup"},{"location":"Integrations/Datadog-Monitor-Notification.html#confirmation-and-testing","text":"Dynamic Komodor link will be added to your next DataDog alert. The link will include the relevant information.","title":"Confirmation and testing"},{"location":"Integrations/Datadog-Monitor-Notification.html#testing","text":"For an end-to-end testing, add the dynamic link to a test monitor and use the 'Test Notification' button in DataDog. Use the generated link in your alert provider and make sure it directs you to the correct service in Komodor. If the link fails, check the dynamic link prerequisites .","title":"Testing"},{"location":"Integrations/Datadog-Monitor-Notification.html#see-also","text":"DataDog documentation on monitor notifications","title":"See also"},{"location":"Integrations/Datadog.html","text":"Datadog Integration \u00b6 DataDog integration allows DataDog Monitor Alerts to be available in Komodor and to suggest related service based on services connection deteced by DataDog. Prerequisites \u00b6 For Komodor service correlation, your services according to Datadog, the following DataDog's service tags should be available on the resources. environment - should match the environment specified on the Datadog service ( DD_ENV ) service - should match the service name specified on the Datadog service ( DD_SERVICE ) DataDog's tags can be done by environment variables, labels, and annotations. To do the correlation, the tags must be a string value and not a reference value. For more information about DataDog tags for Kubernetes go into DataDog tagging documentation Installation Steps \u00b6 Make sure the above prerequisites are met. Locate the Datadog installation tile on Komodor Integrations page. Press Install Integration . Follow the on screen instructions. Confirmation \u00b6 A Datadog Integration tile will be added to the top section under Installed Integrations . Your services that interact with each other will appear under the Related Services section in the Komodor services page.","title":"Datadog"},{"location":"Integrations/Datadog.html#datadog-integration","text":"DataDog integration allows DataDog Monitor Alerts to be available in Komodor and to suggest related service based on services connection deteced by DataDog.","title":"Datadog Integration"},{"location":"Integrations/Datadog.html#prerequisites","text":"For Komodor service correlation, your services according to Datadog, the following DataDog's service tags should be available on the resources. environment - should match the environment specified on the Datadog service ( DD_ENV ) service - should match the service name specified on the Datadog service ( DD_SERVICE ) DataDog's tags can be done by environment variables, labels, and annotations. To do the correlation, the tags must be a string value and not a reference value. For more information about DataDog tags for Kubernetes go into DataDog tagging documentation","title":"Prerequisites"},{"location":"Integrations/Datadog.html#installation-steps","text":"Make sure the above prerequisites are met. Locate the Datadog installation tile on Komodor Integrations page. Press Install Integration . Follow the on screen instructions.","title":"Installation Steps"},{"location":"Integrations/Datadog.html#confirmation","text":"A Datadog Integration tile will be added to the top section under Installed Integrations . Your services that interact with each other will appear under the Related Services section in the Komodor services page.","title":"Confirmation"},{"location":"Integrations/Grafana.html","text":"Grafana Integration \u00b6 Grafana Integration adds Grafana Alert events to your service in Komodor. Installation \u00b6 Prerequisites \u00b6 A running instance of Grafana. Installation Steps \u00b6 Make sure the above prerequisites are met. Locate the Grafana installation tile on the Komodor Integrations page. Press Install Integration . A dialog will open. Go to your Grafana instance. In the Grafana side bar, hover your cursor over the Alerting (bell) icon and then click Notification channels . Press New channel . In the Name field, enter a name for the channel. In the Type field, choose webhook . Copy the webhook URL to the Url field. Open the Notifications settings . Check the Default option. When selected, this option sends a notification on this channel for all alert rules. Press Save Press Install Grafana Installation Confirmation \u00b6 A Grafana Integration tile will be added to the top section under Installed Integrations . When a Grafana alert is triggered it will be added to the relevant service in Komodor.","title":"Grafana"},{"location":"Integrations/Grafana.html#grafana-integration","text":"Grafana Integration adds Grafana Alert events to your service in Komodor.","title":"Grafana Integration"},{"location":"Integrations/Grafana.html#installation","text":"","title":"Installation"},{"location":"Integrations/Grafana.html#prerequisites","text":"A running instance of Grafana.","title":"Prerequisites"},{"location":"Integrations/Grafana.html#installation-steps","text":"Make sure the above prerequisites are met. Locate the Grafana installation tile on the Komodor Integrations page. Press Install Integration . A dialog will open. Go to your Grafana instance. In the Grafana side bar, hover your cursor over the Alerting (bell) icon and then click Notification channels . Press New channel . In the Name field, enter a name for the channel. In the Type field, choose webhook . Copy the webhook URL to the Url field. Open the Notifications settings . Check the Default option. When selected, this option sends a notification on this channel for all alert rules. Press Save Press Install Grafana Installation","title":"Installation Steps"},{"location":"Integrations/Grafana.html#confirmation","text":"A Grafana Integration tile will be added to the top section under Installed Integrations . When a Grafana alert is triggered it will be added to the relevant service in Komodor.","title":"Confirmation"},{"location":"Integrations/LaunchDarkly.html","text":"LaunchDarkly Integration \u00b6 Integration with LaunchDarkly extends the holistic view of the environment with flag change events on the timeline. For example: Feature flag was turned on Feature flag in variation changed Feature flag targeting was changed Installation \u00b6 The LaunchDarkly integration involves two parts: \u00b6 Enabling the integration in Komodor. Creating a LaunchDarkly webhook. Enabling the integration in Komodor \u00b6 To enable the Komodor LaunchDarkly integration go to Komodor integrations page and select LaunchDarkly This will open a window with the LaunchDarkly webhook URL and sign key *Note: you can use the LaunchDarkly Webhook Integration link to continue with the webhook creation Creating the LaunchDarkly webhook \u00b6 To create the LaunchDarkly webhook go to LaunchDarkly Webhook Integration Page and use the values from Komodor's integration page Name: Your integration's name URL: Use the URL from the integration window in Komodor Check the \"Sign this webhook\" checkbox Secret: Use the Secret value from the integration window in Komodor Filter Policy: To send all LaunchDarkly event to Komodor: press the \"+ Add statment\" Choose resources for this policy statement: proj/*:env/*:flag/* Allow or deny actions on the resource: Allow Choose actions to allow or deny: All actions \" Update statement \" Check \" I have read and agree to the Integration Terms and Conditions \" \" Save Settings \" to finish the integration.","title":"LaunchDarkly"},{"location":"Integrations/LaunchDarkly.html#launchdarkly-integration","text":"Integration with LaunchDarkly extends the holistic view of the environment with flag change events on the timeline. For example: Feature flag was turned on Feature flag in variation changed Feature flag targeting was changed","title":"LaunchDarkly Integration"},{"location":"Integrations/LaunchDarkly.html#installation","text":"","title":"Installation"},{"location":"Integrations/LaunchDarkly.html#the-launchdarkly-integration-involves-two-parts","text":"Enabling the integration in Komodor. Creating a LaunchDarkly webhook.","title":"The LaunchDarkly integration involves two parts:"},{"location":"Integrations/LaunchDarkly.html#enabling-the-integration-in-komodor","text":"To enable the Komodor LaunchDarkly integration go to Komodor integrations page and select LaunchDarkly This will open a window with the LaunchDarkly webhook URL and sign key *Note: you can use the LaunchDarkly Webhook Integration link to continue with the webhook creation","title":"Enabling the integration in Komodor"},{"location":"Integrations/LaunchDarkly.html#creating-the-launchdarkly-webhook","text":"To create the LaunchDarkly webhook go to LaunchDarkly Webhook Integration Page and use the values from Komodor's integration page Name: Your integration's name URL: Use the URL from the integration window in Komodor Check the \"Sign this webhook\" checkbox Secret: Use the Secret value from the integration window in Komodor Filter Policy: To send all LaunchDarkly event to Komodor: press the \"+ Add statment\" Choose resources for this policy statement: proj/*:env/*:flag/* Allow or deny actions on the resource: Allow Choose actions to allow or deny: All actions \" Update statement \" Check \" I have read and agree to the Integration Terms and Conditions \" \" Save Settings \" to finish the integration.","title":"Creating the LaunchDarkly webhook"},{"location":"Integrations/MicrosoftTeams.html","text":"Microsoft Teams Integration \u00b6 The Teams integration allows configuration of notifications for events such as Deploy or Health from Komodor to a Teams channel. Komodor supports different granularity levels, Cluster, Namespace and Service for each notification. The notification type, destination, and granularity can be defined in the Komodor Notifications screen. Configuring Teams Notifications \u00b6 Configuration Steps \u00b6 Open up the Komodor Notifications Panel . On this screen you can see all existing notifications or configure new ones. Scroll to the bottom of the page and decide which type of notification you would like to create, Deployment or Health Change. Select Add this notification for the type of notification you would like to configure. Choose the cluster that you would like to alert on, or leave empty to alert on all clusters. Click Continue to proceed to the webhook configuration. Enter the channel name that you would like to post to (starting with @. \"@channel\" for example) Retrieve the webhook URL from Teams using the on screen instructions: Navigate to the channel where you want to add the webhook and select (\u2022\u2022\u2022) More Options from the top navigation bar. Choose Connectors from the drop-down menu and search for Incoming Webhook. Select the Configure button, provide a name, and optionally, upload an image avatar for your webhook. The dialog window will present a unique URL that will map to the channel. Make sure that you copy and paste the URL into the \"webhook URL\" field. Click on Create Notification to complete the Notification Configuration More information on Microsoft Teams integration with webhooks can be found in the Official Documentation .","title":"Microsoft Teams"},{"location":"Integrations/MicrosoftTeams.html#microsoft-teams-integration","text":"The Teams integration allows configuration of notifications for events such as Deploy or Health from Komodor to a Teams channel. Komodor supports different granularity levels, Cluster, Namespace and Service for each notification. The notification type, destination, and granularity can be defined in the Komodor Notifications screen.","title":"Microsoft Teams Integration"},{"location":"Integrations/MicrosoftTeams.html#configuring-teams-notifications","text":"","title":"Configuring Teams Notifications"},{"location":"Integrations/MicrosoftTeams.html#configuration-steps","text":"Open up the Komodor Notifications Panel . On this screen you can see all existing notifications or configure new ones. Scroll to the bottom of the page and decide which type of notification you would like to create, Deployment or Health Change. Select Add this notification for the type of notification you would like to configure. Choose the cluster that you would like to alert on, or leave empty to alert on all clusters. Click Continue to proceed to the webhook configuration. Enter the channel name that you would like to post to (starting with @. \"@channel\" for example) Retrieve the webhook URL from Teams using the on screen instructions: Navigate to the channel where you want to add the webhook and select (\u2022\u2022\u2022) More Options from the top navigation bar. Choose Connectors from the drop-down menu and search for Incoming Webhook. Select the Configure button, provide a name, and optionally, upload an image avatar for your webhook. The dialog window will present a unique URL that will map to the channel. Make sure that you copy and paste the URL into the \"webhook URL\" field. Click on Create Notification to complete the Notification Configuration More information on Microsoft Teams integration with webhooks can be found in the Official Documentation .","title":"Configuration Steps"},{"location":"Integrations/PagerDuty.html","text":"PagerDuty Integration \u00b6 PagerDuty Integration adds PagerDuty Incident events to your services events. NOTE: We currently only support PD events that originate in Datadog. Installation \u00b6 Prerequisites \u00b6 In order to connect your Datadog-PD incidents to your services in Komodor, you need to match environment variables found in Datadog to your services in Kubernetes. Please make sure the following environment variables exist in your kubernetes deployment: - DD_ENV should match the environment specified on the Datadog service - DD_SERVICE should match the service name specified on the Datadog service Installation Steps \u00b6 Make sure the prerequisites above are met. Locate the PagerDuty installation tile on Komodor Integrations page. Press Install Integration . You will be redirected to Pagerduty and back to Komodor successfully. Confirmation \u00b6 A PagerDuty Integration tile will be added to the top section of labeled Installed Integrations . When an issue from Datadog is raised through PD, you will be able to find it in the appropriate Komodor services page.","title":"PagerDuty"},{"location":"Integrations/PagerDuty.html#pagerduty-integration","text":"PagerDuty Integration adds PagerDuty Incident events to your services events. NOTE: We currently only support PD events that originate in Datadog.","title":"PagerDuty Integration"},{"location":"Integrations/PagerDuty.html#installation","text":"","title":"Installation"},{"location":"Integrations/PagerDuty.html#prerequisites","text":"In order to connect your Datadog-PD incidents to your services in Komodor, you need to match environment variables found in Datadog to your services in Kubernetes. Please make sure the following environment variables exist in your kubernetes deployment: - DD_ENV should match the environment specified on the Datadog service - DD_SERVICE should match the service name specified on the Datadog service","title":"Prerequisites"},{"location":"Integrations/PagerDuty.html#installation-steps","text":"Make sure the prerequisites above are met. Locate the PagerDuty installation tile on Komodor Integrations page. Press Install Integration . You will be redirected to Pagerduty and back to Komodor successfully.","title":"Installation Steps"},{"location":"Integrations/PagerDuty.html#confirmation","text":"A PagerDuty Integration tile will be added to the top section of labeled Installed Integrations . When an issue from Datadog is raised through PD, you will be able to find it in the appropriate Komodor services page.","title":"Confirmation"},{"location":"Integrations/Sentry.html","text":"Sentry Integration \u00b6 The Sentry integration allows you to see sentry issues in Komodor. For each service, Komodor automatically maps the relevant Sentry project to Komodor services and allows you to gain a full-service timeline: both relevant changes (deploys, config changes etc\u2019) and issues from Sentry will all be in one place. Installation \u00b6 Installation Steps \u00b6 Make sure the services you track in Komodor use the SENTRY_DSN environment variable. Locate the Sentry installation tile on the Komodor Integrations page. Click Install Integration . A dialog will open with a webhook URL and a link to Sentry to define an internal integration. Go to your Sentry account and click on settings in the left menu. Click on Developer Settings in the settings menu: Click on + New Internal Integration Name this integration Komodor. Paste the webhook URL in the webhook field: To operate properly Komodor needs these permissions: Project - Read Issue & Event - Read Check the issue webhook checkbox Save changes As soon as you save you will see your \u201cclient secret\u201d. Copy the value: Go back to your Komodor integrations page and paste the value of your client secret to the client secret text box. Click Install In your deployment.yaml \u00b6 We use the value of the environment variable SENTRY_DSN to match Sentry Issue events with your services in Komodor. Make sure your kubernetes deployment has the environment variable SENTRY_DSN . Confirmation \u00b6 A Sentry Integration tile will be added to the top section under Installed Integrations . Once completed you will be able to see Sentry events in you services view:","title":"Sentry"},{"location":"Integrations/Sentry.html#sentry-integration","text":"The Sentry integration allows you to see sentry issues in Komodor. For each service, Komodor automatically maps the relevant Sentry project to Komodor services and allows you to gain a full-service timeline: both relevant changes (deploys, config changes etc\u2019) and issues from Sentry will all be in one place.","title":"Sentry Integration"},{"location":"Integrations/Sentry.html#installation","text":"","title":"Installation"},{"location":"Integrations/Sentry.html#installation-steps","text":"Make sure the services you track in Komodor use the SENTRY_DSN environment variable. Locate the Sentry installation tile on the Komodor Integrations page. Click Install Integration . A dialog will open with a webhook URL and a link to Sentry to define an internal integration. Go to your Sentry account and click on settings in the left menu. Click on Developer Settings in the settings menu: Click on + New Internal Integration Name this integration Komodor. Paste the webhook URL in the webhook field: To operate properly Komodor needs these permissions: Project - Read Issue & Event - Read Check the issue webhook checkbox Save changes As soon as you save you will see your \u201cclient secret\u201d. Copy the value: Go back to your Komodor integrations page and paste the value of your client secret to the client secret text box. Click Install","title":"Installation Steps"},{"location":"Integrations/Sentry.html#in-your-deploymentyaml","text":"We use the value of the environment variable SENTRY_DSN to match Sentry Issue events with your services in Komodor. Make sure your kubernetes deployment has the environment variable SENTRY_DSN .","title":"In your deployment.yaml"},{"location":"Integrations/Sentry.html#confirmation","text":"A Sentry Integration tile will be added to the top section under Installed Integrations . Once completed you will be able to see Sentry events in you services view:","title":"Confirmation"},{"location":"Integrations/Slack.html","text":"Slack Integration \u00b6 Overview \u00b6 The Slack integration allows you to recieve notification for events such as successfull or failed deployments, service health state changes, and triggered Workflows from Komodor to a Slack channel. Komodor supports different granularity levels, Cluster, Namespace and Service for each notification. The notification type, destination, and granularity can be defined in the Komodor Notifications screen. Requirements \u00b6 Depending on your companies Slack settings a company admin may be required to enable the integration, the same user will need to be a Komodor Admin in order to start the installation from the Komodor integration page. !!! Note The free version of Slack is limited to 10 applications. Installation \u00b6 Once logged into the Komodor platform click on the Integrations tab. Locate the Slack integration under the Avaiable Integrations section and click on Install Integration to start, this will forward you to the Slack Workspace login page. You might be prompted to login to your Slack workspace, if so login and and click Continue . Click on Allow to complete the Slack integration. Once completed you will be forwarded back to the Komodor Integration page where you will find the Slack integration listed under the Installed Integrations . Creating notifications \u00b6 To enable per service or deployment notifications you may either use the Notifications tab in the UI or create them using deployment annotations .","title":"Slack"},{"location":"Integrations/Slack.html#slack-integration","text":"","title":"Slack Integration"},{"location":"Integrations/Slack.html#overview","text":"The Slack integration allows you to recieve notification for events such as successfull or failed deployments, service health state changes, and triggered Workflows from Komodor to a Slack channel. Komodor supports different granularity levels, Cluster, Namespace and Service for each notification. The notification type, destination, and granularity can be defined in the Komodor Notifications screen.","title":"Overview"},{"location":"Integrations/Slack.html#requirements","text":"Depending on your companies Slack settings a company admin may be required to enable the integration, the same user will need to be a Komodor Admin in order to start the installation from the Komodor integration page. !!! Note The free version of Slack is limited to 10 applications.","title":"Requirements"},{"location":"Integrations/Slack.html#installation","text":"Once logged into the Komodor platform click on the Integrations tab. Locate the Slack integration under the Avaiable Integrations section and click on Install Integration to start, this will forward you to the Slack Workspace login page. You might be prompted to login to your Slack workspace, if so login and and click Continue . Click on Allow to complete the Slack integration. Once completed you will be forwarded back to the Komodor Integration page where you will find the Slack integration listed under the Installed Integrations .","title":"Installation"},{"location":"Integrations/Slack.html#creating-notifications","text":"To enable per service or deployment notifications you may either use the Notifications tab in the UI or create them using deployment annotations .","title":"Creating notifications"},{"location":"Integrations/datadog-webhook.html","text":"Datadog Webhook Integration \u00b6 Komodor-Datadog webhook integration allows Komodor to receive alerts from Datadog Monitors. You will see all alerts in the Komodor Service View. Installation \u00b6 Prerequisites \u00b6 In order for us to connect your services according to Datadog events, the following environment variables should exist on your kubernetes deployments: - DD_ENV should match the environment specified on the Datadog tags - DD_SERVICE should match the service name specified on the Datadog tags Installation Steps \u00b6 Make sure the prerequisites above are met. Locate the Datadog installation tile on Komodor Integrations page. Press Install Integration . Follow the on screen instructions. Confirmation \u00b6 A Datadog Integration tile will be added to the top section under Installed Integrations . Configuring the webhook in Datadog \u00b6 Go to Datadog Webhook Integration Setup Create a + New Webhook Name the webhook komodor Enter the webhook server URL in the URL field: https://app.komodor.com/collector/datadog/webhook Copy the following Payload schema into the Payload field: { \"body\" : \"$EVENT_MSG\" , \"last_updated\" : \"$LAST_UPDATED\" , \"event_type\" : \"$EVENT_TYPE\" , \"title\" : \"$EVENT_TITLE\" , \"date\" : \"$DATE\" , \"org\" : { \"id\" : \"$ORG_ID\" , \"name\" : \"$ORG_NAME\" }, \"id\" : \"$ID\" , \"tags\" : \"$TAGS\" , \"alert\" : { \"id\" : \"$ALERT_ID\" , \"type\" : \"$ALERT_TYPE\" , \"transition\" : \"$ALERT_TRANSITION\" , \"cycleId\" : \"$ALERT_CYCLE_KEY\" , \"priority\" : \"$PRIORITY\" , \"status\" : \"$ALERT_STATUS\" , \"scope\" : \"$ALERT_SCOPE\" , \"query\" : \"$ALERT_QUERY\" , \"metric\" : \"$ALERT_METRIC\" , \"metric_namespace\" : \"$METRIC_NAMESPACE\" }, \"aggregation_key\" : \"$AGGREG_KEY\" , \"link\" : \"$LINK\" , \"snapshot\" : \"$SNAPSHOT\" , \"user\" : \"$USER\" , \"username\" : \"$USERNAME\" , \"email\" : \"$EMAIL\" } Add a Custom Header X-API-KEY with the Api Key found in the Komodor Integration page from the Datadog integration tile at the bottom of the setup modal in a JSON format. { \"X-API-KEY\" : \"YOUR_KOMODOR_API_KEY\" } When completed click on Save . For every monitor you wish to receive alerts from in Komodor. Edit the monitor and add @webhook-komodor at the end of the Monitor message.","title":"Datadog Webhook"},{"location":"Integrations/datadog-webhook.html#datadog-webhook-integration","text":"Komodor-Datadog webhook integration allows Komodor to receive alerts from Datadog Monitors. You will see all alerts in the Komodor Service View.","title":"Datadog Webhook Integration"},{"location":"Integrations/datadog-webhook.html#installation","text":"","title":"Installation"},{"location":"Integrations/datadog-webhook.html#prerequisites","text":"In order for us to connect your services according to Datadog events, the following environment variables should exist on your kubernetes deployments: - DD_ENV should match the environment specified on the Datadog tags - DD_SERVICE should match the service name specified on the Datadog tags","title":"Prerequisites"},{"location":"Integrations/datadog-webhook.html#installation-steps","text":"Make sure the prerequisites above are met. Locate the Datadog installation tile on Komodor Integrations page. Press Install Integration . Follow the on screen instructions.","title":"Installation Steps"},{"location":"Integrations/datadog-webhook.html#confirmation","text":"A Datadog Integration tile will be added to the top section under Installed Integrations .","title":"Confirmation"},{"location":"Integrations/datadog-webhook.html#configuring-the-webhook-in-datadog","text":"Go to Datadog Webhook Integration Setup Create a + New Webhook Name the webhook komodor Enter the webhook server URL in the URL field: https://app.komodor.com/collector/datadog/webhook Copy the following Payload schema into the Payload field: { \"body\" : \"$EVENT_MSG\" , \"last_updated\" : \"$LAST_UPDATED\" , \"event_type\" : \"$EVENT_TYPE\" , \"title\" : \"$EVENT_TITLE\" , \"date\" : \"$DATE\" , \"org\" : { \"id\" : \"$ORG_ID\" , \"name\" : \"$ORG_NAME\" }, \"id\" : \"$ID\" , \"tags\" : \"$TAGS\" , \"alert\" : { \"id\" : \"$ALERT_ID\" , \"type\" : \"$ALERT_TYPE\" , \"transition\" : \"$ALERT_TRANSITION\" , \"cycleId\" : \"$ALERT_CYCLE_KEY\" , \"priority\" : \"$PRIORITY\" , \"status\" : \"$ALERT_STATUS\" , \"scope\" : \"$ALERT_SCOPE\" , \"query\" : \"$ALERT_QUERY\" , \"metric\" : \"$ALERT_METRIC\" , \"metric_namespace\" : \"$METRIC_NAMESPACE\" }, \"aggregation_key\" : \"$AGGREG_KEY\" , \"link\" : \"$LINK\" , \"snapshot\" : \"$SNAPSHOT\" , \"user\" : \"$USER\" , \"username\" : \"$USERNAME\" , \"email\" : \"$EMAIL\" } Add a Custom Header X-API-KEY with the Api Key found in the Komodor Integration page from the Datadog integration tile at the bottom of the setup modal in a JSON format. { \"X-API-KEY\" : \"YOUR_KOMODOR_API_KEY\" } When completed click on Save . For every monitor you wish to receive alerts from in Komodor. Edit the monitor and add @webhook-komodor at the end of the Monitor message.","title":"Configuring the webhook in Datadog"},{"location":"Integrations/github.html","text":"ToDo - add github docs","title":"Github"},{"location":"Integrations/gitlab.html","text":"ToDo - create gitlab docs","title":"Gitlab"},{"location":"Integrations/Okta/Okta.html","text":"Okta Integration \u00b6 Use Okta's deep, pre-built integrations to securely connect to Komodor. Note: Only Okta administrators can add the Komodor application, if you aren't an Okta administrator, please contact your Okta administrator to have the application added. Follow these steps to integrate Komodor with Okta: Go to Okta Admin -> Applications -> Browse App Catalog Search for \"Komodor\". Then click 'Add'. Enter any application label you want in 'Application Label'. This is for internal use only and will also be the nickname for the Application. Go to application -> 'Sign On' tab -> 'Settings' and click 'Edit'. In 'Advanced Sign-on settings' enter variable of 'Account Name' and pass this variable to us, for example: 'Komodorio'. Please change application username format to Email. **NOTE once you defined 'companyName' it can't be changed Click 'View Setup Instructions'. Send the 'Metadata URL' and Account Name variable to support@komodor.com to complete the Okta setup. Once Customer Success has completed the setup you can begin to use Okta for SSO.","title":"Okta"},{"location":"Integrations/Okta/Okta.html#okta-integration","text":"Use Okta's deep, pre-built integrations to securely connect to Komodor. Note: Only Okta administrators can add the Komodor application, if you aren't an Okta administrator, please contact your Okta administrator to have the application added. Follow these steps to integrate Komodor with Okta: Go to Okta Admin -> Applications -> Browse App Catalog Search for \"Komodor\". Then click 'Add'. Enter any application label you want in 'Application Label'. This is for internal use only and will also be the nickname for the Application. Go to application -> 'Sign On' tab -> 'Settings' and click 'Edit'. In 'Advanced Sign-on settings' enter variable of 'Account Name' and pass this variable to us, for example: 'Komodorio'. Please change application username format to Email. **NOTE once you defined 'companyName' it can't be changed Click 'View Setup Instructions'. Send the 'Metadata URL' and Account Name variable to support@komodor.com to complete the Okta setup. Once Customer Success has completed the setup you can begin to use Okta for SSO.","title":"Okta Integration"},{"location":"Learn/index.html","text":"","title":"Index"},{"location":"Learn/Annotations.html","text":"Komodor kubernetes annotations \u00b6 Komodor annotations (AKA Komodor as Code), is a method to allow users to configure everything related to Komodor as part of their native k8s yaml. Komodor annotations should be placed in the deployment resource annotations (annotations set on the pod template are ignored) CI-Deploy Links \u00b6 For each deployment version, you can add a quick link with the job url. How \u00b6 app.komodor.com/deploy.job.name:url Example: Annotation Values Description Example app.komodor.com/deploy.job.jenkins url Link to Jenkins job that deploys the service https://ci.jenkins-ci.org/computer/job Deploy Links \u00b6 For each deployment version, you can add a quick link with the relevant filters already in place! How \u00b6 app.komodor.com/deploy.link.name:url Examples: Annotation Values Description Example app.komodor.com/deploy.link.logs url Link for the specific version logs https://app.logz.io/#/dashboard/kibana/discover?_a=env:123.0.1 app.komodor.com/deploy.link.sentry url Link for the specific version Sentry issues https://sentry.io/organizations/rookoutz/issues/?project=1320440&query=sdk.version%3A1.0.1&statsPeriod=14d Custom Links \u00b6 You can create custom links to external and internal applications by crafting your own URL to the application using a skeleton URL and placeholders provided by Komodor. Just copy the URL of the application you want to link to, identify the placeholders in the URL that are used to query the application, and replace them with placeholders for your own use. Please find the below examples as references for common applications. How \u00b6 app.komodor.com/deploy.link.name:value Examples: Annotation Values Description Example app.komodor.com/deploy.link.coralogix url Link for the custom URL, coralogix https://komodortest.coralogix.com/#/query-new/logs?query=(coralogix.metadata.cluster:(%22${cluster}%22))%20AND%20(coralogix.metadata.namespace:(%22${namespace}%22))%20AND%20(coralogix.metadata.service:(%22${service}%22))&time=from:${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS},to:${timestampEnd=yyyy-MM-dd'T'HH:mm:ss.SSS} app.komodor.com/deploy.link.logzio url Link for the custom URL, logz.io https://app.logz.io/#/dashboard/kibana/discover?_a=(columns:!(message,kubernetes.namespace_name,kubernetes.container_name,params.clusterName),filters:!(('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:kubernetes.namespace_name,negate:!f,params:(query:default),type:phrase),query:(match_phrase:(kubernetes.namespace_name:${namespace}))),('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:params.clusterName,negate:!f,params:(query:main),type:phrase),query:(match_phrase:(params.clusterName:${cluster}))),('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:kubernetes.container_name,negate:!f,params:(query:k8s-events-collector),type:phrase),query:(match_phrase:(kubernetes.container_name:${service})),query:(match_phrase:(kubernetes.container_image:${container[web].image})))),index:'logzioCustomerIndex',interval:auto,query:(language:lucene,query:''),sort:!(!('@timestamp',desc)))&_g=(filters:!(),refreshInterval:(pause:!t,value:0),time:(from:'${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS}',to:'${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS}'))&discoverTab=logz-logs-tab&switchToAccountId=138828&accountIds=true app.komodor.com/deploy.link.datadog url Link for the custom URL, DataDog https://app.datadoghq.com/apm/traces?query=service%3A${service}%20kube_namespace%3A${namespace}%20env%3A${cluster}&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code&historicalData=true&messageDisplay=inline&sort=desc&streamTraces=true&start=${epochStart}&end=${epochEnd}&paused=true The following values can be used to enrich the URL: Placeholder Value Example ${epochStart} * Start Time in Epoch Time ${epochEnd} * End Time in Epoch Time ${service} Service Name ${namespace} Namespace Name ${cluster} Cluster Name ${failedPod} * The pod name of a failed pod that triggered this health event* ${container[<name>].image} * Image name of a container ${container[web].image} ${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS} * Start Time in custom format** ${timestampStart=yyyy-MM-dd} ${timestampEnd=yyyy-MM-dd'T'HH:mm:ss.SSS} * End Time in custom format** ${timestampEnd=yyyy-MM-dd} ${yaml[<spec_path>]} Full yaml's path specification ${yaml[metadata.labels.app]} *Not applicable in Service context. Example on how to use YAML full path: spec : replicas : 5 selector : matchLabels : app : nginx template : spec : containers : - name : test image : nginx:1.14.2 ports : - containerPort : 80 - name : test2 image : nginx:1.14.2 ports : - containerPort : 80 YAML Path Value Explanation ${yaml[spec.replicas]} 5 full path usage ${yaml[spec.template.spec.containers[0].name]} test full path usage using path index ${yaml[spec.my_replicas]} undefined path doesn't exist ${yaml[spec.template.spec.containers]} undefined path doesn't resolve to an actual value *Custom links with a failed pod name will be created on health events only. **Dates can be crafted using the display guidelines of date-fns https://date-fns.org/v2.25.0/docs/format Full example \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : annotation-example annotations : app.komodor.com/service.link.grafana-overall-system-health : \"https://grafana.com/service/annotation-example\" app.komodor.com/service.link.datadog : \"https://datadog.com/dashboard/annotation-example\" app.komodor.com/service.link.playbook : \"https://docs.google.com/playbook\" app.komodor.com/deploy.job.jenkins : \"https://ci.jenkins-ci.org/computer/job\" app.komodor.com/deploy.link.logs : \"https://app.logz.io/#/dashboard/kibana/discover?_a=env:1.0.1\" app.komodor.com/deploy.link.sentry : \"https://sentry.io/organizations/rookoutz/issues/?project=1320440&query=sdk.version%3A1.0.1&statsPeriod=14d\" app.komodor.com/service.link.datadog : \"https://app.datadoghq.com/apm/traces?query=service%3A${service}%20kube_namespace%3A${namespace}%20env%3A${cluster}&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code&historicalData=true&messageDisplay=inline&sort=desc&streamTraces=true&start=${epochStart}&end=${epochEnd}&paused=true\" spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0.1 ports : - containerPort : 8080 protocol : TCP Annotations Best Practices \u00b6 At Komodor we believe that k8s annotations are the best method for describing services metadata. This includes all the \u201cextra\u201d fields used to tag and label your services, both for other team members and for external tools. BTW, We collect data from both annotations and labels. Where does Komodor utilize annotations? \u00b6 Everywhere! Komodor will use these annotations to create powerful connections between services and enrich service information in the following areas: Services explorer Related services Events screen Matching alerts to the correct services Official Kubernetes recommendations \u00b6 app.kubernetes.io/component : database app.kubernetes.io/part-of : wordpress app.kubernetes.io/managed-by : helm Komodor recommendations \u00b6 app.komodor.com/label.team : backend app.komodor.com/label.group : infrastructure app.komodor.com/label.owners : \"infa-team\" app.komodor.com/label.alert-team : \"devs\" app.komodor.com/label.Impacted-by : redis Usage example \u00b6 Tagging Team annotations on relevant services and adding relevant metadata on the alert metadata in datadog. Using the Team name in the alert tools (for example PagerDuty) as part of the Komodor labels.","title":"Annotations"},{"location":"Learn/Annotations.html#komodor-kubernetes-annotations","text":"Komodor annotations (AKA Komodor as Code), is a method to allow users to configure everything related to Komodor as part of their native k8s yaml. Komodor annotations should be placed in the deployment resource annotations (annotations set on the pod template are ignored)","title":"Komodor kubernetes annotations"},{"location":"Learn/Annotations.html#ci-deploy-links","text":"For each deployment version, you can add a quick link with the job url.","title":"CI-Deploy Links"},{"location":"Learn/Annotations.html#how","text":"app.komodor.com/deploy.job.name:url Example: Annotation Values Description Example app.komodor.com/deploy.job.jenkins url Link to Jenkins job that deploys the service https://ci.jenkins-ci.org/computer/job","title":"How"},{"location":"Learn/Annotations.html#deploy-links","text":"For each deployment version, you can add a quick link with the relevant filters already in place!","title":"Deploy Links"},{"location":"Learn/Annotations.html#how_1","text":"app.komodor.com/deploy.link.name:url Examples: Annotation Values Description Example app.komodor.com/deploy.link.logs url Link for the specific version logs https://app.logz.io/#/dashboard/kibana/discover?_a=env:123.0.1 app.komodor.com/deploy.link.sentry url Link for the specific version Sentry issues https://sentry.io/organizations/rookoutz/issues/?project=1320440&query=sdk.version%3A1.0.1&statsPeriod=14d","title":"How"},{"location":"Learn/Annotations.html#custom-links","text":"You can create custom links to external and internal applications by crafting your own URL to the application using a skeleton URL and placeholders provided by Komodor. Just copy the URL of the application you want to link to, identify the placeholders in the URL that are used to query the application, and replace them with placeholders for your own use. Please find the below examples as references for common applications.","title":"Custom Links"},{"location":"Learn/Annotations.html#how_2","text":"app.komodor.com/deploy.link.name:value Examples: Annotation Values Description Example app.komodor.com/deploy.link.coralogix url Link for the custom URL, coralogix https://komodortest.coralogix.com/#/query-new/logs?query=(coralogix.metadata.cluster:(%22${cluster}%22))%20AND%20(coralogix.metadata.namespace:(%22${namespace}%22))%20AND%20(coralogix.metadata.service:(%22${service}%22))&time=from:${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS},to:${timestampEnd=yyyy-MM-dd'T'HH:mm:ss.SSS} app.komodor.com/deploy.link.logzio url Link for the custom URL, logz.io https://app.logz.io/#/dashboard/kibana/discover?_a=(columns:!(message,kubernetes.namespace_name,kubernetes.container_name,params.clusterName),filters:!(('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:kubernetes.namespace_name,negate:!f,params:(query:default),type:phrase),query:(match_phrase:(kubernetes.namespace_name:${namespace}))),('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:params.clusterName,negate:!f,params:(query:main),type:phrase),query:(match_phrase:(params.clusterName:${cluster}))),('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:kubernetes.container_name,negate:!f,params:(query:k8s-events-collector),type:phrase),query:(match_phrase:(kubernetes.container_name:${service})),query:(match_phrase:(kubernetes.container_image:${container[web].image})))),index:'logzioCustomerIndex',interval:auto,query:(language:lucene,query:''),sort:!(!('@timestamp',desc)))&_g=(filters:!(),refreshInterval:(pause:!t,value:0),time:(from:'${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS}',to:'${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS}'))&discoverTab=logz-logs-tab&switchToAccountId=138828&accountIds=true app.komodor.com/deploy.link.datadog url Link for the custom URL, DataDog https://app.datadoghq.com/apm/traces?query=service%3A${service}%20kube_namespace%3A${namespace}%20env%3A${cluster}&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code&historicalData=true&messageDisplay=inline&sort=desc&streamTraces=true&start=${epochStart}&end=${epochEnd}&paused=true The following values can be used to enrich the URL: Placeholder Value Example ${epochStart} * Start Time in Epoch Time ${epochEnd} * End Time in Epoch Time ${service} Service Name ${namespace} Namespace Name ${cluster} Cluster Name ${failedPod} * The pod name of a failed pod that triggered this health event* ${container[<name>].image} * Image name of a container ${container[web].image} ${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS} * Start Time in custom format** ${timestampStart=yyyy-MM-dd} ${timestampEnd=yyyy-MM-dd'T'HH:mm:ss.SSS} * End Time in custom format** ${timestampEnd=yyyy-MM-dd} ${yaml[<spec_path>]} Full yaml's path specification ${yaml[metadata.labels.app]} *Not applicable in Service context. Example on how to use YAML full path: spec : replicas : 5 selector : matchLabels : app : nginx template : spec : containers : - name : test image : nginx:1.14.2 ports : - containerPort : 80 - name : test2 image : nginx:1.14.2 ports : - containerPort : 80 YAML Path Value Explanation ${yaml[spec.replicas]} 5 full path usage ${yaml[spec.template.spec.containers[0].name]} test full path usage using path index ${yaml[spec.my_replicas]} undefined path doesn't exist ${yaml[spec.template.spec.containers]} undefined path doesn't resolve to an actual value *Custom links with a failed pod name will be created on health events only. **Dates can be crafted using the display guidelines of date-fns https://date-fns.org/v2.25.0/docs/format","title":"How"},{"location":"Learn/Annotations.html#full-example","text":"apiVersion : apps/v1 kind : Deployment metadata : name : annotation-example annotations : app.komodor.com/service.link.grafana-overall-system-health : \"https://grafana.com/service/annotation-example\" app.komodor.com/service.link.datadog : \"https://datadog.com/dashboard/annotation-example\" app.komodor.com/service.link.playbook : \"https://docs.google.com/playbook\" app.komodor.com/deploy.job.jenkins : \"https://ci.jenkins-ci.org/computer/job\" app.komodor.com/deploy.link.logs : \"https://app.logz.io/#/dashboard/kibana/discover?_a=env:1.0.1\" app.komodor.com/deploy.link.sentry : \"https://sentry.io/organizations/rookoutz/issues/?project=1320440&query=sdk.version%3A1.0.1&statsPeriod=14d\" app.komodor.com/service.link.datadog : \"https://app.datadoghq.com/apm/traces?query=service%3A${service}%20kube_namespace%3A${namespace}%20env%3A${cluster}&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code&historicalData=true&messageDisplay=inline&sort=desc&streamTraces=true&start=${epochStart}&end=${epochEnd}&paused=true\" spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0.1 ports : - containerPort : 8080 protocol : TCP","title":"Full example"},{"location":"Learn/Annotations.html#annotations-best-practices","text":"At Komodor we believe that k8s annotations are the best method for describing services metadata. This includes all the \u201cextra\u201d fields used to tag and label your services, both for other team members and for external tools. BTW, We collect data from both annotations and labels.","title":"Annotations Best Practices"},{"location":"Learn/Annotations.html#where-does-komodor-utilize-annotations","text":"Everywhere! Komodor will use these annotations to create powerful connections between services and enrich service information in the following areas: Services explorer Related services Events screen Matching alerts to the correct services","title":"Where does Komodor utilize annotations?"},{"location":"Learn/Annotations.html#official-kubernetes-recommendations","text":"app.kubernetes.io/component : database app.kubernetes.io/part-of : wordpress app.kubernetes.io/managed-by : helm","title":"Official Kubernetes recommendations"},{"location":"Learn/Annotations.html#komodor-recommendations","text":"app.komodor.com/label.team : backend app.komodor.com/label.group : infrastructure app.komodor.com/label.owners : \"infa-team\" app.komodor.com/label.alert-team : \"devs\" app.komodor.com/label.Impacted-by : redis","title":"Komodor recommendations"},{"location":"Learn/Annotations.html#usage-example","text":"Tagging Team annotations on relevant services and adding relevant metadata on the alert metadata in datadog. Using the Team name in the alert tools (for example PagerDuty) as part of the Komodor labels.","title":"Usage example"},{"location":"Learn/Interaction-With-The-Cluster.html","text":"Interaction with the Cluster \u00b6 Interaction with the cluster allows you to speed up the troubleshooting process. This is done by asking Komodor's agent to perform actions in the cluster. Prerequisites \u00b6 Install Komodor's watcher (version >=0.1.44 ) --set watcher.enableAgentTaskExecution=true to start the agent with the feature turned on (required for Describe Action ) Extra Permissions Required \u00b6 In order to get logs from the cluster please use --set watcher.allowReadingPodLogs=true to update the RBAC manifests with the required permissions (required for Pod Log Action ) You can turn any of these flags off at any time to disable the features Upgrade \u00b6 helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher --set watcher.enableAgentTaskExecution = true --set watcher.allowReadingPodLogs = true --reuse-values Live Pods \u00b6 In the service, click on the Pod Status and Logs button. The table shows all the pods that belong to the service based on the pod owner controller. Actions \u00b6 Pod Logs \u00b6 Request logs from one of the pods will stream back the last 100 logs from the pod. When a pod was previously restarted by Kubernetes you can see the logs just before the pod was restarted. Pod Description \u00b6 Request returns the same output as kubectl describe pod [NAME]","title":"Interaction with the Cluster"},{"location":"Learn/Interaction-With-The-Cluster.html#interaction-with-the-cluster","text":"Interaction with the cluster allows you to speed up the troubleshooting process. This is done by asking Komodor's agent to perform actions in the cluster.","title":"Interaction with the Cluster"},{"location":"Learn/Interaction-With-The-Cluster.html#prerequisites","text":"Install Komodor's watcher (version >=0.1.44 ) --set watcher.enableAgentTaskExecution=true to start the agent with the feature turned on (required for Describe Action )","title":"Prerequisites"},{"location":"Learn/Interaction-With-The-Cluster.html#extra-permissions-required","text":"In order to get logs from the cluster please use --set watcher.allowReadingPodLogs=true to update the RBAC manifests with the required permissions (required for Pod Log Action ) You can turn any of these flags off at any time to disable the features","title":"Extra Permissions Required"},{"location":"Learn/Interaction-With-The-Cluster.html#upgrade","text":"helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher --set watcher.enableAgentTaskExecution = true --set watcher.allowReadingPodLogs = true --reuse-values","title":"Upgrade"},{"location":"Learn/Interaction-With-The-Cluster.html#live-pods","text":"In the service, click on the Pod Status and Logs button. The table shows all the pods that belong to the service based on the pod owner controller.","title":"Live Pods"},{"location":"Learn/Interaction-With-The-Cluster.html#actions","text":"","title":"Actions"},{"location":"Learn/Interaction-With-The-Cluster.html#pod-logs","text":"Request logs from one of the pods will stream back the last 100 logs from the pod. When a pod was previously restarted by Kubernetes you can see the logs just before the pod was restarted.","title":"Pod Logs"},{"location":"Learn/Interaction-With-The-Cluster.html#pod-description","text":"Request returns the same output as kubectl describe pod [NAME]","title":"Pod Description"},{"location":"Learn/Komodor-Agent.html","text":"The Komodor Agent \u00b6 Installation \u00b6 Get an API Key \u00b6 The API key can be found in the Integration page . Helm \u00b6 helm repo add komodorio https://helm-charts.komodor.io helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher \\ --set apiKey = YOUR_API_KEY_HERE \\ --set watcher.clusterName = CLUSTER_NAME \\ --set watcher.enableAgentTaskExecution = true \\ --set watcher.allowReadingPodLogs = true Kustomize \u00b6 export KOMOKW_API_KEY = # API KEY Required export KOMOKW_CLUSTER_NAME = # Optional kubectl create ns komodor kubectl apply -n komodor -k https://github.com/komodorio/helm-charts/manifests/overlays/full/?ref = master Permissions \u00b6 The Komodor agent uses the native RBAC model of Kubernetes. All the permissions are listed here: helm kustomize base , kustomize final ARM Support \u00b6 Arm64 image is supported via docker manifest. Advanced Configuration \u00b6 You can configure the agent's functionality using the following configuration file: komodor-k8s-watcher.yaml (assuming the RBAC permissions are satisfied). A more detailed list of the configurable parameters can be found here Data Redaction \u00b6 Learn how to set up data redaction in Komodor Resources \u00b6 By default, the Komodor agent watches the majority of the resources in your cluster ( secrets and events are opt out ) You can enable/disable watching a resource using the following command: Helm: --set watcher.resources.RESOURCE=true/off Kustomize: update the configuration file and the RBAC rule to have get , list and watch permissions Namespaces \u00b6 The Komodor agent watches all the namespaces (by default watchNamespace=all ) To watch a single namespace use the following command: Helm: --set watcher.watchNamespace=NAMESPACE Kustomize: patch the configuration file watchNamespace=NAMESPACE Blacklisting \u00b6 Using namespacesBlacklist you can opt list of namespaces Agent Tasks \u00b6 Agent tasks are used to interact with the cluster on demand, read more about interaction with the cluster here To enable agent tasks (default is off ): Helm: --set watcher.enableAgentTaskExecution=true && --set watcher.allowReadingPodLogs=true Kustomize: The full overlay already has this turned on. If you are building it manually from base , patch the configuration file enableAgentTaskExecution=true and make sure to have RBAC permissions to get and list for pods and pods/log Environment Variables \u00b6 Alternativly, you can pass the configuration as environment variables using the KOMOKW_ prefix and by replacing all the . to _, for the root items the camelcase transforms into underscores as well. For example: # apiKey KOMOKW_API_KEY = 1a2b3c4d5e6f7g7h # watcher.resources.replicaSet KOMOKW_RESOURCES_REPLICASET = false # watcher.watchNamespace KOMOKW_WATCH_NAMESPACE = my-namespace # watcher.collectHistory KOMOKW_COLLECT_HISTORY = true Updating the agent \u00b6 Kustomize \u00b6 kubectl apply -n komodor -k https://github.com/komodorio/helm-charts/manifests/overlays/full/?ref = master Helm \u00b6 helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher --reuse-values Uninstalling \u00b6 Kustomize \u00b6 kubectl delete ns komodor Helm \u00b6 helm uninstall k8s-watcher","title":"Komodor's Agent"},{"location":"Learn/Komodor-Agent.html#the-komodor-agent","text":"","title":"The Komodor Agent"},{"location":"Learn/Komodor-Agent.html#installation","text":"","title":"Installation"},{"location":"Learn/Komodor-Agent.html#get-an-api-key","text":"The API key can be found in the Integration page .","title":"Get an API Key"},{"location":"Learn/Komodor-Agent.html#helm","text":"helm repo add komodorio https://helm-charts.komodor.io helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher \\ --set apiKey = YOUR_API_KEY_HERE \\ --set watcher.clusterName = CLUSTER_NAME \\ --set watcher.enableAgentTaskExecution = true \\ --set watcher.allowReadingPodLogs = true","title":"Helm"},{"location":"Learn/Komodor-Agent.html#kustomize","text":"export KOMOKW_API_KEY = # API KEY Required export KOMOKW_CLUSTER_NAME = # Optional kubectl create ns komodor kubectl apply -n komodor -k https://github.com/komodorio/helm-charts/manifests/overlays/full/?ref = master","title":"Kustomize"},{"location":"Learn/Komodor-Agent.html#permissions","text":"The Komodor agent uses the native RBAC model of Kubernetes. All the permissions are listed here: helm kustomize base , kustomize final","title":"Permissions"},{"location":"Learn/Komodor-Agent.html#arm-support","text":"Arm64 image is supported via docker manifest.","title":"ARM Support"},{"location":"Learn/Komodor-Agent.html#advanced-configuration","text":"You can configure the agent's functionality using the following configuration file: komodor-k8s-watcher.yaml (assuming the RBAC permissions are satisfied). A more detailed list of the configurable parameters can be found here","title":"Advanced Configuration"},{"location":"Learn/Komodor-Agent.html#data-redaction","text":"Learn how to set up data redaction in Komodor","title":"Data Redaction"},{"location":"Learn/Komodor-Agent.html#resources","text":"By default, the Komodor agent watches the majority of the resources in your cluster ( secrets and events are opt out ) You can enable/disable watching a resource using the following command: Helm: --set watcher.resources.RESOURCE=true/off Kustomize: update the configuration file and the RBAC rule to have get , list and watch permissions","title":"Resources"},{"location":"Learn/Komodor-Agent.html#namespaces","text":"The Komodor agent watches all the namespaces (by default watchNamespace=all ) To watch a single namespace use the following command: Helm: --set watcher.watchNamespace=NAMESPACE Kustomize: patch the configuration file watchNamespace=NAMESPACE","title":"Namespaces"},{"location":"Learn/Komodor-Agent.html#blacklisting","text":"Using namespacesBlacklist you can opt list of namespaces","title":"Blacklisting"},{"location":"Learn/Komodor-Agent.html#agent-tasks","text":"Agent tasks are used to interact with the cluster on demand, read more about interaction with the cluster here To enable agent tasks (default is off ): Helm: --set watcher.enableAgentTaskExecution=true && --set watcher.allowReadingPodLogs=true Kustomize: The full overlay already has this turned on. If you are building it manually from base , patch the configuration file enableAgentTaskExecution=true and make sure to have RBAC permissions to get and list for pods and pods/log","title":"Agent Tasks"},{"location":"Learn/Komodor-Agent.html#environment-variables","text":"Alternativly, you can pass the configuration as environment variables using the KOMOKW_ prefix and by replacing all the . to _, for the root items the camelcase transforms into underscores as well. For example: # apiKey KOMOKW_API_KEY = 1a2b3c4d5e6f7g7h # watcher.resources.replicaSet KOMOKW_RESOURCES_REPLICASET = false # watcher.watchNamespace KOMOKW_WATCH_NAMESPACE = my-namespace # watcher.collectHistory KOMOKW_COLLECT_HISTORY = true","title":"Environment Variables"},{"location":"Learn/Komodor-Agent.html#updating-the-agent","text":"","title":"Updating the agent"},{"location":"Learn/Komodor-Agent.html#kustomize_1","text":"kubectl apply -n komodor -k https://github.com/komodorio/helm-charts/manifests/overlays/full/?ref = master","title":"Kustomize"},{"location":"Learn/Komodor-Agent.html#helm_1","text":"helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher --reuse-values","title":"Helm"},{"location":"Learn/Komodor-Agent.html#uninstalling","text":"","title":"Uninstalling"},{"location":"Learn/Komodor-Agent.html#kustomize_2","text":"kubectl delete ns komodor","title":"Kustomize"},{"location":"Learn/Komodor-Agent.html#helm_2","text":"helm uninstall k8s-watcher","title":"Helm"},{"location":"Learn/ManageUsers.html","text":"How to Manage Users \u00b6 This article will detail how to Invite, Modify and Delete users in the Komodor platform. How to Invite/Add a new user \u00b6 To invite another user click on the manage team icon in the top right corner, this will take you to the \"Manage Team\" page. Note: You must have the Admin role in order to invite another Admin to the platform. Click on \"Add Member\". Provide the users Full Name, Email Address and select a role for the user and then click \"Send Invite\" to invite the user to the platform. Note: More on user roles can be found here . The user will receive an invitation to the platform, click \"Close\" to finish. How to modify an existing user \u00b6 Click on the manage user icon in the top right corner, this will take you to the \"Manage Team\" page. You will see a list of users for the account, select \"Edit\" to the far right of the user you wish to modify. In the \"Edit Member\" dialogue, modify the users Full Name or Role and click on \"Save Details\" to save the changes. Note: In order to change a users email address you will need to create a new user by inviting them to the platform using their new email address. How to delete a user \u00b6 Click on the manage user icon in the top right corner, this will take you to the \"Manage Team\" page. You will see a list of users for the account, select \"Edit\" beside the user you wish to delete. On the \"Edit Member\" dialogue, click on the red garbage bin in the top right corner to delete the user. Click on \"Yes, Remove\" to delete the user.","title":"Manage Users"},{"location":"Learn/ManageUsers.html#how-to-manage-users","text":"This article will detail how to Invite, Modify and Delete users in the Komodor platform.","title":"How to Manage Users"},{"location":"Learn/ManageUsers.html#how-to-inviteadd-a-new-user","text":"To invite another user click on the manage team icon in the top right corner, this will take you to the \"Manage Team\" page. Note: You must have the Admin role in order to invite another Admin to the platform. Click on \"Add Member\". Provide the users Full Name, Email Address and select a role for the user and then click \"Send Invite\" to invite the user to the platform. Note: More on user roles can be found here . The user will receive an invitation to the platform, click \"Close\" to finish.","title":"How to Invite/Add a new user"},{"location":"Learn/ManageUsers.html#how-to-modify-an-existing-user","text":"Click on the manage user icon in the top right corner, this will take you to the \"Manage Team\" page. You will see a list of users for the account, select \"Edit\" to the far right of the user you wish to modify. In the \"Edit Member\" dialogue, modify the users Full Name or Role and click on \"Save Details\" to save the changes. Note: In order to change a users email address you will need to create a new user by inviting them to the platform using their new email address.","title":"How to modify an existing user"},{"location":"Learn/ManageUsers.html#how-to-delete-a-user","text":"Click on the manage user icon in the top right corner, this will take you to the \"Manage Team\" page. You will see a list of users for the account, select \"Edit\" beside the user you wish to delete. On the \"Edit Member\" dialogue, click on the red garbage bin in the top right corner to delete the user. Click on \"Yes, Remove\" to delete the user.","title":"How to delete a user"},{"location":"Learn/Playbook.html","text":"Workflows \u00b6 Introduction \u00b6 Once configured, Komodor Workflows are designed to automatically detect different failure scenarios in nodes, PersistentVolumeClaims (PVC) and services such as Loadbalancers, Ingress controllers and Endpoints, and investigate certain aspects around them in order to provide additional information to simplify the troubleshooting process and reduce mean time to resolution. Komodor currently offers two Workflows, the Node Detector and PVC Detector. Node Detector \u00b6 The Node Detector is triggered when a node condition changes to a faulty condition and persists longer then the configured duration (default is 60 seconds). Once triggered, we perform the following checks as part of our investigation Is the node ready? Is the node overcommitted? Is the node under pressure? Is the network available? Is the node schedulable? Are pods being evicted? Are user pods healthy? Are system pods healthy? Node overall resource consumption including top 5 pod consumers (requires metric-server installed) Note The Node Detector currently does not detect nodes in an \"unknown\" state, this means spot instance interruptions or scale-down events will not be handled by the Workflow, this could possibly affect other scenarios as well. The Node detector will only run on nodes that have a creation time greater than 3 minutes. PVC Detector \u00b6 Detects PVCs in a pending state. The PVC Detector is triggered when a PVC is in a pending state for the defined duration (default is 60 seconds). - We perform the following checks as part of our investigation - PVC creation, utilization, and readiness issues - Volume provisioner issues - PVC spec changes - Identify the impact on related services Services Detector \u00b6 (Coming Soon) - Detect unhealthy services and get context on the underlying issue. Resource Detector \u00b6 (Coming Soon) - Detects unhealthy resources such as Ingress controller, endpoints and Loadblancers in an unhealthy state. Failed Deployment Detector \u00b6 (Coming Soon) - Identify failed deployments and get a rich context on the failure reason(s). Settings \u00b6 Sensor : Resources can be monitored at either the cluster scope or namespace(s) scope. Duration : The minimum time the issue has to presist prior to running the Workflow checks, if the problem has been resolved during this duration the Workflow will not run. Default is 60 seconds. Sink/Notification : Where Komodor will send a notifications when a Workflow is triggered, supports either Slack or Microsoft Teams. Supported Actions \u00b6 Pause : Pause a workflow and its notifications. Edit : Change Sensors, Duration, Notifications and Name. Remove : Delete a workflow. How to Create a Workflow \u00b6 Click on the Workflows tab Mouse over the workflow you would like to add and click Add Workflow Click Activate on the top right hand corner Configure the Sensor Pick a cluster Pick a namespace(s) or leave this option blank for all namepsaces on the selected cluster Click on Continue Set the minimum duration in seconds, default is 60 seconds. Choose a notification platform and channel, or choose Continue without notifications Name the workflow and click Activate Workflow How to Pause or Resume a Workflow \u00b6 Click on the workflows tab Under Workflows (#) click the workflow you would like to Pause or Resume Click on More Actions on the top right hand corner and select Pause Workflow To resume the workflow click on More Actions and select Resume Workflow How to Edit a Workflow \u00b6 Click on the workflows tab Under Workflows (#) click the workflow you would like to edit Click on More Actions on the top right hand corner and select Edit Configuration Walk through the workflow How to Remove a Workflow \u00b6 Click on the workflows tab Mouse over the workflow you would like to add and click Add Workflow Click Activate on the top right hand corner Configure the Sensor Pick a cluster Pick a namespace(s) or leave this option blank for all namepsaces on the selected cluster Click on Continue Set the minimum duration in seconds, default is 60 seconds. Choose a notification platform and channel, or choose Continue without notifications Name the workflow and click Activate Workflow","title":"Workflows"},{"location":"Learn/Playbook.html#workflows","text":"","title":"Workflows"},{"location":"Learn/Playbook.html#introduction","text":"Once configured, Komodor Workflows are designed to automatically detect different failure scenarios in nodes, PersistentVolumeClaims (PVC) and services such as Loadbalancers, Ingress controllers and Endpoints, and investigate certain aspects around them in order to provide additional information to simplify the troubleshooting process and reduce mean time to resolution. Komodor currently offers two Workflows, the Node Detector and PVC Detector.","title":"Introduction"},{"location":"Learn/Playbook.html#node-detector","text":"The Node Detector is triggered when a node condition changes to a faulty condition and persists longer then the configured duration (default is 60 seconds). Once triggered, we perform the following checks as part of our investigation Is the node ready? Is the node overcommitted? Is the node under pressure? Is the network available? Is the node schedulable? Are pods being evicted? Are user pods healthy? Are system pods healthy? Node overall resource consumption including top 5 pod consumers (requires metric-server installed) Note The Node Detector currently does not detect nodes in an \"unknown\" state, this means spot instance interruptions or scale-down events will not be handled by the Workflow, this could possibly affect other scenarios as well. The Node detector will only run on nodes that have a creation time greater than 3 minutes.","title":"Node Detector"},{"location":"Learn/Playbook.html#pvc-detector","text":"Detects PVCs in a pending state. The PVC Detector is triggered when a PVC is in a pending state for the defined duration (default is 60 seconds). - We perform the following checks as part of our investigation - PVC creation, utilization, and readiness issues - Volume provisioner issues - PVC spec changes - Identify the impact on related services","title":"PVC Detector"},{"location":"Learn/Playbook.html#services-detector","text":"(Coming Soon) - Detect unhealthy services and get context on the underlying issue.","title":"Services Detector"},{"location":"Learn/Playbook.html#resource-detector","text":"(Coming Soon) - Detects unhealthy resources such as Ingress controller, endpoints and Loadblancers in an unhealthy state.","title":"Resource Detector"},{"location":"Learn/Playbook.html#failed-deployment-detector","text":"(Coming Soon) - Identify failed deployments and get a rich context on the failure reason(s).","title":"Failed Deployment Detector"},{"location":"Learn/Playbook.html#settings","text":"Sensor : Resources can be monitored at either the cluster scope or namespace(s) scope. Duration : The minimum time the issue has to presist prior to running the Workflow checks, if the problem has been resolved during this duration the Workflow will not run. Default is 60 seconds. Sink/Notification : Where Komodor will send a notifications when a Workflow is triggered, supports either Slack or Microsoft Teams.","title":"Settings"},{"location":"Learn/Playbook.html#supported-actions","text":"Pause : Pause a workflow and its notifications. Edit : Change Sensors, Duration, Notifications and Name. Remove : Delete a workflow.","title":"Supported Actions"},{"location":"Learn/Playbook.html#how-to-create-a-workflow","text":"Click on the Workflows tab Mouse over the workflow you would like to add and click Add Workflow Click Activate on the top right hand corner Configure the Sensor Pick a cluster Pick a namespace(s) or leave this option blank for all namepsaces on the selected cluster Click on Continue Set the minimum duration in seconds, default is 60 seconds. Choose a notification platform and channel, or choose Continue without notifications Name the workflow and click Activate Workflow","title":"How to Create a Workflow"},{"location":"Learn/Playbook.html#how-to-pause-or-resume-a-workflow","text":"Click on the workflows tab Under Workflows (#) click the workflow you would like to Pause or Resume Click on More Actions on the top right hand corner and select Pause Workflow To resume the workflow click on More Actions and select Resume Workflow","title":"How to Pause or Resume a Workflow"},{"location":"Learn/Playbook.html#how-to-edit-a-workflow","text":"Click on the workflows tab Under Workflows (#) click the workflow you would like to edit Click on More Actions on the top right hand corner and select Edit Configuration Walk through the workflow","title":"How to Edit a Workflow"},{"location":"Learn/Playbook.html#how-to-remove-a-workflow","text":"Click on the workflows tab Mouse over the workflow you would like to add and click Add Workflow Click Activate on the top right hand corner Configure the Sensor Pick a cluster Pick a namespace(s) or leave this option blank for all namepsaces on the selected cluster Click on Continue Set the minimum duration in seconds, default is 60 seconds. Choose a notification platform and channel, or choose Continue without notifications Name the workflow and click Activate Workflow","title":"How to Remove a Workflow"},{"location":"Learn/RBAC.html","text":"Role-Based Access Control \u00b6 Intro \u00b6 Roles categorize users and define what permissions those users have, such as what data they can read or what they can modify. Komodor Roles: \u00b6 Permissions Admin User View all features \u2713 \u2713 Invite Admin member \u2713 \u2718 Invite User member \u2713 \u2713 Edit users' name and rule. \u2713 \u2718 Delete User and Admin members \u2713 \u2718 Add a new integration \u2713 \u2718 See API keys in the Integrations page \u2713 \u2718 Note An account can have multiple admins","title":"RBAC"},{"location":"Learn/RBAC.html#role-based-access-control","text":"","title":"Role-Based Access Control"},{"location":"Learn/RBAC.html#intro","text":"Roles categorize users and define what permissions those users have, such as what data they can read or what they can modify.","title":"Intro"},{"location":"Learn/RBAC.html#komodor-roles","text":"Permissions Admin User View all features \u2713 \u2713 Invite Admin member \u2713 \u2718 Invite User member \u2713 \u2713 Edit users' name and rule. \u2713 \u2718 Delete User and Admin members \u2713 \u2718 Add a new integration \u2713 \u2718 See API keys in the Integrations page \u2713 \u2718 Note An account can have multiple admins","title":"Komodor Roles:"},{"location":"Learn/Sensitive-Information-Redaction.html","text":"Sensitive data redaction in Komodor\u2019s k8s-watcher \u00b6 What is it \u00b6 It\u2019s likely that there are values you don\u2019t want to send to Komodor as plain text. Kubernetes Secrets, for instance, ConfigMap sensitive values or container environment variables. When configured - we will redact the specific value. That way Komodor won't see any sensitive data while you will still see configuration diff. How to integrate \u00b6 Inside komodor-k8s-watcher.yaml you should add a list of string or regular expressions under redact key as such: komodor-k8s-watcher watchNamespace : all namespacesBlacklist : - kube-system redact : - \"PG_.*\" - \".*PASSWORD.*\" nameBlacklist : [ \"leader\" , \"election\" ] collectHistory : false How to integarate using helm upgrade command \u00b6 helm upgrade --install k8s-watcher komodorio/k8s-watcher --set watcher.redact = \"{.*PASSWORD.*,.*password.*,.*KEY.*,.*key.*,.*SECRET.*,.*secret.*}\" --set apiKey = <API-KEY> --set watcher.clusterName = <cluster-name> --set watcher.enableAgentTaskExecution = true --set watcher.allowReadingPodLogs = true Secret Resource \u00b6 By default, Komodor\u2019s k8s-watcher is hashing all secrets values. ConfigMap resource \u00b6 You can preconfigure a list of keys for Kubernetes watcher to also redact specific values from ConfigMap. komodor-k8s-watcher.yaml: redact : - \"SENTRY_API_KEY\" - \"PG_.*\" configmap.yaml: apiVersion : v1 kind : ConfigMap metadata : Name : sensitive-config-map data : SENTRY_API_KEY : super_secret PG_SECRET : super_secret PG_USERNAME : super_secret All the above \u201csuper_secret\u201d will be sent has hashed value. Deployment resource \u00b6 Komodor\u2019s k8s-watcher will hash template.spec.template.[containeres|initContainers].env list of variables inside Deployment objects for pre-configured list of keys or list of regular expressions. komodor-k8s-watcher.yaml: redact : - \"SENTRY_API_KEY\" - \"PG_.*\" deployment.yaml: apiVersion : apps/v1 kind : Deployment metadata : name : sensitive-deployment spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 env : - name : PG_USERNAME value : super_secret - name : SECRET value : this_will_show_up In the above deployment example we will not send the secret values for PG_USERNAME. SECRET will show up as is due to the fact it won\u2019t match any string or regex in our configuration.","title":"Sensitive Information Redaction"},{"location":"Learn/Sensitive-Information-Redaction.html#sensitive-data-redaction-in-komodors-k8s-watcher","text":"","title":"Sensitive data redaction in Komodor\u2019s k8s-watcher"},{"location":"Learn/Sensitive-Information-Redaction.html#what-is-it","text":"It\u2019s likely that there are values you don\u2019t want to send to Komodor as plain text. Kubernetes Secrets, for instance, ConfigMap sensitive values or container environment variables. When configured - we will redact the specific value. That way Komodor won't see any sensitive data while you will still see configuration diff.","title":"What is it"},{"location":"Learn/Sensitive-Information-Redaction.html#how-to-integrate","text":"Inside komodor-k8s-watcher.yaml you should add a list of string or regular expressions under redact key as such: komodor-k8s-watcher watchNamespace : all namespacesBlacklist : - kube-system redact : - \"PG_.*\" - \".*PASSWORD.*\" nameBlacklist : [ \"leader\" , \"election\" ] collectHistory : false","title":"How to integrate"},{"location":"Learn/Sensitive-Information-Redaction.html#how-to-integarate-using-helm-upgrade-command","text":"helm upgrade --install k8s-watcher komodorio/k8s-watcher --set watcher.redact = \"{.*PASSWORD.*,.*password.*,.*KEY.*,.*key.*,.*SECRET.*,.*secret.*}\" --set apiKey = <API-KEY> --set watcher.clusterName = <cluster-name> --set watcher.enableAgentTaskExecution = true --set watcher.allowReadingPodLogs = true","title":"How to integarate using helm upgrade command"},{"location":"Learn/Sensitive-Information-Redaction.html#secret-resource","text":"By default, Komodor\u2019s k8s-watcher is hashing all secrets values.","title":"Secret Resource"},{"location":"Learn/Sensitive-Information-Redaction.html#configmap-resource","text":"You can preconfigure a list of keys for Kubernetes watcher to also redact specific values from ConfigMap. komodor-k8s-watcher.yaml: redact : - \"SENTRY_API_KEY\" - \"PG_.*\" configmap.yaml: apiVersion : v1 kind : ConfigMap metadata : Name : sensitive-config-map data : SENTRY_API_KEY : super_secret PG_SECRET : super_secret PG_USERNAME : super_secret All the above \u201csuper_secret\u201d will be sent has hashed value.","title":"ConfigMap resource"},{"location":"Learn/Sensitive-Information-Redaction.html#deployment-resource","text":"Komodor\u2019s k8s-watcher will hash template.spec.template.[containeres|initContainers].env list of variables inside Deployment objects for pre-configured list of keys or list of regular expressions. komodor-k8s-watcher.yaml: redact : - \"SENTRY_API_KEY\" - \"PG_.*\" deployment.yaml: apiVersion : apps/v1 kind : Deployment metadata : name : sensitive-deployment spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 env : - name : PG_USERNAME value : super_secret - name : SECRET value : this_will_show_up In the above deployment example we will not send the secret values for PG_USERNAME. SECRET will show up as is due to the fact it won\u2019t match any string or regex in our configuration.","title":"Deployment resource"},{"location":"Learn/Static-Prevention.html","text":"Static Prevention \u00b6 Introduction \u00b6 Komodor not only provides you with remediation instructions when troubleshooting Kubernetes incidents, but also helps to prevent them from happening in the first place. How it works? \u00b6 Each time a workload is rolled out, and a change has been made to the workload YAML, Komodor runs a set of statical checks in order to improve your YAML\u2019s reliability and efficiency. The results of our scan are presented under the \"Best Practices\u201d section within every service. If you wish to ignore any check, just click on the ignore button under the Best practices pop-up. Checks we run \u00b6 - Deployment Missing Replicas \u00b6 What is checked ? Check if there is only one replica for a deployment . Why should this be checked ? More than one replica recommended to be scheduled . - Tag Not Specified \u00b6 What is checked ? Check if an image tag is either not specified or the latest tag has not been used . Why should this be checked ? Docker 's latest tag is applied by default to images where a tag hasn' t been specified . Not specifying a specific version of an image can lead to a wide variety of problems . - Pull Policy Not Always \u00b6 What is checked ? Check if an image pull policy is not always . Why should this be checked ? By default , an image will be pulled if it isn 't already cached on the node attempting to run it . This can result in variations in images that are running per node , or potentially provide a way to gain access to an image without having direct access to the ImagePullSecret . - Liveness Probe Missing \u00b6 What is checked ? Check if a liveness probe is not configured for a pod . Why should this be checked ? Liveness probes are designed to ensure that an application stays in a healthy state . When a liveness probe fails , the pod will be restarted . - Readiness Probe Missing \u00b6 What is checked ? Check if a readiness probe is not configured for a pod . Why should this be checked ? A readiness probe can ensure that the traffic is not sent to a pod until it is actually ready to receive the traffic . - CPU Requests Missing \u00b6 What is checked ? Check if resources . requests . cpu attribute is not configured . Why should this be checked ? Setting appropriate resource requests will ensure that all your applications have sufficient compute CPU resources . - CPU Limits Missing \u00b6 What is checked ? Check if resources . limits . cpu attribute is not configured . Why should this be checked ? Setting appropriate resource limits will ensure that your applications do not consume too many CPU resources . - Memory Requests Missing \u00b6 What is checked ? Check if resources . requests . memory attribute is not configured . Why should this be checked ? Setting appropriate resource requests will ensure that all your applications have sufficient memory compute resources . - Memory Limits Missing \u00b6 What is checked ? Check if resources . limits . memory attribute is not configured . Why should this be checked ? Setting appropriate resource limits will ensure that your applications do not consume too many memory resources .","title":"Static Prevention"},{"location":"Learn/Static-Prevention.html#static-prevention","text":"","title":"Static Prevention"},{"location":"Learn/Static-Prevention.html#introduction","text":"Komodor not only provides you with remediation instructions when troubleshooting Kubernetes incidents, but also helps to prevent them from happening in the first place.","title":"Introduction"},{"location":"Learn/Static-Prevention.html#how-it-works","text":"Each time a workload is rolled out, and a change has been made to the workload YAML, Komodor runs a set of statical checks in order to improve your YAML\u2019s reliability and efficiency. The results of our scan are presented under the \"Best Practices\u201d section within every service. If you wish to ignore any check, just click on the ignore button under the Best practices pop-up.","title":"How it works?"},{"location":"Learn/Static-Prevention.html#checks-we-run","text":"","title":"Checks we run"},{"location":"Learn/Static-Prevention.html#-deployment-missing-replicas","text":"What is checked ? Check if there is only one replica for a deployment . Why should this be checked ? More than one replica recommended to be scheduled .","title":"- Deployment Missing Replicas"},{"location":"Learn/Static-Prevention.html#-tag-not-specified","text":"What is checked ? Check if an image tag is either not specified or the latest tag has not been used . Why should this be checked ? Docker 's latest tag is applied by default to images where a tag hasn' t been specified . Not specifying a specific version of an image can lead to a wide variety of problems .","title":"- Tag Not Specified"},{"location":"Learn/Static-Prevention.html#-pull-policy-not-always","text":"What is checked ? Check if an image pull policy is not always . Why should this be checked ? By default , an image will be pulled if it isn 't already cached on the node attempting to run it . This can result in variations in images that are running per node , or potentially provide a way to gain access to an image without having direct access to the ImagePullSecret .","title":"- Pull Policy Not Always"},{"location":"Learn/Static-Prevention.html#-liveness-probe-missing","text":"What is checked ? Check if a liveness probe is not configured for a pod . Why should this be checked ? Liveness probes are designed to ensure that an application stays in a healthy state . When a liveness probe fails , the pod will be restarted .","title":"- Liveness Probe Missing"},{"location":"Learn/Static-Prevention.html#-readiness-probe-missing","text":"What is checked ? Check if a readiness probe is not configured for a pod . Why should this be checked ? A readiness probe can ensure that the traffic is not sent to a pod until it is actually ready to receive the traffic .","title":"- Readiness Probe Missing"},{"location":"Learn/Static-Prevention.html#-cpu-requests-missing","text":"What is checked ? Check if resources . requests . cpu attribute is not configured . Why should this be checked ? Setting appropriate resource requests will ensure that all your applications have sufficient compute CPU resources .","title":"- CPU Requests Missing"},{"location":"Learn/Static-Prevention.html#-cpu-limits-missing","text":"What is checked ? Check if resources . limits . cpu attribute is not configured . Why should this be checked ? Setting appropriate resource limits will ensure that your applications do not consume too many CPU resources .","title":"- CPU Limits Missing"},{"location":"Learn/Static-Prevention.html#-memory-requests-missing","text":"What is checked ? Check if resources . requests . memory attribute is not configured . Why should this be checked ? Setting appropriate resource requests will ensure that all your applications have sufficient memory compute resources .","title":"- Memory Requests Missing"},{"location":"Learn/Static-Prevention.html#-memory-limits-missing","text":"What is checked ? Check if resources . limits . memory attribute is not configured . Why should this be checked ? Setting appropriate resource limits will ensure that your applications do not consume too many memory resources .","title":"- Memory Limits Missing"},{"location":"Learn/config-changes.html","text":"Config Change API Integration \u00b6 Config change API allows users to send changes in their config (from internal tools and infrastructure), and see them as part of the Komodor Service view. How to use \u00b6 Request URL \u00b6 Mandatory query params will be used for service selection: serviceName namespace clusterName URL example https://api.komodor.com/v0/config_change?serviceName=backend-service&namespace=default&clusterName=production\" Authentication \u00b6 To authenticate the request use API Key on your \"REST API\" integration tile in the Komodor app and add it to a header with X-API-KEY name. The REST API key can be found in the Integration page . If REST API integration isn't available for your account, please contact your account manager in Komodor. Body \u00b6 This is the event itself with the relevant configuration you want to be connected to the service as JSON. { key1: value1, key2: value2\u2026 } Config map and Secrets \u00b6 Configmap and Secrets can be shown in events tab, please contact us if you want this option. Configmaps that include the coming words will be ignored: \"istio\" \"cluster-autoscaler-status\" Full Example \u00b6 curl -H \"X-API-KEY: <rest api key>\" -H \"Content-Type: application/json\" -d '{\"key\":\"value\"}' \"https://api.komodor.com/v0/config_change?serviceName=backend-service&namespace=default&clusterName=production\"","title":"Config Changes"},{"location":"Learn/config-changes.html#config-change-api-integration","text":"Config change API allows users to send changes in their config (from internal tools and infrastructure), and see them as part of the Komodor Service view.","title":"Config Change API Integration"},{"location":"Learn/config-changes.html#how-to-use","text":"","title":"How to use"},{"location":"Learn/config-changes.html#request-url","text":"Mandatory query params will be used for service selection: serviceName namespace clusterName URL example https://api.komodor.com/v0/config_change?serviceName=backend-service&namespace=default&clusterName=production\"","title":"Request URL"},{"location":"Learn/config-changes.html#authentication","text":"To authenticate the request use API Key on your \"REST API\" integration tile in the Komodor app and add it to a header with X-API-KEY name. The REST API key can be found in the Integration page . If REST API integration isn't available for your account, please contact your account manager in Komodor.","title":"Authentication"},{"location":"Learn/config-changes.html#body","text":"This is the event itself with the relevant configuration you want to be connected to the service as JSON. { key1: value1, key2: value2\u2026 }","title":"Body"},{"location":"Learn/config-changes.html#config-map-and-secrets","text":"Configmap and Secrets can be shown in events tab, please contact us if you want this option. Configmaps that include the coming words will be ignored: \"istio\" \"cluster-autoscaler-status\"","title":"Config map and Secrets"},{"location":"Learn/config-changes.html#full-example","text":"curl -H \"X-API-KEY: <rest api key>\" -H \"Content-Type: application/json\" -d '{\"key\":\"value\"}' \"https://api.komodor.com/v0/config_change?serviceName=backend-service&namespace=default&clusterName=production\"","title":"Full Example"},{"location":"Learn/komodor-source-control.html","text":"Komodor source control support \u00b6 What is it \u00b6 Enrich services with relevant source control metadata. This allows to show smart-diffs as part of Komodor service changes tracking. Doing so connects the specific service with repositories that might not be the original codebase, such as infrastructure or CI repos, which are still relevant changes. How to integrate \u00b6 Add the following annotations to your service's Kubernetes spec: app.komodor.com/[name] : FULL_REPO_URL app.komodor.com/[name].ref : SOURCE_CONTROL_REF app.komodor.com/[another-name] : ANOTHER_FULL_REPO_URL app.komodor.com/[another-name].ref : ANOTHER_ SOURCE_CONTROL_REF Full example \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : git-example annotations : app.komodor.com/app : https://github.com/komodorio/infra app.komodor.com/app.ref : 3350e3311f9520fe5e237e2d71f339029ee051d8 app.komodor.com/infra : https://github.com/komodorio/helm-charts app.komodor.com/infra.ref : 32b355df32713afc511528d909eff296e91dbe74 spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 ports : - containerPort : 8080 protocol : TCP Microsoft DevOps Pipelines Example \u00b6 Microsoft DevOps Pipelines provides predefined variables that contains all the data needed to configure the source control annotations. Modify your pipeline to use these variables when templating the Kubernetes manifests. annotations : app.komodor.com/app : $(Build.Repository.Uri) app.komodor.com/app.ref : $(Build.SourceVersion) Tracked Files \u00b6 Once the integration with Github is established, Komodor will scan the pull requests files (only the names), to see if there are common, interesting files that have been updated. For example, when there was a change in Dockerfile , Komodor will show that in the Deploy summary. To customize which files will be tracked by Komodor. Using Kubernetes annotation app.komodor.com/tracked_files that accepts multiline string (gitignore like) to specify the files. For example, track any yaml files: apiVersion : apps/v1 kind : Deployment metadata : name : git-example annotations : app.komodor.com/tracked_files : | *.yaml app.komodor.com/app : https://github.com/komodorio/infra app.komodor.com/app.ref : 3350e3311f9520fe5e237e2d71f339029ee051d8 app.komodor.com/infra : https://github.com/komodorio/helm-charts app.komodor.com/infra.ref : 32b355df32713afc511528d909eff296e91dbe74 spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 ports : - containerPort : 8080 protocol : TCP\"","title":"Source Control"},{"location":"Learn/komodor-source-control.html#komodor-source-control-support","text":"","title":"Komodor source control support"},{"location":"Learn/komodor-source-control.html#what-is-it","text":"Enrich services with relevant source control metadata. This allows to show smart-diffs as part of Komodor service changes tracking. Doing so connects the specific service with repositories that might not be the original codebase, such as infrastructure or CI repos, which are still relevant changes.","title":"What is it"},{"location":"Learn/komodor-source-control.html#how-to-integrate","text":"Add the following annotations to your service's Kubernetes spec: app.komodor.com/[name] : FULL_REPO_URL app.komodor.com/[name].ref : SOURCE_CONTROL_REF app.komodor.com/[another-name] : ANOTHER_FULL_REPO_URL app.komodor.com/[another-name].ref : ANOTHER_ SOURCE_CONTROL_REF","title":"How to integrate"},{"location":"Learn/komodor-source-control.html#full-example","text":"apiVersion : apps/v1 kind : Deployment metadata : name : git-example annotations : app.komodor.com/app : https://github.com/komodorio/infra app.komodor.com/app.ref : 3350e3311f9520fe5e237e2d71f339029ee051d8 app.komodor.com/infra : https://github.com/komodorio/helm-charts app.komodor.com/infra.ref : 32b355df32713afc511528d909eff296e91dbe74 spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 ports : - containerPort : 8080 protocol : TCP","title":"Full example"},{"location":"Learn/komodor-source-control.html#microsoft-devops-pipelines-example","text":"Microsoft DevOps Pipelines provides predefined variables that contains all the data needed to configure the source control annotations. Modify your pipeline to use these variables when templating the Kubernetes manifests. annotations : app.komodor.com/app : $(Build.Repository.Uri) app.komodor.com/app.ref : $(Build.SourceVersion)","title":"Microsoft DevOps Pipelines Example"},{"location":"Learn/komodor-source-control.html#tracked-files","text":"Once the integration with Github is established, Komodor will scan the pull requests files (only the names), to see if there are common, interesting files that have been updated. For example, when there was a change in Dockerfile , Komodor will show that in the Deploy summary. To customize which files will be tracked by Komodor. Using Kubernetes annotation app.komodor.com/tracked_files that accepts multiline string (gitignore like) to specify the files. For example, track any yaml files: apiVersion : apps/v1 kind : Deployment metadata : name : git-example annotations : app.komodor.com/tracked_files : | *.yaml app.komodor.com/app : https://github.com/komodorio/infra app.komodor.com/app.ref : 3350e3311f9520fe5e237e2d71f339029ee051d8 app.komodor.com/infra : https://github.com/komodorio/helm-charts app.komodor.com/infra.ref : 32b355df32713afc511528d909eff296e91dbe74 spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 ports : - containerPort : 8080 protocol : TCP\"","title":"Tracked Files"}]}