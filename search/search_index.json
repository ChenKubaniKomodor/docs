{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"","title":"Home"},{"location":"getting-started.html","text":"Getting Started \u00b6","title":"Getting Started"},{"location":"getting-started.html#getting-started","text":"","title":"Getting Started"},{"location":"Integrations/index.html","text":"","title":"Index"},{"location":"Integrations/AlertManager.html","text":"Prometheus/Grafana Alert Manager Integration \u00b6 The Integration of Grafana / Prometheus AlertManager allows alerts triggered by AlertManager to appear on Komodor's timelines. Installation \u00b6 The Alert Manager integration involves 3 steps: \u00b6 Enabling the integration in Komodor. Creating webhook on the alert manager / Grafana. Adding labels to the alert. 1. Enabling the integration in Komodor \u00b6 To enable the Komodor Prometheus Alert Manager integration go to Komodor integrations page , select Prometheus/Grafana Alert Manager, and install the integration. 2. (Option A) The following steps are for manual configuration on alert manager \u00b6 Creating webhook \u00b6 Open your alertmanager.yml configuration file Add a receiver to your receivers list, name the receiver komodor and attach a sink to webhook_configs . In the url field, put the URL that was provided to you during the integration setup. Also set the field send_resolved to true . receivers : - name : komodor webhook_configs : - url : \"<URL_FROM_KOMODOR>\" send_resolved : true Next, in alertmanager.yml , configure a route so that your alert is routed to komodor route : receiver : komodor If you already have configured routes you can config multiple as follows: routes : - match : severity : critical receiver : pagerduty - match : receiver : komodor A note about the continue configuration of AlertManager routing rules . If it is set to false, AlertManager sends the alert to the first matching route and stops. continue default value is false. A full YAML configuration example: global : group_interval : 5m repeat_interval : 12h routes : - match : receiver : komodor receivers : - name : komodor webhook_configs : - url : \"<URL_FROM_KOMODOR>\" send_resolved : true 2. (Option B) Configure alerts from Grafana \u00b6 Go to Alerting -> Contact points . Click New contact point . Enter in name komodor , in Contact point type select Webhook and in Url insert the URL from the UI. Click Alerting -> Notification policies . In the notification policies, configure komodor contact endpoint as you wish to configure alerts to Komodor. 3. Adding labels to the alert. \u00b6 To relate the alert to the relevant workload and make the alerts visible on workloads timelines - adding labels to alerts is required. Each alert without a label will be added to the system without mapping to a specific service. Please note to specify 2 labels on the alert in order to connect them to the Kubernetes workload: service : <workload-name> cluster : <cluster-name> [Optional] defining custom description to the alert on Komodor (specify it in the annotation ): description : <content>","title":"Grafana / Alert Manager"},{"location":"Integrations/AlertManager.html#prometheusgrafana-alert-manager-integration","text":"The Integration of Grafana / Prometheus AlertManager allows alerts triggered by AlertManager to appear on Komodor's timelines.","title":"Prometheus/Grafana Alert Manager Integration"},{"location":"Integrations/AlertManager.html#installation","text":"","title":"Installation"},{"location":"Integrations/AlertManager.html#the-alert-manager-integration-involves-3-steps","text":"Enabling the integration in Komodor. Creating webhook on the alert manager / Grafana. Adding labels to the alert.","title":"The Alert Manager integration involves 3 steps:"},{"location":"Integrations/AlertManager.html#1-enabling-the-integration-in-komodor","text":"To enable the Komodor Prometheus Alert Manager integration go to Komodor integrations page , select Prometheus/Grafana Alert Manager, and install the integration.","title":"1. Enabling the integration in Komodor"},{"location":"Integrations/AlertManager.html#2-option-a-the-following-steps-are-for-manual-configuration-on-alert-manager","text":"","title":"2. (Option A) The following steps are for manual configuration on alert manager"},{"location":"Integrations/AlertManager.html#creating-webhook","text":"Open your alertmanager.yml configuration file Add a receiver to your receivers list, name the receiver komodor and attach a sink to webhook_configs . In the url field, put the URL that was provided to you during the integration setup. Also set the field send_resolved to true . receivers : - name : komodor webhook_configs : - url : \"<URL_FROM_KOMODOR>\" send_resolved : true Next, in alertmanager.yml , configure a route so that your alert is routed to komodor route : receiver : komodor If you already have configured routes you can config multiple as follows: routes : - match : severity : critical receiver : pagerduty - match : receiver : komodor A note about the continue configuration of AlertManager routing rules . If it is set to false, AlertManager sends the alert to the first matching route and stops. continue default value is false. A full YAML configuration example: global : group_interval : 5m repeat_interval : 12h routes : - match : receiver : komodor receivers : - name : komodor webhook_configs : - url : \"<URL_FROM_KOMODOR>\" send_resolved : true","title":"Creating webhook"},{"location":"Integrations/AlertManager.html#2-option-b-configure-alerts-from-grafana","text":"Go to Alerting -> Contact points . Click New contact point . Enter in name komodor , in Contact point type select Webhook and in Url insert the URL from the UI. Click Alerting -> Notification policies . In the notification policies, configure komodor contact endpoint as you wish to configure alerts to Komodor.","title":"2. (Option B) Configure alerts from Grafana"},{"location":"Integrations/AlertManager.html#3-adding-labels-to-the-alert","text":"To relate the alert to the relevant workload and make the alerts visible on workloads timelines - adding labels to alerts is required. Each alert without a label will be added to the system without mapping to a specific service. Please note to specify 2 labels on the alert in order to connect them to the Kubernetes workload: service : <workload-name> cluster : <cluster-name> [Optional] defining custom description to the alert on Komodor (specify it in the annotation ): description : <content>","title":"3. Adding labels to the alert."},{"location":"Integrations/ArgoCD.html","text":"ArgoCD integration \u00b6 This tutorial demonstrates how to utilize ArgoCD\u2019s external links to seamlessly hop from a specific K8s resource on Argo to its parallel on Komodor. This is meant to reduce friction and context-swtiching when troubleshooting incidents. Komodor is a continuous reliability platform for Kubernetes that collects all changes and events across your system, provides a coherent timeline view, and offers actionable insights for remediation. General \u00b6 ArgoCD can generate clickable links to external pages for a resource based on per resource annotation. Adding a Komodor dynamic link to ArgoCD will generate a direct link from an ArgoCD application to the relevant workload in Komodor, based on the Kubernetes workload resources annotation. Thus providing a way to quickly get the full context of an incident and resolve it end-to-end with minimal effort. Why use Komodor on top of ArgoCD? \u00b6 To resolve incidents in complex and distributed environments you need full context which can only be understood by looking at changes and events over time Komodor provides visibility to all resources and not only ArgoCD managed apps Komodor can compliment Argo with more capabilities dedicated specifically for rapid incident response. Prerequisites \u00b6 Installed ArgoCD Installed Komodor Installation \u00b6 Add annotations to the requested Kubernetes workload resources The URL should follow the structure bellow https://app.komodor.com/main/deep-dive/ {{ account-name }} . {{ cluster }} - {{ namespace }} . {{ service }} Example \u00b6 Testing \u00b6 Open the ArgoCD application details page and make sure that the external link icon is visible for the respective resource Click on the external link icon to directly hop onto the relevant workload in Komodor. Demo \u00b6 https://user-images.githubusercontent.com/109901035/185045498-18569afc-0871-4903-8c7c-dff80fbda1cd.mov","title":"ArgoCD"},{"location":"Integrations/ArgoCD.html#argocd-integration","text":"This tutorial demonstrates how to utilize ArgoCD\u2019s external links to seamlessly hop from a specific K8s resource on Argo to its parallel on Komodor. This is meant to reduce friction and context-swtiching when troubleshooting incidents. Komodor is a continuous reliability platform for Kubernetes that collects all changes and events across your system, provides a coherent timeline view, and offers actionable insights for remediation.","title":"ArgoCD integration"},{"location":"Integrations/ArgoCD.html#general","text":"ArgoCD can generate clickable links to external pages for a resource based on per resource annotation. Adding a Komodor dynamic link to ArgoCD will generate a direct link from an ArgoCD application to the relevant workload in Komodor, based on the Kubernetes workload resources annotation. Thus providing a way to quickly get the full context of an incident and resolve it end-to-end with minimal effort.","title":"General"},{"location":"Integrations/ArgoCD.html#why-use-komodor-on-top-of-argocd","text":"To resolve incidents in complex and distributed environments you need full context which can only be understood by looking at changes and events over time Komodor provides visibility to all resources and not only ArgoCD managed apps Komodor can compliment Argo with more capabilities dedicated specifically for rapid incident response.","title":"Why use Komodor on top of ArgoCD?"},{"location":"Integrations/ArgoCD.html#prerequisites","text":"Installed ArgoCD Installed Komodor","title":"Prerequisites"},{"location":"Integrations/ArgoCD.html#installation","text":"Add annotations to the requested Kubernetes workload resources The URL should follow the structure bellow https://app.komodor.com/main/deep-dive/ {{ account-name }} . {{ cluster }} - {{ namespace }} . {{ service }}","title":"Installation"},{"location":"Integrations/ArgoCD.html#example","text":"","title":"Example"},{"location":"Integrations/ArgoCD.html#testing","text":"Open the ArgoCD application details page and make sure that the external link icon is visible for the respective resource Click on the external link icon to directly hop onto the relevant workload in Komodor.","title":"Testing"},{"location":"Integrations/ArgoCD.html#demo","text":"https://user-images.githubusercontent.com/109901035/185045498-18569afc-0871-4903-8c7c-dff80fbda1cd.mov","title":"Demo"},{"location":"Integrations/AzureActiveDirectory.html","text":"Azure active directory \u00b6 Register Komodor App with the Microsoft identity platform \u00b6 Under Manage , select App registrations > New registration \u00b6 Name: Komodor Supported account types: Accounts in any organizational directory (Any Azure AD directory - Multitenant) Redirect URI: Platform: Web URL: https://auth.komodor.com/login/callback Click on the Register button Create a Client secret \u00b6 Select Certificates & secrets > Client secrets > New client secret Description: Komodor client secret Expires: choose whatever right for you Click on the Add button Once generated, copy its value and save it! This secret value is never displayed again after you leave this page Make sure to record the expiration date, you will need to renew the key before that day to avoid a service interruption Add permissions \u00b6 Select API permissions > Add a permission > Microsoft APIs > Microsoft Graph choose Delegated permissions Search Directory choose Directory > Directory.Read.All Click the Add Premission button Add another Redirect URIs \u00b6 Select Authentication > WEB > Add URI Add this url https://komodorio.us.auth0.com/login/callback Save Done! The Komodor app is registered \ud83c\udf3b \u00b6 Please send the next values to your contact at Komodor: \u00b6 Microsoft Azure AD Domain Your Azure AD domain name. You can find this on your Azure AD directory's overview page in the Microsoft Azure portal. Client ID Unique identifier for your registered Azure AD application. Enter the saved value of the Application (client) ID for the app you just registered in Azure AD. Client Secret String used to gain access to your registered Azure AD application. Enter the saved value of the Client secret for the app you just registered in Azure AD. resources: \u00b6 Auth0 tutorial youtube (old Azure version but a really nice video)","title":"Azure Active Directory"},{"location":"Integrations/AzureActiveDirectory.html#azure-active-directory","text":"","title":"Azure active directory"},{"location":"Integrations/AzureActiveDirectory.html#register-komodor-app-with-the-microsoft-identity-platform","text":"","title":"Register Komodor App with the Microsoft identity platform"},{"location":"Integrations/AzureActiveDirectory.html#under-manage-select-app-registrations-new-registration","text":"Name: Komodor Supported account types: Accounts in any organizational directory (Any Azure AD directory - Multitenant) Redirect URI: Platform: Web URL: https://auth.komodor.com/login/callback Click on the Register button","title":"Under\u00a0Manage, select\u00a0App registrations\u00a0&gt;\u00a0New registration"},{"location":"Integrations/AzureActiveDirectory.html#create-a-client-secret","text":"Select Certificates & secrets > Client secrets > New client secret Description: Komodor client secret Expires: choose whatever right for you Click on the Add button Once generated, copy its value and save it! This secret value is never displayed again after you leave this page Make sure to record the expiration date, you will need to renew the key before that day to avoid a service interruption","title":"Create a Client secret"},{"location":"Integrations/AzureActiveDirectory.html#add-permissions","text":"Select API permissions > Add a permission > Microsoft APIs > Microsoft Graph choose Delegated permissions Search Directory choose Directory > Directory.Read.All Click the Add Premission button","title":"Add permissions"},{"location":"Integrations/AzureActiveDirectory.html#add-another-redirect-uris","text":"Select Authentication > WEB > Add URI Add this url https://komodorio.us.auth0.com/login/callback Save","title":"Add another Redirect URIs"},{"location":"Integrations/AzureActiveDirectory.html#done-the-komodor-app-is-registered","text":"","title":"Done! The Komodor app is registered \u00a0\ud83c\udf3b"},{"location":"Integrations/AzureActiveDirectory.html#please-send-the-next-values-to-your-contact-at-komodor","text":"Microsoft Azure AD Domain Your Azure AD domain name. You can find this on your Azure AD directory's overview page in the Microsoft Azure portal. Client ID Unique identifier for your registered Azure AD application. Enter the saved value of the Application (client) ID for the app you just registered in Azure AD. Client Secret String used to gain access to your registered Azure AD application. Enter the saved value of the Client secret for the app you just registered in Azure AD.","title":"Please send the next values to your contact at Komodor:"},{"location":"Integrations/AzureActiveDirectory.html#resources","text":"Auth0 tutorial youtube (old Azure version but a really nice video)","title":"resources:"},{"location":"Integrations/Datadog-Monitor-Notification.html","text":"Datadog Monitor Notification \u00b6 Adding A Komodor dynamic link to DataDog Monitor Notifications will generate a direct link to the relevant service in Komodor. You will see the alert link in your Alerting provider connected to DataDog. Installation \u00b6 Prerequisites \u00b6 To use DataDog variables in monitor notifications, there are two prerequisites imposed by DataDog: Make sure there are variables defined for: cluster name, namespace, service name. Make sure the tags are used in the monitor, or grouped by them. Link Setup \u00b6 Make sure the prerequisites above are met. Add the dynamic link to the DataDog monitor notification message: * https://app.komodor.com/main/deep-dive/YourAccountName.{{cluster_name}}-{{namespace}}.{{service_name}} Please note variable names will vary depending on your local DataDog setup. Example: For account greatcompany alerting PagerDuty. variables: cluster_name k8s_clustername ,namespace k8s_namespace , service_name k8s_servicename The dynamic link will be: @pagerduty-Datadog https://app.komodor.com/main/deep-dive/greatcompany. {{ k8s_clustername }} - {{ k8s_namespace }} . {{ k8s_servicename }} Confirmation and testing \u00b6 Dynamic Komodor link will be added to your next DataDog alert. The link will include the relevant information. Testing \u00b6 For an end-to-end testing, add the dynamic link to a test monitor and use the 'Test Notification' button in DataDog. Use the generated link in your alert provider and make sure it directs you to the correct service in Komodor. If the link fails, check the dynamic link prerequisites . See also \u00b6 DataDog documentation on monitor notifications","title":"Datadog Monitor Notifications"},{"location":"Integrations/Datadog-Monitor-Notification.html#datadog-monitor-notification","text":"Adding A Komodor dynamic link to DataDog Monitor Notifications will generate a direct link to the relevant service in Komodor. You will see the alert link in your Alerting provider connected to DataDog.","title":"Datadog Monitor Notification"},{"location":"Integrations/Datadog-Monitor-Notification.html#installation","text":"","title":"Installation"},{"location":"Integrations/Datadog-Monitor-Notification.html#prerequisites","text":"To use DataDog variables in monitor notifications, there are two prerequisites imposed by DataDog: Make sure there are variables defined for: cluster name, namespace, service name. Make sure the tags are used in the monitor, or grouped by them.","title":"Prerequisites"},{"location":"Integrations/Datadog-Monitor-Notification.html#link-setup","text":"Make sure the prerequisites above are met. Add the dynamic link to the DataDog monitor notification message: * https://app.komodor.com/main/deep-dive/YourAccountName.{{cluster_name}}-{{namespace}}.{{service_name}} Please note variable names will vary depending on your local DataDog setup. Example: For account greatcompany alerting PagerDuty. variables: cluster_name k8s_clustername ,namespace k8s_namespace , service_name k8s_servicename The dynamic link will be: @pagerduty-Datadog https://app.komodor.com/main/deep-dive/greatcompany. {{ k8s_clustername }} - {{ k8s_namespace }} . {{ k8s_servicename }}","title":"Link Setup"},{"location":"Integrations/Datadog-Monitor-Notification.html#confirmation-and-testing","text":"Dynamic Komodor link will be added to your next DataDog alert. The link will include the relevant information.","title":"Confirmation and testing"},{"location":"Integrations/Datadog-Monitor-Notification.html#testing","text":"For an end-to-end testing, add the dynamic link to a test monitor and use the 'Test Notification' button in DataDog. Use the generated link in your alert provider and make sure it directs you to the correct service in Komodor. If the link fails, check the dynamic link prerequisites .","title":"Testing"},{"location":"Integrations/Datadog-Monitor-Notification.html#see-also","text":"DataDog documentation on monitor notifications","title":"See also"},{"location":"Integrations/Datadog.html","text":"Datadog Integration \u00b6 DataDog integration allows DataDog Monitor Alerts to be available in Komodor and to suggest related service based on services connection deteced by DataDog. Prerequisites \u00b6 For Komodor service correlation, your services according to Datadog, the following DataDog's service tags should be available on the resources. environment - should match the environment specified on the Datadog service ( DD_ENV ) service - should match the service name specified on the Datadog service ( DD_SERVICE ) DataDog's tags can be done by environment variables, labels, and annotations. To do the correlation, the tags must be a string value and not a reference value. For more information about DataDog tags for Kubernetes go into DataDog tagging documentation Installation Steps \u00b6 Make sure the above prerequisites are met. Locate the Datadog installation tile on Komodor Integrations page. Press Install Integration . Follow the on screen instructions. Confirmation \u00b6 A Datadog Integration tile will be added to the top section under Installed Integrations . Your services that interact with each other will appear under the Related Services section in the Komodor services page.","title":"Datadog"},{"location":"Integrations/Datadog.html#datadog-integration","text":"DataDog integration allows DataDog Monitor Alerts to be available in Komodor and to suggest related service based on services connection deteced by DataDog.","title":"Datadog Integration"},{"location":"Integrations/Datadog.html#prerequisites","text":"For Komodor service correlation, your services according to Datadog, the following DataDog's service tags should be available on the resources. environment - should match the environment specified on the Datadog service ( DD_ENV ) service - should match the service name specified on the Datadog service ( DD_SERVICE ) DataDog's tags can be done by environment variables, labels, and annotations. To do the correlation, the tags must be a string value and not a reference value. For more information about DataDog tags for Kubernetes go into DataDog tagging documentation","title":"Prerequisites"},{"location":"Integrations/Datadog.html#installation-steps","text":"Make sure the above prerequisites are met. Locate the Datadog installation tile on Komodor Integrations page. Press Install Integration . Follow the on screen instructions.","title":"Installation Steps"},{"location":"Integrations/Datadog.html#confirmation","text":"A Datadog Integration tile will be added to the top section under Installed Integrations . Your services that interact with each other will appear under the Related Services section in the Komodor services page.","title":"Confirmation"},{"location":"Integrations/LaunchDarkly.html","text":"LaunchDarkly Integration \u00b6 Integration with LaunchDarkly extends the holistic view of the environment with flag change events on the timeline. For example: Feature flag was turned on Feature flag in variation changed Feature flag targeting was changed Installation \u00b6 The LaunchDarkly integration involves two parts: \u00b6 Enabling the integration in Komodor. Creating a LaunchDarkly webhook. Enabling the integration in Komodor \u00b6 To enable the Komodor LaunchDarkly integration go to Komodor integrations page and select LaunchDarkly This will open a window with the LaunchDarkly webhook URL and sign key *Note: you can use the LaunchDarkly Webhook Integration link to continue with the webhook creation Creating the LaunchDarkly webhook \u00b6 To create the LaunchDarkly webhook go to LaunchDarkly Webhook Integration Page and use the values from Komodor's integration page Name: Your integration's name URL: Use the URL from the integration window in Komodor Check the \"Sign this webhook\" checkbox Secret: Use the Secret value from the integration window in Komodor Filter Policy: To send all LaunchDarkly event to Komodor: press the \"+ Add statment\" Choose resources for this policy statement: proj/*:env/*:flag/* Allow or deny actions on the resource: Allow Choose actions to allow or deny: All actions \" Update statement \" Check \" I have read and agree to the Integration Terms and Conditions \" \" Save Settings \" to finish the integration.","title":"LaunchDarkly"},{"location":"Integrations/LaunchDarkly.html#launchdarkly-integration","text":"Integration with LaunchDarkly extends the holistic view of the environment with flag change events on the timeline. For example: Feature flag was turned on Feature flag in variation changed Feature flag targeting was changed","title":"LaunchDarkly Integration"},{"location":"Integrations/LaunchDarkly.html#installation","text":"","title":"Installation"},{"location":"Integrations/LaunchDarkly.html#the-launchdarkly-integration-involves-two-parts","text":"Enabling the integration in Komodor. Creating a LaunchDarkly webhook.","title":"The LaunchDarkly integration involves two parts:"},{"location":"Integrations/LaunchDarkly.html#enabling-the-integration-in-komodor","text":"To enable the Komodor LaunchDarkly integration go to Komodor integrations page and select LaunchDarkly This will open a window with the LaunchDarkly webhook URL and sign key *Note: you can use the LaunchDarkly Webhook Integration link to continue with the webhook creation","title":"Enabling the integration in Komodor"},{"location":"Integrations/LaunchDarkly.html#creating-the-launchdarkly-webhook","text":"To create the LaunchDarkly webhook go to LaunchDarkly Webhook Integration Page and use the values from Komodor's integration page Name: Your integration's name URL: Use the URL from the integration window in Komodor Check the \"Sign this webhook\" checkbox Secret: Use the Secret value from the integration window in Komodor Filter Policy: To send all LaunchDarkly event to Komodor: press the \"+ Add statment\" Choose resources for this policy statement: proj/*:env/*:flag/* Allow or deny actions on the resource: Allow Choose actions to allow or deny: All actions \" Update statement \" Check \" I have read and agree to the Integration Terms and Conditions \" \" Save Settings \" to finish the integration.","title":"Creating the LaunchDarkly webhook"},{"location":"Integrations/MicrosoftTeams.html","text":"Microsoft Teams Integration \u00b6 The Teams integration allows you to be notified on issues triggered by Komodor Monitors . Configuring Teams Notifications \u00b6 Configuration Steps \u00b6 Open up the Komodor Monitors page . Select the Cluster Select the Monitor type Create/modify Monitor rule, configure it's trigger conditions, the scope on which you'd like to be notified on and specify the Teams channel you want the notification to be sent to. On your first use, you will have to configure a Teams channel, to do so, click on \"Add New Channel\" and follow the guide provided in the UI Save the rule","title":"Microsoft Teams"},{"location":"Integrations/MicrosoftTeams.html#microsoft-teams-integration","text":"The Teams integration allows you to be notified on issues triggered by Komodor Monitors .","title":"Microsoft Teams Integration"},{"location":"Integrations/MicrosoftTeams.html#configuring-teams-notifications","text":"","title":"Configuring Teams Notifications"},{"location":"Integrations/MicrosoftTeams.html#configuration-steps","text":"Open up the Komodor Monitors page . Select the Cluster Select the Monitor type Create/modify Monitor rule, configure it's trigger conditions, the scope on which you'd like to be notified on and specify the Teams channel you want the notification to be sent to. On your first use, you will have to configure a Teams channel, to do so, click on \"Add New Channel\" and follow the guide provided in the UI Save the rule","title":"Configuration Steps"},{"location":"Integrations/Opsgenie.html","text":"Opsgenie Alert Event Integration \u00b6 Opsgenie integration allows Opsgenie Alerts to be available in Komodor timelines. Installation \u00b6 The Opsgenie integration involves 3 steps: \u00b6 Enabling the integration in Komodor. Integrate Opsgenie with Webhook. Adding labels to the alert. 1. Enabling the integration in Komodor \u00b6 To enable the Komodor Opsgenie integration go to Komodor integrations page , select Opsgenie and install the integration. 2. Integrate Opsgenie with Webhook \u00b6 Follow the following documentation to create an Opsgenie webhook . 3. Adding labels to the alert. \u00b6 By default, an Opsgenie alert will be added to the Komodor timeline as a \"Cross-service event\". To associate Opsgenie alerts to specific/multiple workloads, it is required to specify the labels associated with the relevant Kubernetes workloads as extra properties of the Opsgenie alert. <label_name> : <label_value>","title":"Opsgenie"},{"location":"Integrations/Opsgenie.html#opsgenie-alert-event-integration","text":"Opsgenie integration allows Opsgenie Alerts to be available in Komodor timelines.","title":"Opsgenie Alert Event Integration"},{"location":"Integrations/Opsgenie.html#installation","text":"","title":"Installation"},{"location":"Integrations/Opsgenie.html#the-opsgenie-integration-involves-3-steps","text":"Enabling the integration in Komodor. Integrate Opsgenie with Webhook. Adding labels to the alert.","title":"The Opsgenie integration involves 3 steps:"},{"location":"Integrations/Opsgenie.html#1-enabling-the-integration-in-komodor","text":"To enable the Komodor Opsgenie integration go to Komodor integrations page , select Opsgenie and install the integration.","title":"1. Enabling the integration in Komodor"},{"location":"Integrations/Opsgenie.html#2-integrate-opsgenie-with-webhook","text":"Follow the following documentation to create an Opsgenie webhook .","title":"2. Integrate Opsgenie with Webhook"},{"location":"Integrations/Opsgenie.html#3-adding-labels-to-the-alert","text":"By default, an Opsgenie alert will be added to the Komodor timeline as a \"Cross-service event\". To associate Opsgenie alerts to specific/multiple workloads, it is required to specify the labels associated with the relevant Kubernetes workloads as extra properties of the Opsgenie alert. <label_name> : <label_value>","title":"3. Adding labels to the alert."},{"location":"Integrations/PagerDuty.html","text":"PagerDuty Integration \u00b6 PagerDuty Integration adds PagerDuty Incident events to your services events. NOTE: We currently only support PD events that originate in Datadog. Installation \u00b6 Prerequisites \u00b6 In order to connect your Datadog-PD incidents to your services in Komodor, you need to match environment variables found in Datadog to your services in Kubernetes. Please make sure the following environment variables exist in your kubernetes deployment: - DD_ENV should match the environment specified on the Datadog service - DD_SERVICE should match the service name specified on the Datadog service Installation Steps \u00b6 Make sure the prerequisites above are met. Locate the PagerDuty installation tile on Komodor Integrations page. Press Install Integration . You will be redirected to Pagerduty and back to Komodor successfully. Confirmation \u00b6 A PagerDuty Integration tile will be added to the top section of labeled Installed Integrations . When an issue from Datadog is raised through PD, you will be able to find it in the appropriate Komodor services page.","title":"PagerDuty"},{"location":"Integrations/PagerDuty.html#pagerduty-integration","text":"PagerDuty Integration adds PagerDuty Incident events to your services events. NOTE: We currently only support PD events that originate in Datadog.","title":"PagerDuty Integration"},{"location":"Integrations/PagerDuty.html#installation","text":"","title":"Installation"},{"location":"Integrations/PagerDuty.html#prerequisites","text":"In order to connect your Datadog-PD incidents to your services in Komodor, you need to match environment variables found in Datadog to your services in Kubernetes. Please make sure the following environment variables exist in your kubernetes deployment: - DD_ENV should match the environment specified on the Datadog service - DD_SERVICE should match the service name specified on the Datadog service","title":"Prerequisites"},{"location":"Integrations/PagerDuty.html#installation-steps","text":"Make sure the prerequisites above are met. Locate the PagerDuty installation tile on Komodor Integrations page. Press Install Integration . You will be redirected to Pagerduty and back to Komodor successfully.","title":"Installation Steps"},{"location":"Integrations/PagerDuty.html#confirmation","text":"A PagerDuty Integration tile will be added to the top section of labeled Installed Integrations . When an issue from Datadog is raised through PD, you will be able to find it in the appropriate Komodor services page.","title":"Confirmation"},{"location":"Integrations/Sentry.html","text":"Sentry Integration \u00b6 The Sentry integration allows you to see sentry issues in Komodor. For each service, Komodor automatically maps the relevant Sentry project to Komodor services and allows you to gain a full-service timeline: both relevant changes (deploys, config changes etc\u2019) and issues from Sentry will all be in one place. Installation \u00b6 Installation Steps \u00b6 Make sure the services you track in Komodor use the SENTRY_DSN environment variable. Locate the Sentry installation tile on the Komodor Integrations page. Click Install Integration . A dialog will open with a webhook URL and a link to Sentry to define an internal integration. Go to your Sentry account and click on settings in the left menu. Click on Developer Settings in the settings menu: Click on + New Internal Integration Name this integration Komodor. Paste the webhook URL in the webhook field: To operate properly Komodor needs these permissions: Project - Read Issue & Event - Read Check the issue webhook checkbox Save changes As soon as you save you will see your \u201cclient secret\u201d. Copy the value: Go back to your Komodor integrations page and paste the value of your client secret to the client secret text box. Click Install In your deployment.yaml \u00b6 We use the value of the environment variable SENTRY_DSN to match Sentry Issue events with your services in Komodor. Make sure your kubernetes deployment has the environment variable SENTRY_DSN . Confirmation \u00b6 A Sentry Integration tile will be added to the top section under Installed Integrations . Once completed you will be able to see Sentry events in you services view:","title":"Sentry"},{"location":"Integrations/Sentry.html#sentry-integration","text":"The Sentry integration allows you to see sentry issues in Komodor. For each service, Komodor automatically maps the relevant Sentry project to Komodor services and allows you to gain a full-service timeline: both relevant changes (deploys, config changes etc\u2019) and issues from Sentry will all be in one place.","title":"Sentry Integration"},{"location":"Integrations/Sentry.html#installation","text":"","title":"Installation"},{"location":"Integrations/Sentry.html#installation-steps","text":"Make sure the services you track in Komodor use the SENTRY_DSN environment variable. Locate the Sentry installation tile on the Komodor Integrations page. Click Install Integration . A dialog will open with a webhook URL and a link to Sentry to define an internal integration. Go to your Sentry account and click on settings in the left menu. Click on Developer Settings in the settings menu: Click on + New Internal Integration Name this integration Komodor. Paste the webhook URL in the webhook field: To operate properly Komodor needs these permissions: Project - Read Issue & Event - Read Check the issue webhook checkbox Save changes As soon as you save you will see your \u201cclient secret\u201d. Copy the value: Go back to your Komodor integrations page and paste the value of your client secret to the client secret text box. Click Install","title":"Installation Steps"},{"location":"Integrations/Sentry.html#in-your-deploymentyaml","text":"We use the value of the environment variable SENTRY_DSN to match Sentry Issue events with your services in Komodor. Make sure your kubernetes deployment has the environment variable SENTRY_DSN .","title":"In your deployment.yaml"},{"location":"Integrations/Sentry.html#confirmation","text":"A Sentry Integration tile will be added to the top section under Installed Integrations . Once completed you will be able to see Sentry events in you services view:","title":"Confirmation"},{"location":"Integrations/Slack.html","text":"Slack Integration \u00b6 Overview \u00b6 The Slack integration allows you to be notified on issues triggered by Komodor Monitors . Requirements \u00b6 Depending on your companies Slack settings a company admin may be required to enable the integration, the same user will need to be a Komodor Admin in order to start the installation from the Komodor integration page. !!! Note The free version of Slack is limited to 10 applications. Installation \u00b6 Once logged into the Komodor platform click on the Integrations tab. Locate the Slack integration under the Avaiable Integrations section and click on Install Integration to start, this will forward you to the Slack Workspace login page. You might be prompted to login to your Slack workspace, if so login and and click Continue . Click on Allow to complete the Slack integration. Once completed you will be forwarded back to the Komodor Integration page where you will find the Slack integration listed under the Installed Integrations . Creating notifications \u00b6 To enable notifications use the Monitors tab in the UI Open up the Komodor Monitors page . Select the Cluster Select the Monitor type Create/modify Monitor rule, configure it's trigger conditions, the scope on which you'd like to be notified on and specify the Slack channel you want the notification to be sent to. Save the rule","title":"Slack"},{"location":"Integrations/Slack.html#slack-integration","text":"","title":"Slack Integration"},{"location":"Integrations/Slack.html#overview","text":"The Slack integration allows you to be notified on issues triggered by Komodor Monitors .","title":"Overview"},{"location":"Integrations/Slack.html#requirements","text":"Depending on your companies Slack settings a company admin may be required to enable the integration, the same user will need to be a Komodor Admin in order to start the installation from the Komodor integration page. !!! Note The free version of Slack is limited to 10 applications.","title":"Requirements"},{"location":"Integrations/Slack.html#installation","text":"Once logged into the Komodor platform click on the Integrations tab. Locate the Slack integration under the Avaiable Integrations section and click on Install Integration to start, this will forward you to the Slack Workspace login page. You might be prompted to login to your Slack workspace, if so login and and click Continue . Click on Allow to complete the Slack integration. Once completed you will be forwarded back to the Komodor Integration page where you will find the Slack integration listed under the Installed Integrations .","title":"Installation"},{"location":"Integrations/Slack.html#creating-notifications","text":"To enable notifications use the Monitors tab in the UI Open up the Komodor Monitors page . Select the Cluster Select the Monitor type Create/modify Monitor rule, configure it's trigger conditions, the scope on which you'd like to be notified on and specify the Slack channel you want the notification to be sent to. Save the rule","title":"Creating notifications"},{"location":"Integrations/datadog-webhook.html","text":"Datadog Webhook Integration \u00b6 Komodor-Datadog webhook integration allows Komodor to receive alerts from Datadog Monitors. You will see all alerts in the Komodor Service View. Installation \u00b6 Prerequisites \u00b6 In order for us to connect your services according to Datadog events, the following environment variables should exist on your kubernetes deployments: - DD_ENV should match the environment specified on the Datadog tags - DD_SERVICE should match the service name specified on the Datadog tags Installation Steps \u00b6 Make sure the prerequisites above are met. Locate the Datadog installation tile on Komodor Integrations page. Press Install Integration . Follow the on screen instructions. Confirmation \u00b6 A Datadog Integration tile will be added to the top section under Installed Integrations . Configuring the webhook in Datadog \u00b6 Go to Datadog Webhook Integration Setup Create a + New Webhook Name the webhook komodor Enter the webhook server URL in the URL field: https://app.komodor.com/collector/datadog/webhook Copy the following Payload schema into the Payload field: { \"body\" : \"$EVENT_MSG\" , \"last_updated\" : \"$LAST_UPDATED\" , \"event_type\" : \"$EVENT_TYPE\" , \"title\" : \"$EVENT_TITLE\" , \"date\" : \"$DATE\" , \"org\" : { \"id\" : \"$ORG_ID\" , \"name\" : \"$ORG_NAME\" }, \"id\" : \"$ID\" , \"tags\" : \"$TAGS\" , \"alert\" : { \"id\" : \"$ALERT_ID\" , \"type\" : \"$ALERT_TYPE\" , \"transition\" : \"$ALERT_TRANSITION\" , \"cycleId\" : \"$ALERT_CYCLE_KEY\" , \"priority\" : \"$PRIORITY\" , \"status\" : \"$ALERT_STATUS\" , \"scope\" : \"$ALERT_SCOPE\" , \"query\" : \"$ALERT_QUERY\" , \"metric\" : \"$ALERT_METRIC\" , \"metric_namespace\" : \"$METRIC_NAMESPACE\" }, \"aggregation_key\" : \"$AGGREG_KEY\" , \"link\" : \"$LINK\" , \"snapshot\" : \"$SNAPSHOT\" , \"user\" : \"$USER\" , \"username\" : \"$USERNAME\" , \"email\" : \"$EMAIL\" } Add a Custom Header X-API-KEY with the Api Key found in the Komodor Integration page from the Datadog integration tile at the bottom of the setup modal in a JSON format. { \"X-API-KEY\" : \"YOUR_KOMODOR_API_KEY\" } When completed click on Save . For every monitor you wish to receive alerts from in Komodor. Edit the monitor and add @webhook-komodor at the end of the Monitor message.","title":"Datadog Webhook"},{"location":"Integrations/datadog-webhook.html#datadog-webhook-integration","text":"Komodor-Datadog webhook integration allows Komodor to receive alerts from Datadog Monitors. You will see all alerts in the Komodor Service View.","title":"Datadog Webhook Integration"},{"location":"Integrations/datadog-webhook.html#installation","text":"","title":"Installation"},{"location":"Integrations/datadog-webhook.html#prerequisites","text":"In order for us to connect your services according to Datadog events, the following environment variables should exist on your kubernetes deployments: - DD_ENV should match the environment specified on the Datadog tags - DD_SERVICE should match the service name specified on the Datadog tags","title":"Prerequisites"},{"location":"Integrations/datadog-webhook.html#installation-steps","text":"Make sure the prerequisites above are met. Locate the Datadog installation tile on Komodor Integrations page. Press Install Integration . Follow the on screen instructions.","title":"Installation Steps"},{"location":"Integrations/datadog-webhook.html#confirmation","text":"A Datadog Integration tile will be added to the top section under Installed Integrations .","title":"Confirmation"},{"location":"Integrations/datadog-webhook.html#configuring-the-webhook-in-datadog","text":"Go to Datadog Webhook Integration Setup Create a + New Webhook Name the webhook komodor Enter the webhook server URL in the URL field: https://app.komodor.com/collector/datadog/webhook Copy the following Payload schema into the Payload field: { \"body\" : \"$EVENT_MSG\" , \"last_updated\" : \"$LAST_UPDATED\" , \"event_type\" : \"$EVENT_TYPE\" , \"title\" : \"$EVENT_TITLE\" , \"date\" : \"$DATE\" , \"org\" : { \"id\" : \"$ORG_ID\" , \"name\" : \"$ORG_NAME\" }, \"id\" : \"$ID\" , \"tags\" : \"$TAGS\" , \"alert\" : { \"id\" : \"$ALERT_ID\" , \"type\" : \"$ALERT_TYPE\" , \"transition\" : \"$ALERT_TRANSITION\" , \"cycleId\" : \"$ALERT_CYCLE_KEY\" , \"priority\" : \"$PRIORITY\" , \"status\" : \"$ALERT_STATUS\" , \"scope\" : \"$ALERT_SCOPE\" , \"query\" : \"$ALERT_QUERY\" , \"metric\" : \"$ALERT_METRIC\" , \"metric_namespace\" : \"$METRIC_NAMESPACE\" }, \"aggregation_key\" : \"$AGGREG_KEY\" , \"link\" : \"$LINK\" , \"snapshot\" : \"$SNAPSHOT\" , \"user\" : \"$USER\" , \"username\" : \"$USERNAME\" , \"email\" : \"$EMAIL\" } Add a Custom Header X-API-KEY with the Api Key found in the Komodor Integration page from the Datadog integration tile at the bottom of the setup modal in a JSON format. { \"X-API-KEY\" : \"YOUR_KOMODOR_API_KEY\" } When completed click on Save . For every monitor you wish to receive alerts from in Komodor. Edit the monitor and add @webhook-komodor at the end of the Monitor message.","title":"Configuring the webhook in Datadog"},{"location":"Integrations/github.html","text":"ToDo - add github docs","title":"Github"},{"location":"Integrations/gitlab.html","text":"ToDo - create gitlab docs","title":"Gitlab"},{"location":"Integrations/Okta/Okta.html","text":"Okta Integration \u00b6 Use Okta's deep, pre-built integrations to securely connect to Komodor. Note: Only Okta administrators can add the Komodor application, if you aren't an Okta administrator, please contact your Okta administrator to have the application added. Follow these steps to integrate Komodor with Okta: Go to Okta Admin -> Applications -> Browse App Catalog Search for \"Komodor\". Then click 'Add'. Enter any application label you want in 'Application Label'. This is for internal use only and will also be the nickname for the Application. Go to application -> 'Sign On' tab -> 'Settings' and click 'Edit'. In 'Advanced Sign-on settings' enter variable of 'Account Name' and pass this variable to us, for example: 'Komodorio'. Please change application username format to Email. **NOTE once you defined 'companyName' it can't be changed Click 'View Setup Instructions'. Send the 'Metadata URL' and Account Name variable to support@komodor.com to complete the Okta setup. Once Customer Success has completed the setup you can begin to use Okta for SSO.","title":"Okta"},{"location":"Integrations/Okta/Okta.html#okta-integration","text":"Use Okta's deep, pre-built integrations to securely connect to Komodor. Note: Only Okta administrators can add the Komodor application, if you aren't an Okta administrator, please contact your Okta administrator to have the application added. Follow these steps to integrate Komodor with Okta: Go to Okta Admin -> Applications -> Browse App Catalog Search for \"Komodor\". Then click 'Add'. Enter any application label you want in 'Application Label'. This is for internal use only and will also be the nickname for the Application. Go to application -> 'Sign On' tab -> 'Settings' and click 'Edit'. In 'Advanced Sign-on settings' enter variable of 'Account Name' and pass this variable to us, for example: 'Komodorio'. Please change application username format to Email. **NOTE once you defined 'companyName' it can't be changed Click 'View Setup Instructions'. Send the 'Metadata URL' and Account Name variable to support@komodor.com to complete the Okta setup. Once Customer Success has completed the setup you can begin to use Okta for SSO.","title":"Okta Integration"},{"location":"Learn/index.html","text":"","title":"Index"},{"location":"Learn/Actions.html","text":"Actions (initial release) \u00b6 To allow our users to close the troubleshooting loop through Komodor, we\u2019re adding the ability to perform Actions through the platform. Using Komodor you can run multiple actions against your resource. You'll be able to easily track what was done and by whom. Prerequisites \u00b6 Agent version 0.1.104 Required permissions (permissions can be modified as needed) - apiGroups: - apps resources: - deployments/scale - statefulsets/scale - deployments - replicasets - statefulsets - daemonsets verbs: - patch - apiGroups: - \"\" resources: - pods verbs: - delete - apiGroups: - batch resources: - jobs verbs: - delete - create - apiGroups: - \"\" resources: - pods - persistentvolumeclaims - configmaps - services - persistentvolumes - storageclasses verbs: - delete - patch - update - create - apiGroups: - apps resources: - replicasets - deployments - statefulsets - daemonsets verbs: - delete - patch - update - create - apiGroups: - batch resources: - cronjobs - jobs verbs: - delete - patch - update - create - apiGroups: - networking.k8s.io resources: - ingresses - networkpolicies verbs: - delete - patch - update - create - apiGroups: - \"\" resources: - nodes verbs: - patch - apiGroups: - \"\" resources: - pods/eviction verbs: - create Please note: To perform actions against your resources, the user have to be either an account-admin or be provided with permission to perform actions, to read more about Komodor RBAC How to opt-in \u00b6 For convienece purposes we've seperated the actions helm chart values into two sections - watcher.actions.basic - Enables basic actions (Delete pods, Scale and restart deployments, statefulsets, replicasets. Restart and trigger jobs) - watcher.actions.advanced - Enables advanced actions (Update, Create and Delete resources, Cordon/Uncordon nodes) New cluster installation \u00b6 To install a new cluster with actions enabled just follow the installation process from the Komodor console Cluster upgrade \u00b6 helm repo add komodorio https :// helm-charts . komodor . io ; helm repo update ; helm upgrade --install k8s-watcher komodorio / k8s-watcher --set watcher . actions . basic = true --set watcher . actions . advanced = true --reuse-values How to revoke \u00b6 To disable the usage of Actions using helm, use the following command: helm repo add komodorio https :// helm-charts . komodor . io ; helm repo update ; helm upgrade --install k8s-watcher komodorio / k8s-watcher --set watcher . actions . basic = false --set watcher . actions . advanced = false --reuse-values How does it work? \u00b6 User triggers a Manual Action through the Komodor platform A command is registered to the Komodor SaaS The Agent running in the cluster fetches the command from the Komodor SaaS (communication is always done from the Agent outside of the cluster) The command is triggered against the Kuberenetes API Kubernetes will now execute the command During the entire process you can track the changes/events through a dedicated Event that will be created on the Komodor timeline. Please note: Due to Kubernetes nature, this feature is built in an asynchronous way, review the timeline after triggering any action for updates Audit \u00b6 For auditing purposes, Manual Actions events are created on the Komodor timeline What type of Actions are supported and where can they be triggered from? \u00b6 We currently support the following actions: - Scale service - Allows modifying the number of replicas for a Service. Can be triggered from Deployment/StatefulSet inspection pages (under Workloads) and also from a Service timeline page - Delete Pod - Deletes/kills a specific Pod. Can be triggered from both the Pod inspection page (under Workloads) and the Pods & Logs screen - Restart service - Triggers a rolling restart of all the Pods of a Service. Can be triggered from Deployment/StatefulSet inspection page (under Workloads) ans also from a Service timeline page - Rollback service - Rolls back a service to the previous generation - Re-trigger Job/CronJob - Re-creates the Job to trigger a new run of it. Can be triggered from a Job/CronJob timeline, Job/CronJob inspection pages (under Workloads) and from a Job event drawer - Cordon/Uncordon node - Allows marking a node as unscehduable, preventing new Pods from being scheduled on it. You can revert this by using the Uncordon action - Delete/Edit resources","title":"Actions"},{"location":"Learn/Actions.html#actions-initial-release","text":"To allow our users to close the troubleshooting loop through Komodor, we\u2019re adding the ability to perform Actions through the platform. Using Komodor you can run multiple actions against your resource. You'll be able to easily track what was done and by whom.","title":"Actions (initial release)"},{"location":"Learn/Actions.html#prerequisites","text":"Agent version 0.1.104 Required permissions (permissions can be modified as needed) - apiGroups: - apps resources: - deployments/scale - statefulsets/scale - deployments - replicasets - statefulsets - daemonsets verbs: - patch - apiGroups: - \"\" resources: - pods verbs: - delete - apiGroups: - batch resources: - jobs verbs: - delete - create - apiGroups: - \"\" resources: - pods - persistentvolumeclaims - configmaps - services - persistentvolumes - storageclasses verbs: - delete - patch - update - create - apiGroups: - apps resources: - replicasets - deployments - statefulsets - daemonsets verbs: - delete - patch - update - create - apiGroups: - batch resources: - cronjobs - jobs verbs: - delete - patch - update - create - apiGroups: - networking.k8s.io resources: - ingresses - networkpolicies verbs: - delete - patch - update - create - apiGroups: - \"\" resources: - nodes verbs: - patch - apiGroups: - \"\" resources: - pods/eviction verbs: - create Please note: To perform actions against your resources, the user have to be either an account-admin or be provided with permission to perform actions, to read more about Komodor RBAC","title":"Prerequisites"},{"location":"Learn/Actions.html#how-to-opt-in","text":"For convienece purposes we've seperated the actions helm chart values into two sections - watcher.actions.basic - Enables basic actions (Delete pods, Scale and restart deployments, statefulsets, replicasets. Restart and trigger jobs) - watcher.actions.advanced - Enables advanced actions (Update, Create and Delete resources, Cordon/Uncordon nodes)","title":"How to opt-in"},{"location":"Learn/Actions.html#new-cluster-installation","text":"To install a new cluster with actions enabled just follow the installation process from the Komodor console","title":"New cluster installation"},{"location":"Learn/Actions.html#cluster-upgrade","text":"helm repo add komodorio https :// helm-charts . komodor . io ; helm repo update ; helm upgrade --install k8s-watcher komodorio / k8s-watcher --set watcher . actions . basic = true --set watcher . actions . advanced = true --reuse-values","title":"Cluster upgrade"},{"location":"Learn/Actions.html#how-to-revoke","text":"To disable the usage of Actions using helm, use the following command: helm repo add komodorio https :// helm-charts . komodor . io ; helm repo update ; helm upgrade --install k8s-watcher komodorio / k8s-watcher --set watcher . actions . basic = false --set watcher . actions . advanced = false --reuse-values","title":"How to revoke"},{"location":"Learn/Actions.html#how-does-it-work","text":"User triggers a Manual Action through the Komodor platform A command is registered to the Komodor SaaS The Agent running in the cluster fetches the command from the Komodor SaaS (communication is always done from the Agent outside of the cluster) The command is triggered against the Kuberenetes API Kubernetes will now execute the command During the entire process you can track the changes/events through a dedicated Event that will be created on the Komodor timeline. Please note: Due to Kubernetes nature, this feature is built in an asynchronous way, review the timeline after triggering any action for updates","title":"How does it work?"},{"location":"Learn/Actions.html#audit","text":"For auditing purposes, Manual Actions events are created on the Komodor timeline","title":"Audit"},{"location":"Learn/Actions.html#what-type-of-actions-are-supported-and-where-can-they-be-triggered-from","text":"We currently support the following actions: - Scale service - Allows modifying the number of replicas for a Service. Can be triggered from Deployment/StatefulSet inspection pages (under Workloads) and also from a Service timeline page - Delete Pod - Deletes/kills a specific Pod. Can be triggered from both the Pod inspection page (under Workloads) and the Pods & Logs screen - Restart service - Triggers a rolling restart of all the Pods of a Service. Can be triggered from Deployment/StatefulSet inspection page (under Workloads) ans also from a Service timeline page - Rollback service - Rolls back a service to the previous generation - Re-trigger Job/CronJob - Re-creates the Job to trigger a new run of it. Can be triggered from a Job/CronJob timeline, Job/CronJob inspection pages (under Workloads) and from a Job event drawer - Cordon/Uncordon node - Allows marking a node as unscehduable, preventing new Pods from being scheduled on it. You can revert this by using the Uncordon action - Delete/Edit resources","title":"What type of Actions are supported and where can they be triggered from?"},{"location":"Learn/Annotations.html","text":"Komodor kubernetes annotations \u00b6 Komodor annotations (AKA Komodor as Code), is a method to allow users to configure everything related to Komodor as part of their native k8s yaml. Komodor annotations should be placed in the deployment resource annotations (annotations set on the pod template are ignored) CI-Deploy Links \u00b6 For each deployment version, you can add a quick link with the job url. How \u00b6 app.komodor.com/deploy.job.name:url Example: Annotation Values Description Example app.komodor.com/deploy.job.jenkins url Link to Jenkins job that deploys the service https://ci.jenkins-ci.org/computer/job Deploy Links \u00b6 For each deployment version, you can add a quick link with the relevant filters already in place! How \u00b6 app.komodor.com/deploy.link.name:url Examples: Annotation Values Description Example app.komodor.com/deploy.link.logs url Link for the specific version logs https://app.logz.io/#/dashboard/kibana/discover?_a=env:123.0.1 app.komodor.com/deploy.link.sentry url Link for the specific version Sentry issues https://sentry.io/organizations/rookoutz/issues/?project=1320440&query=sdk.version%3A1.0.1&statsPeriod=14d Custom Links \u00b6 You can create custom links to external and internal applications by crafting your own URL to the application using a skeleton URL and placeholders provided by Komodor. Just copy the URL of the application you want to link to, identify the placeholders in the URL that are used to query the application, and replace them with placeholders for your own use. Please find the below examples as references for common applications. How \u00b6 app.komodor.com/deploy.link.name:value Examples: Annotation Values Description Example app.komodor.com/deploy.link.coralogix url Link for the custom URL, coralogix https://komodortest.coralogix.com/#/query-new/logs?query=(coralogix.metadata.cluster:(%22${cluster}%22))%20AND%20(coralogix.metadata.namespace:(%22${namespace}%22))%20AND%20(coralogix.metadata.service:(%22${service}%22))&time=from:${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS},to:${timestampEnd=yyyy-MM-dd'T'HH:mm:ss.SSS} app.komodor.com/deploy.link.logzio url Link for the custom URL, logz.io https://app.logz.io/#/dashboard/kibana/discover?_a=(columns:!(message,kubernetes.namespace_name,kubernetes.container_name,params.clusterName),filters:!(('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:kubernetes.namespace_name,negate:!f,params:(query:default),type:phrase),query:(match_phrase:(kubernetes.namespace_name:${namespace}))),('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:params.clusterName,negate:!f,params:(query:main),type:phrase),query:(match_phrase:(params.clusterName:${cluster}))),('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:kubernetes.container_name,negate:!f,params:(query:k8s-events-collector),type:phrase),query:(match_phrase:(kubernetes.container_name:${service})),query:(match_phrase:(kubernetes.container_image:${container[web].image})))),index:'logzioCustomerIndex',interval:auto,query:(language:lucene,query:''),sort:!(!('@timestamp',desc)))&_g=(filters:!(),refreshInterval:(pause:!t,value:0),time:(from:'${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS}',to:'${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS}'))&discoverTab=logz-logs-tab&switchToAccountId=138828&accountIds=true app.komodor.com/deploy.link.datadog url Link for the custom URL, DataDog https://app.datadoghq.com/apm/traces?query=service%3A${service}%20kube_namespace%3A${namespace}%20env%3A${cluster}&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code&historicalData=true&messageDisplay=inline&sort=desc&streamTraces=true&start=${epochStart}&end=${epochEnd}&paused=true The following values can be used to enrich the URL: Placeholder Value Example ${epochStart} Start Time in Epoch Time ${epochEnd} End Time in Epoch Time ${service} Service Name ${namespace} Namespace Name ${cluster} Cluster Name ${failedPod} * The pod name of a failed pod that triggered this health event* ${container[<name>].image} ** Image name of a container ${container[web].image} ${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS} Start Time in custom format*** ${timestampStart=yyyy-MM-dd} ${timestampEnd=yyyy-MM-dd'T'HH:mm:ss.SSS} End Time in custom format*** ${timestampEnd=yyyy-MM-dd} ${yaml[<spec_path>]} Full yaml's path specification ${yaml[metadata.labels.app]} *Not applicable in Service context. **Custom links with a failed pod name will be created on health events only. ***Dates can be crafted using the display guidelines of date-fns https://date-fns.org/v2.25.0/docs/format Example on how to use YAML full path: spec : replicas : 5 selector : matchLabels : app : nginx template : spec : containers : - name : test image : nginx:1.14.2 ports : - containerPort : 80 - name : test2 image : nginx:1.14.2 ports : - containerPort : 80 metadata : lables : app.kubernetes.io/name : nginx app.kubernetes.io/managed-by : helm YAML Path Value Explanation ${yaml[spec.replicas]} 5 full path usage ${yaml[spec.template.spec.containers[0].name]} test full path usage using path index ${yaml[spec.my_replicas]} undefined path doesn't exist ${yaml[spec.template.spec.containers]} undefined path doesn't resolve to an actual value ${yaml[spec.metadata.template.labels['app.kubernetes.io/name']]} nginx full path usage using dictionary key Full example \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : annotation-example annotations : app.komodor.com/service.link.grafana-overall-system-health : \"https://grafana.com/service/annotation-example\" app.komodor.com/service.link.datadog : \"https://datadog.com/dashboard/annotation-example\" app.komodor.com/service.link.playbook : \"https://docs.google.com/playbook\" app.komodor.com/deploy.job.jenkins : \"https://ci.jenkins-ci.org/computer/job\" app.komodor.com/deploy.link.logs : \"https://app.logz.io/#/dashboard/kibana/discover?_a=env:1.0.1\" app.komodor.com/deploy.link.sentry : \"https://sentry.io/organizations/rookoutz/issues/?project=1320440&query=sdk.version%3A1.0.1&statsPeriod=14d\" app.komodor.com/service.link.datadog : \"https://app.datadoghq.com/apm/traces?query=service%3A${service}%20kube_namespace%3A${namespace}%20env%3A${cluster}&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code&historicalData=true&messageDisplay=inline&sort=desc&streamTraces=true&start=${epochStart}&end=${epochEnd}&paused=true\" spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0.1 ports : - containerPort : 8080 protocol : TCP Annotations Best Practices \u00b6 At Komodor we believe that k8s annotations are the best method for describing services metadata. This includes all the \u201cextra\u201d fields used to tag and label your services, both for other team members and for external tools. BTW, We collect data from both annotations and labels. Where does Komodor utilize annotations? \u00b6 Everywhere! Komodor will use these annotations to create powerful connections between services and enrich service information in the following areas: Services explorer Related services Events screen Matching alerts to the correct services Official Kubernetes recommendations \u00b6 app.kubernetes.io/component : database app.kubernetes.io/part-of : wordpress app.kubernetes.io/managed-by : helm Komodor recommendations \u00b6 app.komodor.com/label.team : backend app.komodor.com/label.group : infrastructure app.komodor.com/label.owners : \"infa-team\" app.komodor.com/label.alert-team : \"devs\" app.komodor.com/label.Impacted-by : redis Usage example \u00b6 Tagging Team annotations on relevant services and adding relevant metadata on the alert metadata in datadog. Using the Team name in the alert tools (for example PagerDuty) as part of the Komodor labels.","title":"Annotations"},{"location":"Learn/Annotations.html#komodor-kubernetes-annotations","text":"Komodor annotations (AKA Komodor as Code), is a method to allow users to configure everything related to Komodor as part of their native k8s yaml. Komodor annotations should be placed in the deployment resource annotations (annotations set on the pod template are ignored)","title":"Komodor kubernetes annotations"},{"location":"Learn/Annotations.html#ci-deploy-links","text":"For each deployment version, you can add a quick link with the job url.","title":"CI-Deploy Links"},{"location":"Learn/Annotations.html#how","text":"app.komodor.com/deploy.job.name:url Example: Annotation Values Description Example app.komodor.com/deploy.job.jenkins url Link to Jenkins job that deploys the service https://ci.jenkins-ci.org/computer/job","title":"How"},{"location":"Learn/Annotations.html#deploy-links","text":"For each deployment version, you can add a quick link with the relevant filters already in place!","title":"Deploy Links"},{"location":"Learn/Annotations.html#how_1","text":"app.komodor.com/deploy.link.name:url Examples: Annotation Values Description Example app.komodor.com/deploy.link.logs url Link for the specific version logs https://app.logz.io/#/dashboard/kibana/discover?_a=env:123.0.1 app.komodor.com/deploy.link.sentry url Link for the specific version Sentry issues https://sentry.io/organizations/rookoutz/issues/?project=1320440&query=sdk.version%3A1.0.1&statsPeriod=14d","title":"How"},{"location":"Learn/Annotations.html#custom-links","text":"You can create custom links to external and internal applications by crafting your own URL to the application using a skeleton URL and placeholders provided by Komodor. Just copy the URL of the application you want to link to, identify the placeholders in the URL that are used to query the application, and replace them with placeholders for your own use. Please find the below examples as references for common applications.","title":"Custom Links"},{"location":"Learn/Annotations.html#how_2","text":"app.komodor.com/deploy.link.name:value Examples: Annotation Values Description Example app.komodor.com/deploy.link.coralogix url Link for the custom URL, coralogix https://komodortest.coralogix.com/#/query-new/logs?query=(coralogix.metadata.cluster:(%22${cluster}%22))%20AND%20(coralogix.metadata.namespace:(%22${namespace}%22))%20AND%20(coralogix.metadata.service:(%22${service}%22))&time=from:${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS},to:${timestampEnd=yyyy-MM-dd'T'HH:mm:ss.SSS} app.komodor.com/deploy.link.logzio url Link for the custom URL, logz.io https://app.logz.io/#/dashboard/kibana/discover?_a=(columns:!(message,kubernetes.namespace_name,kubernetes.container_name,params.clusterName),filters:!(('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:kubernetes.namespace_name,negate:!f,params:(query:default),type:phrase),query:(match_phrase:(kubernetes.namespace_name:${namespace}))),('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:params.clusterName,negate:!f,params:(query:main),type:phrase),query:(match_phrase:(params.clusterName:${cluster}))),('$state':(store:appState),meta:(alias:!n,disabled:!f,index:'logzioCustomerIndex',key:kubernetes.container_name,negate:!f,params:(query:k8s-events-collector),type:phrase),query:(match_phrase:(kubernetes.container_name:${service})),query:(match_phrase:(kubernetes.container_image:${container[web].image})))),index:'logzioCustomerIndex',interval:auto,query:(language:lucene,query:''),sort:!(!('@timestamp',desc)))&_g=(filters:!(),refreshInterval:(pause:!t,value:0),time:(from:'${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS}',to:'${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS}'))&discoverTab=logz-logs-tab&switchToAccountId=138828&accountIds=true app.komodor.com/deploy.link.datadog url Link for the custom URL, DataDog https://app.datadoghq.com/apm/traces?query=service%3A${service}%20kube_namespace%3A${namespace}%20env%3A${cluster}&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code&historicalData=true&messageDisplay=inline&sort=desc&streamTraces=true&start=${epochStart}&end=${epochEnd}&paused=true The following values can be used to enrich the URL: Placeholder Value Example ${epochStart} Start Time in Epoch Time ${epochEnd} End Time in Epoch Time ${service} Service Name ${namespace} Namespace Name ${cluster} Cluster Name ${failedPod} * The pod name of a failed pod that triggered this health event* ${container[<name>].image} ** Image name of a container ${container[web].image} ${timestampStart=yyyy-MM-dd'T'HH:mm:ss.SSS} Start Time in custom format*** ${timestampStart=yyyy-MM-dd} ${timestampEnd=yyyy-MM-dd'T'HH:mm:ss.SSS} End Time in custom format*** ${timestampEnd=yyyy-MM-dd} ${yaml[<spec_path>]} Full yaml's path specification ${yaml[metadata.labels.app]} *Not applicable in Service context. **Custom links with a failed pod name will be created on health events only. ***Dates can be crafted using the display guidelines of date-fns https://date-fns.org/v2.25.0/docs/format Example on how to use YAML full path: spec : replicas : 5 selector : matchLabels : app : nginx template : spec : containers : - name : test image : nginx:1.14.2 ports : - containerPort : 80 - name : test2 image : nginx:1.14.2 ports : - containerPort : 80 metadata : lables : app.kubernetes.io/name : nginx app.kubernetes.io/managed-by : helm YAML Path Value Explanation ${yaml[spec.replicas]} 5 full path usage ${yaml[spec.template.spec.containers[0].name]} test full path usage using path index ${yaml[spec.my_replicas]} undefined path doesn't exist ${yaml[spec.template.spec.containers]} undefined path doesn't resolve to an actual value ${yaml[spec.metadata.template.labels['app.kubernetes.io/name']]} nginx full path usage using dictionary key","title":"How"},{"location":"Learn/Annotations.html#full-example","text":"apiVersion : apps/v1 kind : Deployment metadata : name : annotation-example annotations : app.komodor.com/service.link.grafana-overall-system-health : \"https://grafana.com/service/annotation-example\" app.komodor.com/service.link.datadog : \"https://datadog.com/dashboard/annotation-example\" app.komodor.com/service.link.playbook : \"https://docs.google.com/playbook\" app.komodor.com/deploy.job.jenkins : \"https://ci.jenkins-ci.org/computer/job\" app.komodor.com/deploy.link.logs : \"https://app.logz.io/#/dashboard/kibana/discover?_a=env:1.0.1\" app.komodor.com/deploy.link.sentry : \"https://sentry.io/organizations/rookoutz/issues/?project=1320440&query=sdk.version%3A1.0.1&statsPeriod=14d\" app.komodor.com/service.link.datadog : \"https://app.datadoghq.com/apm/traces?query=service%3A${service}%20kube_namespace%3A${namespace}%20env%3A${cluster}&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code&historicalData=true&messageDisplay=inline&sort=desc&streamTraces=true&start=${epochStart}&end=${epochEnd}&paused=true\" spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0.1 ports : - containerPort : 8080 protocol : TCP","title":"Full example"},{"location":"Learn/Annotations.html#annotations-best-practices","text":"At Komodor we believe that k8s annotations are the best method for describing services metadata. This includes all the \u201cextra\u201d fields used to tag and label your services, both for other team members and for external tools. BTW, We collect data from both annotations and labels.","title":"Annotations Best Practices"},{"location":"Learn/Annotations.html#where-does-komodor-utilize-annotations","text":"Everywhere! Komodor will use these annotations to create powerful connections between services and enrich service information in the following areas: Services explorer Related services Events screen Matching alerts to the correct services","title":"Where does Komodor utilize annotations?"},{"location":"Learn/Annotations.html#official-kubernetes-recommendations","text":"app.kubernetes.io/component : database app.kubernetes.io/part-of : wordpress app.kubernetes.io/managed-by : helm","title":"Official Kubernetes recommendations"},{"location":"Learn/Annotations.html#komodor-recommendations","text":"app.komodor.com/label.team : backend app.komodor.com/label.group : infrastructure app.komodor.com/label.owners : \"infa-team\" app.komodor.com/label.alert-team : \"devs\" app.komodor.com/label.Impacted-by : redis","title":"Komodor recommendations"},{"location":"Learn/Annotations.html#usage-example","text":"Tagging Team annotations on relevant services and adding relevant metadata on the alert metadata in datadog. Using the Team name in the alert tools (for example PagerDuty) as part of the Komodor labels.","title":"Usage example"},{"location":"Learn/Interaction-With-The-Cluster.html","text":"Interaction with the Cluster \u00b6 Interaction with the cluster allows you to speed up the troubleshooting process. This is done by asking Komodor's agent to perform actions in the cluster. Prerequisites \u00b6 Install Komodor's watcher (version >=0.1.44 ) --set watcher.enableAgentTaskExecution=true to start the agent with the feature turned on (required for Describe Action ) Extra Permissions Required \u00b6 In order to get logs from the cluster please use --set watcher.allowReadingPodLogs=true to update the RBAC manifests with the required permissions (required for Pod Log Action ) You can turn any of these flags off at any time to disable the features Upgrade \u00b6 helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher --set watcher.enableAgentTaskExecution = true --set watcher.allowReadingPodLogs = true --reuse-values Live Pods \u00b6 In the service, click on the Pod Status and Logs button. The table shows all the pods that belong to the service based on the pod owner controller. Actions \u00b6 Pod Logs \u00b6 Request logs from one of the pods will stream back the last 100 logs from the pod. When a pod was previously restarted by Kubernetes you can see the logs just before the pod was restarted. Pod Description \u00b6 Request returns the same output as kubectl describe pod [NAME]","title":"Interaction with the Cluster"},{"location":"Learn/Interaction-With-The-Cluster.html#interaction-with-the-cluster","text":"Interaction with the cluster allows you to speed up the troubleshooting process. This is done by asking Komodor's agent to perform actions in the cluster.","title":"Interaction with the Cluster"},{"location":"Learn/Interaction-With-The-Cluster.html#prerequisites","text":"Install Komodor's watcher (version >=0.1.44 ) --set watcher.enableAgentTaskExecution=true to start the agent with the feature turned on (required for Describe Action )","title":"Prerequisites"},{"location":"Learn/Interaction-With-The-Cluster.html#extra-permissions-required","text":"In order to get logs from the cluster please use --set watcher.allowReadingPodLogs=true to update the RBAC manifests with the required permissions (required for Pod Log Action ) You can turn any of these flags off at any time to disable the features","title":"Extra Permissions Required"},{"location":"Learn/Interaction-With-The-Cluster.html#upgrade","text":"helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher --set watcher.enableAgentTaskExecution = true --set watcher.allowReadingPodLogs = true --reuse-values","title":"Upgrade"},{"location":"Learn/Interaction-With-The-Cluster.html#live-pods","text":"In the service, click on the Pod Status and Logs button. The table shows all the pods that belong to the service based on the pod owner controller.","title":"Live Pods"},{"location":"Learn/Interaction-With-The-Cluster.html#actions","text":"","title":"Actions"},{"location":"Learn/Interaction-With-The-Cluster.html#pod-logs","text":"Request logs from one of the pods will stream back the last 100 logs from the pod. When a pod was previously restarted by Kubernetes you can see the logs just before the pod was restarted.","title":"Pod Logs"},{"location":"Learn/Interaction-With-The-Cluster.html#pod-description","text":"Request returns the same output as kubectl describe pod [NAME]","title":"Pod Description"},{"location":"Learn/Komodor-Agent.html","text":"The Komodor Agent \u00b6 Installation \u00b6 Get an API Key \u00b6 The API key can be found in the Integration page . Helm \u00b6 helm repo add komodorio https://helm-charts.komodor.io helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher \\ --set apiKey = YOUR_API_KEY_HERE \\ --set watcher.clusterName = CLUSTER_NAME \\ --set watcher.enableAgentTaskExecution = true \\ --set watcher.allowReadingPodLogs = true Kustomize \u00b6 export KOMOKW_API_KEY = # API KEY Required export KOMOKW_CLUSTER_NAME = # Optional kubectl create ns komodor kubectl apply -n komodor -k https://github.com/komodorio/helm-charts/manifests/overlays/full/?ref = master Permissions \u00b6 The Komodor agent uses the native RBAC model of Kubernetes. All the permissions are listed here: helm kustomize base , kustomize final ARM Support \u00b6 Arm64 image is supported via docker manifest. Advanced Configuration \u00b6 You can configure the agent's functionality using the following configuration file: komodor-k8s-watcher.yaml (assuming the RBAC permissions are satisfied). A more detailed list of the configurable parameters can be found here Data Redaction \u00b6 Learn how to set up data redaction in Komodor Resources \u00b6 By default, the Komodor agent watches the majority of the resources in your cluster ( secrets and events are opt out ) You can enable/disable watching a resource using the following command: Helm: --set watcher.resources.RESOURCE=true/off Kustomize: update the configuration file and the RBAC rule to have get , list and watch permissions Namespaces \u00b6 The Komodor agent watches all the namespaces (by default watchNamespace=all ) To watch a single namespace use the following command: Helm: --set watcher.watchNamespace=NAMESPACE Kustomize: patch the configuration file watchNamespace=NAMESPACE Denylist \u00b6 Using namespacesDenylist you can opt list of namespaces Agent Tasks \u00b6 Agent tasks are used to interact with the cluster on demand, read more about interaction with the cluster here To enable agent tasks (default is off ): Helm: --set watcher.enableAgentTaskExecution=true && --set watcher.allowReadingPodLogs=true Kustomize: The full overlay already has this turned on. If you are building it manually from base , patch the configuration file enableAgentTaskExecution=true and make sure to have RBAC permissions to get and list for pods and pods/log Environment Variables \u00b6 Alternativly, you can pass the configuration as environment variables using the KOMOKW_ prefix and by replacing all the . to _, for the root items the camelcase transforms into underscores as well. For example: # apiKey KOMOKW_API_KEY = 1a2b3c4d5e6f7g7h # watcher.resources.replicaSet KOMOKW_RESOURCES_REPLICASET = false # watcher.watchNamespace KOMOKW_WATCH_NAMESPACE = my-namespace # watcher.collectHistory KOMOKW_COLLECT_HISTORY = true Updating the agent \u00b6 Kustomize \u00b6 kubectl apply -n komodor -k https://github.com/komodorio/helm-charts/manifests/overlays/full/?ref = master Helm \u00b6 helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher --reuse-values Uninstalling \u00b6 Kustomize \u00b6 kubectl delete ns komodor Helm \u00b6 helm uninstall k8s-watcher","title":"Komodor's Agent"},{"location":"Learn/Komodor-Agent.html#the-komodor-agent","text":"","title":"The Komodor Agent"},{"location":"Learn/Komodor-Agent.html#installation","text":"","title":"Installation"},{"location":"Learn/Komodor-Agent.html#get-an-api-key","text":"The API key can be found in the Integration page .","title":"Get an API Key"},{"location":"Learn/Komodor-Agent.html#helm","text":"helm repo add komodorio https://helm-charts.komodor.io helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher \\ --set apiKey = YOUR_API_KEY_HERE \\ --set watcher.clusterName = CLUSTER_NAME \\ --set watcher.enableAgentTaskExecution = true \\ --set watcher.allowReadingPodLogs = true","title":"Helm"},{"location":"Learn/Komodor-Agent.html#kustomize","text":"export KOMOKW_API_KEY = # API KEY Required export KOMOKW_CLUSTER_NAME = # Optional kubectl create ns komodor kubectl apply -n komodor -k https://github.com/komodorio/helm-charts/manifests/overlays/full/?ref = master","title":"Kustomize"},{"location":"Learn/Komodor-Agent.html#permissions","text":"The Komodor agent uses the native RBAC model of Kubernetes. All the permissions are listed here: helm kustomize base , kustomize final","title":"Permissions"},{"location":"Learn/Komodor-Agent.html#arm-support","text":"Arm64 image is supported via docker manifest.","title":"ARM Support"},{"location":"Learn/Komodor-Agent.html#advanced-configuration","text":"You can configure the agent's functionality using the following configuration file: komodor-k8s-watcher.yaml (assuming the RBAC permissions are satisfied). A more detailed list of the configurable parameters can be found here","title":"Advanced Configuration"},{"location":"Learn/Komodor-Agent.html#data-redaction","text":"Learn how to set up data redaction in Komodor","title":"Data Redaction"},{"location":"Learn/Komodor-Agent.html#resources","text":"By default, the Komodor agent watches the majority of the resources in your cluster ( secrets and events are opt out ) You can enable/disable watching a resource using the following command: Helm: --set watcher.resources.RESOURCE=true/off Kustomize: update the configuration file and the RBAC rule to have get , list and watch permissions","title":"Resources"},{"location":"Learn/Komodor-Agent.html#namespaces","text":"The Komodor agent watches all the namespaces (by default watchNamespace=all ) To watch a single namespace use the following command: Helm: --set watcher.watchNamespace=NAMESPACE Kustomize: patch the configuration file watchNamespace=NAMESPACE","title":"Namespaces"},{"location":"Learn/Komodor-Agent.html#denylist","text":"Using namespacesDenylist you can opt list of namespaces","title":"Denylist"},{"location":"Learn/Komodor-Agent.html#agent-tasks","text":"Agent tasks are used to interact with the cluster on demand, read more about interaction with the cluster here To enable agent tasks (default is off ): Helm: --set watcher.enableAgentTaskExecution=true && --set watcher.allowReadingPodLogs=true Kustomize: The full overlay already has this turned on. If you are building it manually from base , patch the configuration file enableAgentTaskExecution=true and make sure to have RBAC permissions to get and list for pods and pods/log","title":"Agent Tasks"},{"location":"Learn/Komodor-Agent.html#environment-variables","text":"Alternativly, you can pass the configuration as environment variables using the KOMOKW_ prefix and by replacing all the . to _, for the root items the camelcase transforms into underscores as well. For example: # apiKey KOMOKW_API_KEY = 1a2b3c4d5e6f7g7h # watcher.resources.replicaSet KOMOKW_RESOURCES_REPLICASET = false # watcher.watchNamespace KOMOKW_WATCH_NAMESPACE = my-namespace # watcher.collectHistory KOMOKW_COLLECT_HISTORY = true","title":"Environment Variables"},{"location":"Learn/Komodor-Agent.html#updating-the-agent","text":"","title":"Updating the agent"},{"location":"Learn/Komodor-Agent.html#kustomize_1","text":"kubectl apply -n komodor -k https://github.com/komodorio/helm-charts/manifests/overlays/full/?ref = master","title":"Kustomize"},{"location":"Learn/Komodor-Agent.html#helm_1","text":"helm repo update helm upgrade --install k8s-watcher komodorio/k8s-watcher --reuse-values","title":"Helm"},{"location":"Learn/Komodor-Agent.html#uninstalling","text":"","title":"Uninstalling"},{"location":"Learn/Komodor-Agent.html#kustomize_2","text":"kubectl delete ns komodor","title":"Kustomize"},{"location":"Learn/Komodor-Agent.html#helm_2","text":"helm uninstall k8s-watcher","title":"Helm"},{"location":"Learn/ManageUsers.html","text":"How to Manage Users \u00b6 This article will detail how to Invite, Modify and Delete users in the Komodor platform. How to Invite/Add a new user \u00b6 To invite another user click on the manage team icon in the top right corner, this will take you to the \"Manage Team\" page. Note: You must have the Admin role in order to invite another Admin to the platform. Click on \"Add Member\". Provide the users Full Name, Email Address and select a role for the user and then click \"Send Invite\" to invite the user to the platform. Note: More on user roles can be found here . The user will receive an invitation to the platform, click \"Close\" to finish. How to modify an existing user \u00b6 Click on the manage user icon in the top right corner, this will take you to the \"Manage Team\" page. You will see a list of users for the account, select \"Edit\" to the far right of the user you wish to modify. In the \"Edit Member\" dialogue, modify the users Full Name or Role and click on \"Save Details\" to save the changes. Note: In order to change a users email address you will need to create a new user by inviting them to the platform using their new email address. How to delete a user \u00b6 Click on the manage user icon in the top right corner, this will take you to the \"Manage Team\" page. You will see a list of users for the account, select \"Edit\" beside the user you wish to delete. On the \"Edit Member\" dialogue, click on the red garbage bin in the top right corner to delete the user. Click on \"Yes, Remove\" to delete the user.","title":"Manage Users"},{"location":"Learn/ManageUsers.html#how-to-manage-users","text":"This article will detail how to Invite, Modify and Delete users in the Komodor platform.","title":"How to Manage Users"},{"location":"Learn/ManageUsers.html#how-to-inviteadd-a-new-user","text":"To invite another user click on the manage team icon in the top right corner, this will take you to the \"Manage Team\" page. Note: You must have the Admin role in order to invite another Admin to the platform. Click on \"Add Member\". Provide the users Full Name, Email Address and select a role for the user and then click \"Send Invite\" to invite the user to the platform. Note: More on user roles can be found here . The user will receive an invitation to the platform, click \"Close\" to finish.","title":"How to Invite/Add a new user"},{"location":"Learn/ManageUsers.html#how-to-modify-an-existing-user","text":"Click on the manage user icon in the top right corner, this will take you to the \"Manage Team\" page. You will see a list of users for the account, select \"Edit\" to the far right of the user you wish to modify. In the \"Edit Member\" dialogue, modify the users Full Name or Role and click on \"Save Details\" to save the changes. Note: In order to change a users email address you will need to create a new user by inviting them to the platform using their new email address.","title":"How to modify an existing user"},{"location":"Learn/ManageUsers.html#how-to-delete-a-user","text":"Click on the manage user icon in the top right corner, this will take you to the \"Manage Team\" page. You will see a list of users for the account, select \"Edit\" beside the user you wish to delete. On the \"Edit Member\" dialogue, click on the red garbage bin in the top right corner to delete the user. Click on \"Yes, Remove\" to delete the user.","title":"How to delete a user"},{"location":"Learn/Metrics.html","text":"Metrics \u00b6 Komodor enables you to overview metrics like cpu and memory on your cluster using the prometheus metrics server. Prerequisites \u00b6 Agent version from 0.1.108 Installed prometheus on your cluster Coming soon: prometheus installation integration! How does it work? \u00b6 Komodor agent identifies Prometheus in the cluster and saves the configuration details. The agent sends an HTTP request to the Prometheus metrics server and gets matric results like CPU and memory. The received data is processed and displayed in the Pods and Nodes screens. Pods: \u00b6 Data is displayed in 2 columns: %CPU/R - percentage of cpu usage per request %MEM/R - percentage of memory usage per request More columns can be added: %CPU/L, %MEM/L, CPU, Memory Nodes: \u00b6 Data is displayed in 3 columns: %CPU - utilization of CPU allocation in percentage %Memory - utilization of memory allocation in percentage %Disk - utilization of disk capacity in percentage More usage columns can be added: CPU, Memory, Disk Add metrics integration \u00b6 In case you have prometheus in your cluster and the agent doesn\u2019t identify it you can add metrics integration. Steps: \u00b6 Go to integration screen and click on Prometheus metrics server Enter the fqdn as follows: <namespace>/<service>:<port> Namespace - prometheus service namespace Service - prometheus service name Port - The port that prometheus service is listening to Choose how you installed prometheus: helm or other operator Click install","title":"Metrics"},{"location":"Learn/Metrics.html#metrics","text":"Komodor enables you to overview metrics like cpu and memory on your cluster using the prometheus metrics server.","title":"Metrics"},{"location":"Learn/Metrics.html#prerequisites","text":"Agent version from 0.1.108 Installed prometheus on your cluster Coming soon: prometheus installation integration!","title":"Prerequisites"},{"location":"Learn/Metrics.html#how-does-it-work","text":"Komodor agent identifies Prometheus in the cluster and saves the configuration details. The agent sends an HTTP request to the Prometheus metrics server and gets matric results like CPU and memory. The received data is processed and displayed in the Pods and Nodes screens.","title":"How does it work?"},{"location":"Learn/Metrics.html#pods","text":"Data is displayed in 2 columns: %CPU/R - percentage of cpu usage per request %MEM/R - percentage of memory usage per request More columns can be added: %CPU/L, %MEM/L, CPU, Memory","title":"Pods:"},{"location":"Learn/Metrics.html#nodes","text":"Data is displayed in 3 columns: %CPU - utilization of CPU allocation in percentage %Memory - utilization of memory allocation in percentage %Disk - utilization of disk capacity in percentage More usage columns can be added: CPU, Memory, Disk","title":"Nodes:"},{"location":"Learn/Metrics.html#add-metrics-integration","text":"In case you have prometheus in your cluster and the agent doesn\u2019t identify it you can add metrics integration.","title":"Add metrics integration"},{"location":"Learn/Metrics.html#steps","text":"Go to integration screen and click on Prometheus metrics server Enter the fqdn as follows: <namespace>/<service>:<port> Namespace - prometheus service namespace Service - prometheus service name Port - The port that prometheus service is listening to Choose how you installed prometheus: helm or other operator Click install","title":"Steps:"},{"location":"Learn/Monitors.html","text":"Monitors \u00b6 Komodor Monitors are built to detect different scenarios, investigate certain aspects around them and provide additional context to simplify the troubleshooting process and reduce the MTTR. Supported configurations per Monitor: \u00b6 Komodor monitors are being configured on a cluster level, each monitor supports the following configurations: - Trigger conditions - specify when this monitor should be triggered, conditions vary per monitor. - Scope - which resources should be monitored, the scope can be configured for the entire cluster, specific namespaces, annotations or labels, the relationship between the selected resources is currently OR relationship. - Sink/Notification - where do you want to receive notifications (Slack/Teams/OpsGenie/PagerDuty). Prerequisites: \u00b6 Required agent version - 1.0.78 (recommended - latest version) Availability Monitor \u00b6 Monitor your workload\u2019s health (available replicas < desired replicas), creates an Availability issue on the Events and Service timelines that provides relevant information to resolve the issue. The Availability monitor will not be triggered during an active rollout. Please note Modifying the scope of an Availability monitor might affect (remove) events from the timeline. Monitor is triggered by - Service (Deployment/DaemonSet/Rollout/StatefulSet) number of available replicas < desired replicas by the specified conditions for the defined duration The following checks are performed - Pods health Foreach Pod we'll provide the following: Phase, Reason, Pod events Containers list with their state, reason & logs Service latest deployments Service describe Please note Data provided in the Availability issue checks is a snapshot in time for when the issue occurred. Deploy Monitor \u00b6 A Deploy monitor will be trigerred whenever a resource is being deployed/rolled-out. Using the Deploy Monitor configuration you can define on what resources (scope) and in what occasion (failed deploy/successful deploy/both) when you would like to get a notification in one of your notification channel (Slack/Teams) Node Monitor \u00b6 Monitors Nodes with faulty Conditions . Monitor is triggered by - Node Conditions change to a faulty Condition, the faulty condition/s last through the configured Duration We perform the following checks as part of our investigation Is the node ready? Is the node overcommitted? Is the node under pressure? Are system Pods healthy? Is the network available? Are Pods being evicted? -Are user pods healthy? Is the node scheduable? Node overall resource consumption including top 5 pod consumers (requires metric-server installed) Notes The Node detector currently does not deal with nodes in an Unknown state (this means Spot interruptions or scale-down events will not be handled by the WF, could affect other scenarios as well) Will only run on Nodes that are created for more than 3 minutes (there is a 3-minute delay from Node create time prior to running the WF) PVC Monitor \u00b6 Monitors PVCs in a pending state. Triggered by - PVC in a pending state for the defined duration We perform the following checks as part of our investigation PVC creation, utilization, and readiness issues Volume provisioner related issues PVC spec change Identify the impact on your services Job Monitor \u00b6 The Job Monitor will be triggered when a job fails execution. It allows you to get notified for Job failures on the defined scope. CronJob Monitor \u00b6 The CronJob Monitor will be triggered when a job (managed by CronJob) fails execution. It allows you to get notified for the first (first failure ater a success) or any CronJob failures on the defined scope.","title":"Monitors"},{"location":"Learn/Monitors.html#monitors","text":"Komodor Monitors are built to detect different scenarios, investigate certain aspects around them and provide additional context to simplify the troubleshooting process and reduce the MTTR.","title":"Monitors"},{"location":"Learn/Monitors.html#supported-configurations-per-monitor","text":"Komodor monitors are being configured on a cluster level, each monitor supports the following configurations: - Trigger conditions - specify when this monitor should be triggered, conditions vary per monitor. - Scope - which resources should be monitored, the scope can be configured for the entire cluster, specific namespaces, annotations or labels, the relationship between the selected resources is currently OR relationship. - Sink/Notification - where do you want to receive notifications (Slack/Teams/OpsGenie/PagerDuty).","title":"Supported configurations per Monitor:"},{"location":"Learn/Monitors.html#prerequisites","text":"Required agent version - 1.0.78 (recommended - latest version)","title":"Prerequisites:"},{"location":"Learn/Monitors.html#availability-monitor","text":"Monitor your workload\u2019s health (available replicas < desired replicas), creates an Availability issue on the Events and Service timelines that provides relevant information to resolve the issue. The Availability monitor will not be triggered during an active rollout. Please note Modifying the scope of an Availability monitor might affect (remove) events from the timeline. Monitor is triggered by - Service (Deployment/DaemonSet/Rollout/StatefulSet) number of available replicas < desired replicas by the specified conditions for the defined duration The following checks are performed - Pods health Foreach Pod we'll provide the following: Phase, Reason, Pod events Containers list with their state, reason & logs Service latest deployments Service describe Please note Data provided in the Availability issue checks is a snapshot in time for when the issue occurred.","title":"Availability Monitor"},{"location":"Learn/Monitors.html#deploy-monitor","text":"A Deploy monitor will be trigerred whenever a resource is being deployed/rolled-out. Using the Deploy Monitor configuration you can define on what resources (scope) and in what occasion (failed deploy/successful deploy/both) when you would like to get a notification in one of your notification channel (Slack/Teams)","title":"Deploy Monitor"},{"location":"Learn/Monitors.html#node-monitor","text":"Monitors Nodes with faulty Conditions . Monitor is triggered by - Node Conditions change to a faulty Condition, the faulty condition/s last through the configured Duration We perform the following checks as part of our investigation Is the node ready? Is the node overcommitted? Is the node under pressure? Are system Pods healthy? Is the network available? Are Pods being evicted? -Are user pods healthy? Is the node scheduable? Node overall resource consumption including top 5 pod consumers (requires metric-server installed) Notes The Node detector currently does not deal with nodes in an Unknown state (this means Spot interruptions or scale-down events will not be handled by the WF, could affect other scenarios as well) Will only run on Nodes that are created for more than 3 minutes (there is a 3-minute delay from Node create time prior to running the WF)","title":"Node Monitor"},{"location":"Learn/Monitors.html#pvc-monitor","text":"Monitors PVCs in a pending state. Triggered by - PVC in a pending state for the defined duration We perform the following checks as part of our investigation PVC creation, utilization, and readiness issues Volume provisioner related issues PVC spec change Identify the impact on your services","title":"PVC Monitor"},{"location":"Learn/Monitors.html#job-monitor","text":"The Job Monitor will be triggered when a job fails execution. It allows you to get notified for Job failures on the defined scope.","title":"Job Monitor"},{"location":"Learn/Monitors.html#cronjob-monitor","text":"The CronJob Monitor will be triggered when a job (managed by CronJob) fails execution. It allows you to get notified for the first (first failure ater a success) or any CronJob failures on the defined scope.","title":"CronJob Monitor"},{"location":"Learn/RBAC.html","text":"Role-Based Access Control \u00b6 Intro \u00b6 Komodor allows assigning users with Roles to control their access and permissions (such as what data they can read or which resources they can modify). Komodor Roles \u00b6 Roles are built from one or more policies. Built-in Roles \u00b6 account-admin - has full permissions on the account viewer - has access to view all resources on the account developer - has access to view all resources and perform the following basic actions: \"delete:pod\", \"scale:deployment\", \"scale:statefulset\", \"restart:deployment\", \"restart:statefulset\", \"rerun:job\" Learn more about actions Please note - The built-in account-admin role and policy cannot be modified, there has to be at-least one account-admin on the account, the last account-admin cannot be removed or provided with a different role. Komodor Policies \u00b6 Policies define a set of actions & resources they can performed at. A policy is built from a list of Statements, formatted as follows: [ { \"actions\": [], \"resources\": [] }, { \"actions\": [], \"resources\": [] } ] Actions \u00b6 A list of allowed actions, formatted as follows: action:supported-resource-type List of the supported combinations: Action Supported Resource Types view * (all) edit * (all), deployment, statefulset, daemonset, replicaset, jobs, cronjob, configmap, secret, service, ingress delete * (all), deployment, statefulset, daemonset, replicaset, jobs, cronjob, pvc, pv, storageclasse, secret, service, ingress scale deployment, statefulset restart deployment, statefulset, daemonset manage users, monitors, integrations run cronjob rerun job Please note : view:all permission is required in any policy to allow viewing anything in Komodor, you can limit the allowed access using the Resources clause manage permissions cannot be scoped, once provided the user will have access to manage all resources of the provided category Only account-admin s or users with manage:users permission can see the Roles & Policies pages under in the settings section Resources \u00b6 A list of resources, formatted as follows: { \"cluster\": \"*\", \"namespaces\": [] } The cluster clause supports specifying a specific cluster or \"*\" (any). The namespaces clause is optional and allows specifying a list of one or more namespaces. Policy Examples \u00b6 1 - The following policy allows performing the following actions on resources running in the kube-system namespaces on all clusters and on namespaces brain and k8s-watcher on the cluster named main - Deletion of Pods - Scaling all supported resource types - Restarting all supported resource types [ { \"actions\": [ \"view:all\" \"delete:pod\", \"scale:deployment\", \"restart:deployment\", ], \"resources\": [ { \"cluster\": \"main\", \"namespaces\": [ \"brain\", \"k8s-watcher\" ] }, { \"cluster\": \"*\", \"namespaces\": [ \"kube-system\" ] } ] } ] 2 - The following policy allows viewing all resources and managing all Komodor configurations for all clusters [ { \"actions\": [ \"view:all\" \"manage:monitors\" ], \"resources\": [ { \"cluster\": \"*\", } ] } ] Policy Creation \u00b6 You can easily create new policies using the Komodor platform Access the settings page Go to the Policies page Add policy You will now enter a policy creation wizard, you can easily manage your policies with it Create a role associated with this policy Associate the role to an existing/new user/s","title":"RBAC"},{"location":"Learn/RBAC.html#role-based-access-control","text":"","title":"Role-Based Access Control"},{"location":"Learn/RBAC.html#intro","text":"Komodor allows assigning users with Roles to control their access and permissions (such as what data they can read or which resources they can modify).","title":"Intro"},{"location":"Learn/RBAC.html#komodor-roles","text":"Roles are built from one or more policies.","title":"Komodor Roles"},{"location":"Learn/RBAC.html#built-in-roles","text":"account-admin - has full permissions on the account viewer - has access to view all resources on the account developer - has access to view all resources and perform the following basic actions: \"delete:pod\", \"scale:deployment\", \"scale:statefulset\", \"restart:deployment\", \"restart:statefulset\", \"rerun:job\" Learn more about actions Please note - The built-in account-admin role and policy cannot be modified, there has to be at-least one account-admin on the account, the last account-admin cannot be removed or provided with a different role.","title":"Built-in Roles"},{"location":"Learn/RBAC.html#komodor-policies","text":"Policies define a set of actions & resources they can performed at. A policy is built from a list of Statements, formatted as follows: [ { \"actions\": [], \"resources\": [] }, { \"actions\": [], \"resources\": [] } ]","title":"Komodor Policies"},{"location":"Learn/RBAC.html#actions","text":"A list of allowed actions, formatted as follows: action:supported-resource-type List of the supported combinations: Action Supported Resource Types view * (all) edit * (all), deployment, statefulset, daemonset, replicaset, jobs, cronjob, configmap, secret, service, ingress delete * (all), deployment, statefulset, daemonset, replicaset, jobs, cronjob, pvc, pv, storageclasse, secret, service, ingress scale deployment, statefulset restart deployment, statefulset, daemonset manage users, monitors, integrations run cronjob rerun job Please note : view:all permission is required in any policy to allow viewing anything in Komodor, you can limit the allowed access using the Resources clause manage permissions cannot be scoped, once provided the user will have access to manage all resources of the provided category Only account-admin s or users with manage:users permission can see the Roles & Policies pages under in the settings section","title":"Actions"},{"location":"Learn/RBAC.html#resources","text":"A list of resources, formatted as follows: { \"cluster\": \"*\", \"namespaces\": [] } The cluster clause supports specifying a specific cluster or \"*\" (any). The namespaces clause is optional and allows specifying a list of one or more namespaces.","title":"Resources"},{"location":"Learn/RBAC.html#policy-examples","text":"1 - The following policy allows performing the following actions on resources running in the kube-system namespaces on all clusters and on namespaces brain and k8s-watcher on the cluster named main - Deletion of Pods - Scaling all supported resource types - Restarting all supported resource types [ { \"actions\": [ \"view:all\" \"delete:pod\", \"scale:deployment\", \"restart:deployment\", ], \"resources\": [ { \"cluster\": \"main\", \"namespaces\": [ \"brain\", \"k8s-watcher\" ] }, { \"cluster\": \"*\", \"namespaces\": [ \"kube-system\" ] } ] } ] 2 - The following policy allows viewing all resources and managing all Komodor configurations for all clusters [ { \"actions\": [ \"view:all\" \"manage:monitors\" ], \"resources\": [ { \"cluster\": \"*\", } ] } ]","title":"Policy Examples"},{"location":"Learn/RBAC.html#policy-creation","text":"You can easily create new policies using the Komodor platform Access the settings page Go to the Policies page Add policy You will now enter a policy creation wizard, you can easily manage your policies with it Create a role associated with this policy Associate the role to an existing/new user/s","title":"Policy Creation"},{"location":"Learn/Sensitive-Information-Redaction.html","text":"Sensitive data redaction in Komodor\u2019s k8s-watcher \u00b6 What is it \u00b6 It\u2019s likely that there are values you don\u2019t want to send to Komodor as plain text. Kubernetes Secrets, for instance, ConfigMap sensitive values, container environment variables or pod logs. When configured - we will redact the specific value. That way Komodor won't see any sensitive data while you will still see configuration diff. How to integrate \u00b6 Inside komodor-k8s-watcher.yaml you should add a list of string or regular expressions under redact and redactLogs key as such: komodor-k8s-watcher watchNamespace : all namespacesBlacklist : - kube-system redact : - \"PG_.*\" - \".*PASSWORD.*\" redactLogs : - \"password=(.+?)\\b\" - \"(?U)\\\"sessionId\\\": (\\\".+\\\"{1})\" nameBlacklist : [ \"leader\" , \"election\" ] collectHistory : false How to integrate using helm upgrade command \u00b6 helm upgrade --install k8s-watcher komodorio/k8s-watcher --set watcher.redact = \"{.*PASSWORD.*,.*password.*,.*KEY.*,.*key.*,.*SECRET.*,.*secret.*}\" --set watcher.redactLogs = \"{password=(.+?)\\b,(?U)\\\"sessionId\\\": (\\\".+\\\"{1})}\" --set apiKey = <API-KEY> --set watcher.clusterName = <cluster-name> --set watcher.enableAgentTaskExecution = true --set watcher.allowReadingPodLogs = true How to integrate using environment variables \u00b6 Separate multiple values with a whitespace in the environment variable value. To include a whitespace in the patterns to redact, make sure to use \\s as it the patterns are regexp. export KOMOKW_REDACT = \".*password.* PG_.*\" export KOMOKW_REDACT_LOGS = \"password=(.+?)\\b (?U)\\\"sessionId\\\": (\\\".+\\\"{1})\" Secret Resource \u00b6 By default, Komodor\u2019s agent is hashing all secrets values. ConfigMap resource \u00b6 You can preconfigure a list of keys for Kubernetes watcher to also redact specific values from ConfigMap. komodor-k8s-watcher.yaml: redact : - \"SENTRY_API_KEY\" - \"PG_.*\" configmap.yaml: apiVersion : v1 kind : ConfigMap metadata : Name : sensitive-config-map data : SENTRY_API_KEY : super_secret PG_SECRET : super_secret PG_USERNAME : super_secret All the above \u201csuper_secret\u201d will be sent has hashed value. Deployment resource \u00b6 Komodor\u2019s agent will hash template.spec.template.[containeres|initContainers].env list of variables inside Deployment objects for pre-configured list of keys or list of regular expressions. komodor-k8s-watcher.yaml: redact : - \"SENTRY_API_KEY\" - \"PG_.*\" deployment.yaml: apiVersion : apps/v1 kind : Deployment metadata : name : sensitive-deployment spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 env : - name : PG_USERNAME value : super_secret - name : SECRET value : this_will_show_up In the above deployment example we will not send the secret values for PG_USERNAME. SECRET will show up as is due to the fact it won\u2019t match any string or regex in our configuration. Pod Logs \u00b6 Note: Pod Logs redaction is available starting from Komodor Agent version 0.1.126 Komodor's agent will redact any logs matching one of the patterns set in the redactLogs configuration. komodor-k8s-watcher.yaml: redactLogs : - \"password=(.+?)\\b\" - \"(?U)\\\"sessionId\\\": (\\\".+\\\"{1})\" Environment variables: export KOMOKW_REDACT_LOGS = \"password=(.+?)\\b (?U)\\\"sessionId\\\": (\\\".+\\\"{1})\" Example logs: INPUT : example my password=supersecre t a n d some t hi n g else OUTPUT : example my <REDACTED> a n d some t hi n g else INPUT : { \"level\" : \"INFO\" , \"message\" : \"User has added Item 12453 to Basket\" , \"sessionId\" : \"SESS456\" , \"timestamp\" : 1634477804 } OUTPUT : { \"level\" : \"INFO\" , \"message\" : \"User has added Item 12453 to Basket\" , <REDACTED> , \"timestamp\" : 1634477804 } Testing logs redaction patterns \u00b6 You can easily test the patterns you want to configure before deploying by using our docker image and our utilities command. \u276f docker run --rm -e KOMOKW_REDACT_LOGS = \"redaction\" komodorio/k8s-watcher test -logredactor -inputlog = \"The log line you want to test redaction here\" Patterns to redact: [ redaction ] Input log ( before redaction ) : The log line you want to test redaction here Output log ( after redaction ) : The log line you want to test <REDACTED> here","title":"Sensitive Information Redaction"},{"location":"Learn/Sensitive-Information-Redaction.html#sensitive-data-redaction-in-komodors-k8s-watcher","text":"","title":"Sensitive data redaction in Komodor\u2019s k8s-watcher"},{"location":"Learn/Sensitive-Information-Redaction.html#what-is-it","text":"It\u2019s likely that there are values you don\u2019t want to send to Komodor as plain text. Kubernetes Secrets, for instance, ConfigMap sensitive values, container environment variables or pod logs. When configured - we will redact the specific value. That way Komodor won't see any sensitive data while you will still see configuration diff.","title":"What is it"},{"location":"Learn/Sensitive-Information-Redaction.html#how-to-integrate","text":"Inside komodor-k8s-watcher.yaml you should add a list of string or regular expressions under redact and redactLogs key as such: komodor-k8s-watcher watchNamespace : all namespacesBlacklist : - kube-system redact : - \"PG_.*\" - \".*PASSWORD.*\" redactLogs : - \"password=(.+?)\\b\" - \"(?U)\\\"sessionId\\\": (\\\".+\\\"{1})\" nameBlacklist : [ \"leader\" , \"election\" ] collectHistory : false","title":"How to integrate"},{"location":"Learn/Sensitive-Information-Redaction.html#how-to-integrate-using-helm-upgrade-command","text":"helm upgrade --install k8s-watcher komodorio/k8s-watcher --set watcher.redact = \"{.*PASSWORD.*,.*password.*,.*KEY.*,.*key.*,.*SECRET.*,.*secret.*}\" --set watcher.redactLogs = \"{password=(.+?)\\b,(?U)\\\"sessionId\\\": (\\\".+\\\"{1})}\" --set apiKey = <API-KEY> --set watcher.clusterName = <cluster-name> --set watcher.enableAgentTaskExecution = true --set watcher.allowReadingPodLogs = true","title":"How to integrate using helm upgrade command"},{"location":"Learn/Sensitive-Information-Redaction.html#how-to-integrate-using-environment-variables","text":"Separate multiple values with a whitespace in the environment variable value. To include a whitespace in the patterns to redact, make sure to use \\s as it the patterns are regexp. export KOMOKW_REDACT = \".*password.* PG_.*\" export KOMOKW_REDACT_LOGS = \"password=(.+?)\\b (?U)\\\"sessionId\\\": (\\\".+\\\"{1})\"","title":"How to integrate using environment variables"},{"location":"Learn/Sensitive-Information-Redaction.html#secret-resource","text":"By default, Komodor\u2019s agent is hashing all secrets values.","title":"Secret Resource"},{"location":"Learn/Sensitive-Information-Redaction.html#configmap-resource","text":"You can preconfigure a list of keys for Kubernetes watcher to also redact specific values from ConfigMap. komodor-k8s-watcher.yaml: redact : - \"SENTRY_API_KEY\" - \"PG_.*\" configmap.yaml: apiVersion : v1 kind : ConfigMap metadata : Name : sensitive-config-map data : SENTRY_API_KEY : super_secret PG_SECRET : super_secret PG_USERNAME : super_secret All the above \u201csuper_secret\u201d will be sent has hashed value.","title":"ConfigMap resource"},{"location":"Learn/Sensitive-Information-Redaction.html#deployment-resource","text":"Komodor\u2019s agent will hash template.spec.template.[containeres|initContainers].env list of variables inside Deployment objects for pre-configured list of keys or list of regular expressions. komodor-k8s-watcher.yaml: redact : - \"SENTRY_API_KEY\" - \"PG_.*\" deployment.yaml: apiVersion : apps/v1 kind : Deployment metadata : name : sensitive-deployment spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 env : - name : PG_USERNAME value : super_secret - name : SECRET value : this_will_show_up In the above deployment example we will not send the secret values for PG_USERNAME. SECRET will show up as is due to the fact it won\u2019t match any string or regex in our configuration.","title":"Deployment resource"},{"location":"Learn/Sensitive-Information-Redaction.html#pod-logs","text":"Note: Pod Logs redaction is available starting from Komodor Agent version 0.1.126 Komodor's agent will redact any logs matching one of the patterns set in the redactLogs configuration. komodor-k8s-watcher.yaml: redactLogs : - \"password=(.+?)\\b\" - \"(?U)\\\"sessionId\\\": (\\\".+\\\"{1})\" Environment variables: export KOMOKW_REDACT_LOGS = \"password=(.+?)\\b (?U)\\\"sessionId\\\": (\\\".+\\\"{1})\" Example logs: INPUT : example my password=supersecre t a n d some t hi n g else OUTPUT : example my <REDACTED> a n d some t hi n g else INPUT : { \"level\" : \"INFO\" , \"message\" : \"User has added Item 12453 to Basket\" , \"sessionId\" : \"SESS456\" , \"timestamp\" : 1634477804 } OUTPUT : { \"level\" : \"INFO\" , \"message\" : \"User has added Item 12453 to Basket\" , <REDACTED> , \"timestamp\" : 1634477804 }","title":"Pod Logs"},{"location":"Learn/Sensitive-Information-Redaction.html#testing-logs-redaction-patterns","text":"You can easily test the patterns you want to configure before deploying by using our docker image and our utilities command. \u276f docker run --rm -e KOMOKW_REDACT_LOGS = \"redaction\" komodorio/k8s-watcher test -logredactor -inputlog = \"The log line you want to test redaction here\" Patterns to redact: [ redaction ] Input log ( before redaction ) : The log line you want to test redaction here Output log ( after redaction ) : The log line you want to test <REDACTED> here","title":"Testing logs redaction patterns"},{"location":"Learn/Static-Prevention.html","text":"Static Prevention \u00b6 Introduction \u00b6 Komodor not only provides you with remediation instructions when troubleshooting Kubernetes incidents, but also helps to prevent them from happening in the first place. How it works? \u00b6 Each time a workload is rolled out, and a change has been made to the workload YAML, Komodor runs a set of statical checks in order to improve your YAML\u2019s reliability and efficiency. The results of our scan are presented under the \"Best Practices\u201d section within every service. If you wish to ignore any check, just click on the ignore button under the Best practices pop-up. Checks we run \u00b6 - Deployment Missing Replicas \u00b6 What is checked ? Check if there is only one replica for a deployment . Why should this be checked ? More than one replica recommended to be scheduled . - Tag Not Specified \u00b6 What is checked ? Check if an image tag is either not specified or the latest tag has not been used . Why should this be checked ? Docker 's latest tag is applied by default to images where a tag hasn' t been specified . Not specifying a specific version of an image can lead to a wide variety of problems . - Pull Policy Not Always \u00b6 What is checked ? Check if an image pull policy is not always . Why should this be checked ? By default , an image will be pulled if it isn 't already cached on the node attempting to run it . This can result in variations in images that are running per node , or potentially provide a way to gain access to an image without having direct access to the ImagePullSecret . - Liveness Probe Missing \u00b6 What is checked ? Check if a liveness probe is not configured for a pod . Why should this be checked ? Liveness probes are designed to ensure that an application stays in a healthy state . When a liveness probe fails , the pod will be restarted . - Readiness Probe Missing \u00b6 What is checked ? Check if a readiness probe is not configured for a pod . Why should this be checked ? A readiness probe can ensure that the traffic is not sent to a pod until it is actually ready to receive the traffic . - CPU Requests Missing \u00b6 What is checked ? Check if resources . requests . cpu attribute is not configured . Why should this be checked ? Setting appropriate resource requests will ensure that all your applications have sufficient compute CPU resources . - CPU Limits Missing \u00b6 What is checked ? Check if resources . limits . cpu attribute is not configured . Why should this be checked ? Setting appropriate resource limits will ensure that your applications do not consume too many CPU resources . - Memory Requests Missing \u00b6 What is checked ? Check if resources . requests . memory attribute is not configured . Why should this be checked ? Setting appropriate resource requests will ensure that all your applications have sufficient memory compute resources . - Memory Limits Missing \u00b6 What is checked ? Check if resources . limits . memory attribute is not configured . Why should this be checked ? Setting appropriate resource limits will ensure that your applications do not consume too many memory resources .","title":"Static Prevention"},{"location":"Learn/Static-Prevention.html#static-prevention","text":"","title":"Static Prevention"},{"location":"Learn/Static-Prevention.html#introduction","text":"Komodor not only provides you with remediation instructions when troubleshooting Kubernetes incidents, but also helps to prevent them from happening in the first place.","title":"Introduction"},{"location":"Learn/Static-Prevention.html#how-it-works","text":"Each time a workload is rolled out, and a change has been made to the workload YAML, Komodor runs a set of statical checks in order to improve your YAML\u2019s reliability and efficiency. The results of our scan are presented under the \"Best Practices\u201d section within every service. If you wish to ignore any check, just click on the ignore button under the Best practices pop-up.","title":"How it works?"},{"location":"Learn/Static-Prevention.html#checks-we-run","text":"","title":"Checks we run"},{"location":"Learn/Static-Prevention.html#-deployment-missing-replicas","text":"What is checked ? Check if there is only one replica for a deployment . Why should this be checked ? More than one replica recommended to be scheduled .","title":"- Deployment Missing Replicas"},{"location":"Learn/Static-Prevention.html#-tag-not-specified","text":"What is checked ? Check if an image tag is either not specified or the latest tag has not been used . Why should this be checked ? Docker 's latest tag is applied by default to images where a tag hasn' t been specified . Not specifying a specific version of an image can lead to a wide variety of problems .","title":"- Tag Not Specified"},{"location":"Learn/Static-Prevention.html#-pull-policy-not-always","text":"What is checked ? Check if an image pull policy is not always . Why should this be checked ? By default , an image will be pulled if it isn 't already cached on the node attempting to run it . This can result in variations in images that are running per node , or potentially provide a way to gain access to an image without having direct access to the ImagePullSecret .","title":"- Pull Policy Not Always"},{"location":"Learn/Static-Prevention.html#-liveness-probe-missing","text":"What is checked ? Check if a liveness probe is not configured for a pod . Why should this be checked ? Liveness probes are designed to ensure that an application stays in a healthy state . When a liveness probe fails , the pod will be restarted .","title":"- Liveness Probe Missing"},{"location":"Learn/Static-Prevention.html#-readiness-probe-missing","text":"What is checked ? Check if a readiness probe is not configured for a pod . Why should this be checked ? A readiness probe can ensure that the traffic is not sent to a pod until it is actually ready to receive the traffic .","title":"- Readiness Probe Missing"},{"location":"Learn/Static-Prevention.html#-cpu-requests-missing","text":"What is checked ? Check if resources . requests . cpu attribute is not configured . Why should this be checked ? Setting appropriate resource requests will ensure that all your applications have sufficient compute CPU resources .","title":"- CPU Requests Missing"},{"location":"Learn/Static-Prevention.html#-cpu-limits-missing","text":"What is checked ? Check if resources . limits . cpu attribute is not configured . Why should this be checked ? Setting appropriate resource limits will ensure that your applications do not consume too many CPU resources .","title":"- CPU Limits Missing"},{"location":"Learn/Static-Prevention.html#-memory-requests-missing","text":"What is checked ? Check if resources . requests . memory attribute is not configured . Why should this be checked ? Setting appropriate resource requests will ensure that all your applications have sufficient memory compute resources .","title":"- Memory Requests Missing"},{"location":"Learn/Static-Prevention.html#-memory-limits-missing","text":"What is checked ? Check if resources . limits . memory attribute is not configured . Why should this be checked ? Setting appropriate resource limits will ensure that your applications do not consume too many memory resources .","title":"- Memory Limits Missing"},{"location":"Learn/Test-Utilities-Commands.html","text":"Test Utilities Commands \u00b6 Komodor Agent Test Utilities \u00b6 You can use the komodorio/k8s-watcher docker image to run the test command. Komodor Agent Test Utilities Usage : - connectivity Run utility to test connectivity to Komodor ' s API . Requires env var : [ KOMOKW_API_KEY ] - inputlog string Used in conjunction with - logredactor to provide an input log to redact ( default \"Example log with supersecret password:abc123 and username=root\" ) - logredactor Run utility to test data redaction for pod logs . Requires env var : [ KOMOKW_REDACT_LOG ] Testing connectivity to Komodor API \u00b6 \u276f docker run --rm -e KOMOKW_API_KEY = \"YOUR_API_KEY\" komodorio/k8s-watcher test -connectivity no configuration file found, starting with env vars / defaults time = \"2022-09-15T13:27:29Z\" level = info msg = \"Using cluster name <production> from context\" time = \"2022-09-15T13:27:29Z\" level = info msg = \"Identifying agent with Komodor servers\" agentId = xxx serverHost = \"https://app.komodor.com\" time = \"2022-09-15T13:27:30Z\" level = info msg = \"Successfully connected to Komodor API !!!\" Testing logs redaction patterns \u00b6 You can easily test the patterns you want to configure before deploying by using our docker image and our utilities command. \u276f docker run --rm -e KOMOKW_REDACT_LOGS = \"redaction\" komodorio/k8s-watcher test -logredactor -inputlog = \"The log line you want to test redaction here\" Patterns to redact: [ redaction ] Input log ( before redaction ) : The log line you want to test redaction here Output log ( after redaction ) : The log line you want to test <REDACTED> here","title":"Test Utilities Commands"},{"location":"Learn/Test-Utilities-Commands.html#test-utilities-commands","text":"","title":"Test Utilities Commands"},{"location":"Learn/Test-Utilities-Commands.html#komodor-agent-test-utilities","text":"You can use the komodorio/k8s-watcher docker image to run the test command. Komodor Agent Test Utilities Usage : - connectivity Run utility to test connectivity to Komodor ' s API . Requires env var : [ KOMOKW_API_KEY ] - inputlog string Used in conjunction with - logredactor to provide an input log to redact ( default \"Example log with supersecret password:abc123 and username=root\" ) - logredactor Run utility to test data redaction for pod logs . Requires env var : [ KOMOKW_REDACT_LOG ]","title":"Komodor Agent Test Utilities"},{"location":"Learn/Test-Utilities-Commands.html#testing-connectivity-to-komodor-api","text":"\u276f docker run --rm -e KOMOKW_API_KEY = \"YOUR_API_KEY\" komodorio/k8s-watcher test -connectivity no configuration file found, starting with env vars / defaults time = \"2022-09-15T13:27:29Z\" level = info msg = \"Using cluster name <production> from context\" time = \"2022-09-15T13:27:29Z\" level = info msg = \"Identifying agent with Komodor servers\" agentId = xxx serverHost = \"https://app.komodor.com\" time = \"2022-09-15T13:27:30Z\" level = info msg = \"Successfully connected to Komodor API !!!\"","title":"Testing connectivity to Komodor API"},{"location":"Learn/Test-Utilities-Commands.html#testing-logs-redaction-patterns","text":"You can easily test the patterns you want to configure before deploying by using our docker image and our utilities command. \u276f docker run --rm -e KOMOKW_REDACT_LOGS = \"redaction\" komodorio/k8s-watcher test -logredactor -inputlog = \"The log line you want to test redaction here\" Patterns to redact: [ redaction ] Input log ( before redaction ) : The log line you want to test redaction here Output log ( after redaction ) : The log line you want to test <REDACTED> here","title":"Testing logs redaction patterns"},{"location":"Learn/config-changes.html","text":"Config Change API Integration \u00b6 Config change API allows users to send changes in their config (from internal tools and infrastructure), and see them as part of the Komodor Service view. How to use \u00b6 Request URL \u00b6 Mandatory query params will be used for service selection: serviceName namespace clusterName URL example https://api.komodor.com/v0/config_change?serviceName=backend-service&namespace=default&clusterName=production\" Authentication \u00b6 To authenticate the request use API Key on your \"REST API\" integration tile in the Komodor app and add it to a header with X-API-KEY name. The REST API key can be found in the Integration page . If REST API integration isn't available for your account, please contact your account manager in Komodor. Body \u00b6 This is the event itself with the relevant configuration you want to be connected to the service as JSON. { key1: value1, key2: value2\u2026 } Config map and Secrets \u00b6 Configmap and Secrets can be shown in events tab, please contact us if you want this option. Configmaps that include the coming words will be ignored: \"istio\" \"cluster-autoscaler-status\" Full Example \u00b6 curl -H \"X-API-KEY: <rest api key>\" -H \"Content-Type: application/json\" -d '{\"key\":\"value\"}' \"https://api.komodor.com/v0/config_change?serviceName=backend-service&namespace=default&clusterName=production\"","title":"Config Changes"},{"location":"Learn/config-changes.html#config-change-api-integration","text":"Config change API allows users to send changes in their config (from internal tools and infrastructure), and see them as part of the Komodor Service view.","title":"Config Change API Integration"},{"location":"Learn/config-changes.html#how-to-use","text":"","title":"How to use"},{"location":"Learn/config-changes.html#request-url","text":"Mandatory query params will be used for service selection: serviceName namespace clusterName URL example https://api.komodor.com/v0/config_change?serviceName=backend-service&namespace=default&clusterName=production\"","title":"Request URL"},{"location":"Learn/config-changes.html#authentication","text":"To authenticate the request use API Key on your \"REST API\" integration tile in the Komodor app and add it to a header with X-API-KEY name. The REST API key can be found in the Integration page . If REST API integration isn't available for your account, please contact your account manager in Komodor.","title":"Authentication"},{"location":"Learn/config-changes.html#body","text":"This is the event itself with the relevant configuration you want to be connected to the service as JSON. { key1: value1, key2: value2\u2026 }","title":"Body"},{"location":"Learn/config-changes.html#config-map-and-secrets","text":"Configmap and Secrets can be shown in events tab, please contact us if you want this option. Configmaps that include the coming words will be ignored: \"istio\" \"cluster-autoscaler-status\"","title":"Config map and Secrets"},{"location":"Learn/config-changes.html#full-example","text":"curl -H \"X-API-KEY: <rest api key>\" -H \"Content-Type: application/json\" -d '{\"key\":\"value\"}' \"https://api.komodor.com/v0/config_change?serviceName=backend-service&namespace=default&clusterName=production\"","title":"Full Example"},{"location":"Learn/komodor-source-control.html","text":"Komodor source control support \u00b6 What is it \u00b6 Enrich services with relevant source control metadata. This allows to show smart-diffs as part of Komodor service changes tracking. Doing so connects the specific service with repositories that might not be the original codebase, such as infrastructure or CI repos, which are still relevant changes. How to integrate \u00b6 Add the following annotations to your service's Kubernetes spec: app.komodor.com/[name] : FULL_REPO_URL app.komodor.com/[name].ref : SOURCE_CONTROL_REF app.komodor.com/[another-name] : ANOTHER_FULL_REPO_URL app.komodor.com/[another-name].ref : ANOTHER_ SOURCE_CONTROL_REF Full example \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : git-example annotations : app.komodor.com/app : https://github.com/komodorio/infra app.komodor.com/app.ref : 3350e3311f9520fe5e237e2d71f339029ee051d8 app.komodor.com/infra : https://github.com/komodorio/helm-charts app.komodor.com/infra.ref : 32b355df32713afc511528d909eff296e91dbe74 spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 ports : - containerPort : 8080 protocol : TCP Microsoft DevOps Pipelines Example \u00b6 Microsoft DevOps Pipelines provides predefined variables that contains all the data needed to configure the source control annotations. Modify your pipeline to use these variables when templating the Kubernetes manifests. annotations : app.komodor.com/app : $(Build.Repository.Uri) app.komodor.com/app.ref : $(Build.SourceVersion) Tracked Files \u00b6 Once the integration with Github is established, Komodor will scan the pull requests files (only the names), to see if there are common, interesting files that have been updated. For example, when there was a change in Dockerfile , Komodor will show that in the Deploy summary. To customize which files will be tracked by Komodor. Using Kubernetes annotation app.komodor.com/tracked_files that accepts multiline string (gitignore like) to specify the files. For example, track any yaml files: apiVersion : apps/v1 kind : Deployment metadata : name : git-example annotations : app.komodor.com/tracked_files : | *.yaml app.komodor.com/app : https://github.com/komodorio/infra app.komodor.com/app.ref : 3350e3311f9520fe5e237e2d71f339029ee051d8 app.komodor.com/infra : https://github.com/komodorio/helm-charts app.komodor.com/infra.ref : 32b355df32713afc511528d909eff296e91dbe74 spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 ports : - containerPort : 8080 protocol : TCP\"","title":"Source Control"},{"location":"Learn/komodor-source-control.html#komodor-source-control-support","text":"","title":"Komodor source control support"},{"location":"Learn/komodor-source-control.html#what-is-it","text":"Enrich services with relevant source control metadata. This allows to show smart-diffs as part of Komodor service changes tracking. Doing so connects the specific service with repositories that might not be the original codebase, such as infrastructure or CI repos, which are still relevant changes.","title":"What is it"},{"location":"Learn/komodor-source-control.html#how-to-integrate","text":"Add the following annotations to your service's Kubernetes spec: app.komodor.com/[name] : FULL_REPO_URL app.komodor.com/[name].ref : SOURCE_CONTROL_REF app.komodor.com/[another-name] : ANOTHER_FULL_REPO_URL app.komodor.com/[another-name].ref : ANOTHER_ SOURCE_CONTROL_REF","title":"How to integrate"},{"location":"Learn/komodor-source-control.html#full-example","text":"apiVersion : apps/v1 kind : Deployment metadata : name : git-example annotations : app.komodor.com/app : https://github.com/komodorio/infra app.komodor.com/app.ref : 3350e3311f9520fe5e237e2d71f339029ee051d8 app.komodor.com/infra : https://github.com/komodorio/helm-charts app.komodor.com/infra.ref : 32b355df32713afc511528d909eff296e91dbe74 spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 ports : - containerPort : 8080 protocol : TCP","title":"Full example"},{"location":"Learn/komodor-source-control.html#microsoft-devops-pipelines-example","text":"Microsoft DevOps Pipelines provides predefined variables that contains all the data needed to configure the source control annotations. Modify your pipeline to use these variables when templating the Kubernetes manifests. annotations : app.komodor.com/app : $(Build.Repository.Uri) app.komodor.com/app.ref : $(Build.SourceVersion)","title":"Microsoft DevOps Pipelines Example"},{"location":"Learn/komodor-source-control.html#tracked-files","text":"Once the integration with Github is established, Komodor will scan the pull requests files (only the names), to see if there are common, interesting files that have been updated. For example, when there was a change in Dockerfile , Komodor will show that in the Deploy summary. To customize which files will be tracked by Komodor. Using Kubernetes annotation app.komodor.com/tracked_files that accepts multiline string (gitignore like) to specify the files. For example, track any yaml files: apiVersion : apps/v1 kind : Deployment metadata : name : git-example annotations : app.komodor.com/tracked_files : | *.yaml app.komodor.com/app : https://github.com/komodorio/infra app.komodor.com/app.ref : 3350e3311f9520fe5e237e2d71f339029ee051d8 app.komodor.com/infra : https://github.com/komodorio/helm-charts app.komodor.com/infra.ref : 32b355df32713afc511528d909eff296e91dbe74 spec : selector : matchLabels : run : example replicas : 1 template : metadata : labels : run : example spec : containers : - name : hello-world image : gcr.io/google-samples/node-hello:1.0 ports : - containerPort : 8080 protocol : TCP\"","title":"Tracked Files"}]}